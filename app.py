import streamlit as st
import pandas as pd
import io
import json
from datetime import datetime
import time
import gspread
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import Flow
import logging
from typing import Optional, Dict, Any, List
import hashlib
import traceback
from contextlib import contextmanager
import zlib
import base64
import urllib.parse as urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ", 
    page_icon="ğŸ“Š",
    layout="wide"
)

# ç³»ç»Ÿé…ç½®
ADMIN_PASSWORD = st.secrets.get("system", {}).get("admin_password", "admin123")
PERMISSIONS_SHEET_NAME = "store_permissions"
REPORTS_SHEET_NAME = "store_reports"
SYSTEM_INFO_SHEET_NAME = "system_info"
MAX_RETRIES = 3
RETRY_DELAY = 1
MAX_CHUNK_SIZE = 25000
CACHE_DURATION = 300
COMPRESSION_LEVEL = 9

# OAuthé…ç½®
SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive',
    'https://www.googleapis.com/auth/drive.file'
]

# CSSæ ·å¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
        margin-bottom: 2rem;
    }
    .store-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .admin-panel {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #fdcb6e;
        margin: 1rem 0;
    }
    .oauth-panel {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        padding: 2rem;
        border-radius: 15px;
        border: 2px solid #48cab2;
        margin: 1rem 0;
        text-align: center;
    }
    .receivable-positive {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        color: #721c24;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #f093fb;
        margin: 1rem 0;
        text-align: center;
    }
    .receivable-negative {
        background: linear-gradient(135deg, #a8edea 0%, #d299c2 100%);
        color: #0c4128;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #48cab2;
        margin: 1rem 0;
        text-align: center;
    }
    .status-success {
        background: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #c3e6cb;
        margin: 1rem 0;
    }
    .status-error {
        background: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #f5c6cb;
        margin: 1rem 0;
    }
    .auth-button {
        display: inline-block;
        padding: 12px 24px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        text-decoration: none;
        border-radius: 8px;
        font-weight: bold;
        border: none;
        cursor: pointer;
        font-size: 16px;
    }
    .auth-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }
    </style>
""", unsafe_allow_html=True)

class OAuthError(Exception):
    """OAuthè®¤è¯å¼‚å¸¸"""
    pass

@contextmanager
def error_handler(operation_name: str):
    """é€šç”¨é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        st.error(f"âŒ {operation_name} å¤±è´¥: {str(e)}")
        raise

def get_oauth_flow():
    """åˆ›å»ºOAuth flow"""
    try:
        client_config = {
            "web": {
                "client_id": st.secrets["google_oauth"]["client_id"],
                "client_secret": st.secrets["google_oauth"]["client_secret"],
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token"
            }
        }
        
        flow = Flow.from_client_config(
            client_config,
            scopes=SCOPES,
            redirect_uri=st.secrets["google_oauth"]["redirect_uri"]
        )
        
        return flow
    except KeyError as e:
        st.error(f"âŒ OAuthé…ç½®ç¼ºå¤±: {str(e)}")
        st.markdown("""
        è¯·ç¡®ä¿ `.streamlit/secrets.toml` åŒ…å«ä»¥ä¸‹é…ç½®ï¼š
        ```toml
        [google_oauth]
        client_id = "ä½ çš„å®¢æˆ·ç«¯ID"
        client_secret = "ä½ çš„å®¢æˆ·ç«¯å¯†é’¥"
        redirect_uri = "https://ä½ çš„åº”ç”¨åŸŸå.streamlit.app"
        ```
        """)
        return None
    except Exception as e:
        st.error(f"âŒ OAuth flowåˆ›å»ºå¤±è´¥: {str(e)}")
        return None

def show_oauth_authorization():
    """æ˜¾ç¤ºOAuthæˆæƒç•Œé¢"""
    st.markdown('<div class="oauth-panel">', unsafe_allow_html=True)
    st.markdown("### ğŸ” Googleè´¦å·æˆæƒ")
    st.markdown("ä½¿ç”¨ä½ çš„ä¸ªäººGoogleè´¦å·ç™»å½•ï¼Œäº«å—15GBå­˜å‚¨ç©ºé—´ï¼")
    
    flow = get_oauth_flow()
    if not flow:
        st.error("âŒ OAuthé…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥secretsé…ç½®")
        return False
    
    # ç”ŸæˆæˆæƒURL
    auth_url, state = flow.authorization_url(
        access_type='offline',
        include_granted_scopes='true',
        prompt='consent'
    )
    
    # å­˜å‚¨stateç”¨äºéªŒè¯
    st.session_state['oauth_state'] = state
    
    # æ˜¾ç¤ºæˆæƒæŒ‰é’®
    st.markdown(f"""
    <a href="{auth_url}" target="_blank" class="auth-button">
        ğŸš€ ç‚¹å‡»æˆæƒGoogleè´¦å·
    </a>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # å¤„ç†æˆæƒå›è°ƒ
    query_params = st.experimental_get_query_params()
    
    if 'code' in query_params:
        try:
            # éªŒè¯state
            if 'state' in query_params:
                received_state = query_params['state'][0]
                expected_state = st.session_state.get('oauth_state', '')
                if received_state != expected_state:
                    st.error("âŒ æˆæƒéªŒè¯å¤±è´¥ï¼šçŠ¶æ€ä¸åŒ¹é…")
                    return False
            
            # è·å–æˆæƒç 
            auth_code = query_params['code'][0]
            
            with st.spinner("æ­£åœ¨å®Œæˆæˆæƒ..."):
                # äº¤æ¢token
                flow.fetch_token(code=auth_code)
                credentials = flow.credentials
                
                # å­˜å‚¨å‡­æ®
                st.session_state['google_credentials'] = {
                    'token': credentials.token,
                    'refresh_token': credentials.refresh_token,
                    'id_token': credentials.id_token,
                    'token_uri': credentials.token_uri,
                    'client_id': credentials.client_id,
                    'client_secret': credentials.client_secret,
                    'scopes': credentials.scopes
                }
                
                # æ¸…ç†URLå‚æ•°
                st.experimental_set_query_params()
                
                st.success("âœ… æˆæƒæˆåŠŸï¼")
                st.balloons()
                time.sleep(1)
                st.rerun()
                
        except Exception as e:
            st.error(f"âŒ æˆæƒå¤„ç†å¤±è´¥: {str(e)}")
            logger.error(f"OAuthæˆæƒå¤±è´¥: {str(e)}")
            return False
    
    else:
        st.markdown("""
        **æ“ä½œæ­¥éª¤ï¼š**
        1. ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®
        2. é€‰æ‹©ä½ çš„Googleè´¦å·
        3. åŒæ„æƒé™è¯·æ±‚
        4. ç³»ç»Ÿå°†è‡ªåŠ¨å®Œæˆæˆæƒ
        """)
    
    st.markdown('</div>', unsafe_allow_html=True)
    return False

def get_google_sheets_client():
    """è·å–Google Sheetså®¢æˆ·ç«¯ï¼ˆOAuthç‰ˆæœ¬ï¼‰"""
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‡­æ®
    if 'google_credentials' not in st.session_state:
        return None
    
    try:
        # ä»session stateæ¢å¤å‡­æ®
        cred_data = st.session_state['google_credentials']
        credentials = Credentials(
            token=cred_data['token'],
            refresh_token=cred_data.get('refresh_token'),
            id_token=cred_data.get('id_token'),
            token_uri=cred_data['token_uri'],
            client_id=cred_data['client_id'],
            client_secret=cred_data['client_secret'],
            scopes=cred_data['scopes']
        )
        
        # æ£€æŸ¥å¹¶åˆ·æ–°token
        if credentials.expired and credentials.refresh_token:
            request = Request()
            credentials.refresh(request)
            
            # æ›´æ–°å­˜å‚¨çš„å‡­æ®
            st.session_state['google_credentials'].update({
                'token': credentials.token,
                'id_token': credentials.id_token
            })
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = gspread.authorize(credentials)
        
        # æµ‹è¯•è¿æ¥
        try:
            client.openall()  # è¿™ä¼šéªŒè¯æƒé™
        except Exception as e:
            if "unauthorized" in str(e).lower() or "invalid" in str(e).lower():
                # å‡­æ®æ— æ•ˆï¼Œæ¸…é™¤å¹¶é‡æ–°æˆæƒ
                del st.session_state['google_credentials']
                st.error("âŒ æˆæƒå·²è¿‡æœŸï¼Œè¯·é‡æ–°æˆæƒ")
                st.rerun()
            raise
        
        logger.info("Google Sheetså®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼ˆOAuthï¼‰")
        return client
        
    except Exception as e:
        logger.error(f"OAuthå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        # å¦‚æœå‡­æ®æœ‰é—®é¢˜ï¼Œæ¸…é™¤å¹¶é‡æ–°æˆæƒ
        if 'google_credentials' in st.session_state:
            del st.session_state['google_credentials']
        raise OAuthError(f"è®¤è¯å¤±è´¥: {str(e)}")

def retry_operation(func, *args, max_retries=MAX_RETRIES, delay=RETRY_DELAY, **kwargs):
    """é‡è¯•æ“ä½œè£…é¥°å™¨"""
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            error_msg = str(e)
            if "unauthorized" in error_msg.lower() or "invalid" in error_msg.lower():
                # è®¤è¯é—®é¢˜ä¸é‡è¯•
                raise OAuthError(error_msg)
            if attempt == max_retries - 1:
                logger.error(f"æ“ä½œå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {error_msg}")
                raise
            logger.warning(f"æ“ä½œå¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {error_msg}")
            time.sleep(delay * (attempt + 1))

def compress_data(data: str) -> str:
    """å‹ç¼©æ•°æ®"""
    try:
        compressed = zlib.compress(data.encode('utf-8'), COMPRESSION_LEVEL)
        encoded = base64.b64encode(compressed).decode('ascii')
        logger.info(f"æ•°æ®å‹ç¼©: {len(data)} -> {len(encoded)} bytes (å‹ç¼©ç‡: {(1-len(encoded)/len(data))*100:.1f}%)")
        return encoded
    except Exception as e:
        logger.error(f"æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}")
        return data

def decompress_data(compressed_data: str) -> str:
    """è§£å‹æ•°æ®"""
    try:
        decoded = base64.b64decode(compressed_data.encode('ascii'))
        decompressed = zlib.decompress(decoded).decode('utf-8')
        return decompressed
    except Exception:
        # å¦‚æœè§£å‹å¤±è´¥ï¼Œè¯´æ˜æ•°æ®æœªå‹ç¼©
        return compressed_data

def get_or_create_spreadsheet(gc, name="é—¨åº—æŠ¥è¡¨ç³»ç»Ÿæ•°æ®"):
    """è·å–æˆ–åˆ›å»ºè¡¨æ ¼"""
    def _operation():
        try:
            spreadsheet = gc.open(name)
            logger.info(f"è¡¨æ ¼ '{name}' å·²å­˜åœ¨")
            return spreadsheet
        except gspread.SpreadsheetNotFound:
            logger.info(f"åˆ›å»ºæ–°è¡¨æ ¼ '{name}'")
            spreadsheet = gc.create(name)
            return spreadsheet
    
    return retry_operation(_operation)

def get_or_create_worksheet(spreadsheet, name, rows=500, cols=10):
    """è·å–æˆ–åˆ›å»ºå·¥ä½œè¡¨"""
    def _operation():
        try:
            worksheet = spreadsheet.worksheet(name)
            logger.info(f"å·¥ä½œè¡¨ '{name}' å·²å­˜åœ¨")
            return worksheet
        except gspread.WorksheetNotFound:
            logger.info(f"åˆ›å»ºæ–°å·¥ä½œè¡¨ '{name}'")
            worksheet = spreadsheet.add_worksheet(title=name, rows=rows, cols=cols)
            return worksheet
    
    return retry_operation(_operation)

def clean_and_compress_dataframe(df: pd.DataFrame) -> str:
    """æ¸…ç†å¹¶å‹ç¼©DataFrame"""
    try:
        df_cleaned = df.copy()
        for col in df_cleaned.columns:
            df_cleaned[col] = df_cleaned[col].astype(str)
            df_cleaned[col] = df_cleaned[col].replace({
                'nan': '', 'None': '', 'NaT': '', 'null': '', '<NA>': ''
            })
            # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
            df_cleaned[col] = df_cleaned[col].apply(
                lambda x: x[:800] + '...' if len(str(x)) > 800 else x
            )
        
        # è½¬æ¢ä¸ºJSON
        json_data = df_cleaned.to_json(orient='records', force_ascii=False, ensure_ascii=False)
        
        # å‹ç¼©æ•°æ®
        compressed_data = compress_data(json_data)
        
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: {len(df_cleaned)} è¡Œ -> {len(compressed_data)} å­—ç¬¦")
        return compressed_data
        
    except Exception as e:
        logger.error(f"æ¸…ç†å‹ç¼©DataFrameå¤±è´¥: {str(e)}")
        raise

def save_permissions_to_sheets(df: pd.DataFrame, gc) -> bool:
    """ä¿å­˜æƒé™æ•°æ®"""
    with error_handler("ä¿å­˜æƒé™æ•°æ®"):
        def _save_operation():
            spreadsheet = get_or_create_spreadsheet(gc)
            worksheet = get_or_create_worksheet(spreadsheet, PERMISSIONS_SHEET_NAME, rows=100, cols=5)
            
            # æ¸…ç©ºç°æœ‰æ•°æ®
            worksheet.clear()
            time.sleep(1)
            
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å‹ç¼©æƒé™æ•°æ®
            compressed_data = []
            for _, row in df.iterrows():
                store_name = str(row.iloc[0]).strip()
                user_id = str(row.iloc[1]).strip()
                compressed_data.append(f"{store_name}|{user_id}")
            
            # å°†æ‰€æœ‰æƒé™æ•°æ®å‹ç¼©åˆ°ä¸€ä¸ªå•å…ƒæ ¼
            all_permissions = ";".join(compressed_data)
            compressed_permissions = compress_data(all_permissions)
            
            # ä¿å­˜åˆ°è¡¨æ ¼
            data = [
                ['æ•°æ®ç±»å‹', 'å‹ç¼©æ•°æ®', 'è®°å½•æ•°', 'æ›´æ–°æ—¶é—´'],
                ['permissions', compressed_permissions, len(df), current_time]
            ]
            
            worksheet.update('A1', data)
            logger.info(f"æƒé™æ•°æ®ä¿å­˜æˆåŠŸ: {len(df)} æ¡è®°å½•")
            
            # æ¸…é™¤ç¼“å­˜
            if 'cache_permissions_load' in st.session_state:
                del st.session_state['cache_permissions_load']
            
            return True
        
        return retry_operation(_save_operation)

def load_permissions_from_sheets(gc) -> Optional[pd.DataFrame]:
    """åŠ è½½æƒé™æ•°æ®"""
    # æ£€æŸ¥ç¼“å­˜
    if 'cache_permissions_load' in st.session_state:
        cache_data = st.session_state['cache_permissions_load']
        if time.time() - cache_data['timestamp'] < CACHE_DURATION:
            logger.info("ä»ç¼“å­˜åŠ è½½æƒé™æ•°æ®")
            return cache_data['data']
    
    with error_handler("åŠ è½½æƒé™æ•°æ®"):
        def _load_operation():
            try:
                spreadsheet = get_or_create_spreadsheet(gc)
                worksheet = spreadsheet.worksheet(PERMISSIONS_SHEET_NAME)
                data = worksheet.get_all_values()
                
                if len(data) <= 1:
                    logger.info("æƒé™è¡¨ä¸ºç©º")
                    return None
                
                # è§£æå‹ç¼©æ•°æ®
                if len(data) >= 2 and len(data[1]) >= 2:
                    compressed_data = data[1][1]
                    
                    # è§£å‹æ•°æ®
                    decompressed_data = decompress_data(compressed_data)
                    
                    # è§£ææƒé™æ•°æ®
                    permissions = []
                    for item in decompressed_data.split(';'):
                        if '|' in item:
                            store_name, user_id = item.split('|', 1)
                            permissions.append({
                                'é—¨åº—åç§°': store_name.strip(),
                                'äººå‘˜ç¼–å·': user_id.strip()
                            })
                    
                    if permissions:
                        result_df = pd.DataFrame(permissions)
                        # ç§»é™¤ç©ºè¡Œ
                        result_df = result_df[
                            (result_df['é—¨åº—åç§°'] != '') & 
                            (result_df['äººå‘˜ç¼–å·'] != '')
                        ]
                        
                        logger.info(f"æƒé™æ•°æ®åŠ è½½æˆåŠŸ: {len(result_df)} æ¡è®°å½•")
                        
                        # è®¾ç½®ç¼“å­˜
                        st.session_state['cache_permissions_load'] = {
                            'data': result_df,
                            'timestamp': time.time()
                        }
                        return result_df
                
                return None
                
            except gspread.WorksheetNotFound:
                logger.info("æƒé™è¡¨ä¸å­˜åœ¨")
                return None
        
        return retry_operation(_load_operation)

def save_reports_to_sheets(reports_dict: Dict[str, pd.DataFrame], gc) -> bool:
    """ä¿å­˜æŠ¥è¡¨æ•°æ®"""
    with error_handler("ä¿å­˜æŠ¥è¡¨æ•°æ®"):
        def _save_operation():
            spreadsheet = get_or_create_spreadsheet(gc)
            worksheet = get_or_create_worksheet(spreadsheet, REPORTS_SHEET_NAME, rows=300, cols=8)
            
            # æ¸…ç©ºç°æœ‰æ•°æ®
            with st.spinner("æ¸…ç†æ—§æ•°æ®..."):
                worksheet.clear()
                time.sleep(1)
            
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            all_data = [['é—¨åº—åç§°', 'å‹ç¼©æ•°æ®', 'è¡Œæ•°', 'åˆ—æ•°', 'æ›´æ–°æ—¶é—´', 'åˆ†ç‰‡åºå·', 'æ€»åˆ†ç‰‡æ•°', 'æ•°æ®å“ˆå¸Œ']]
            
            with st.spinner("å‹ç¼©å¹¶ä¿å­˜æ•°æ®..."):
                total_stores = len(reports_dict)
                progress_bar = st.progress(0)
                
                for idx, (store_name, df) in enumerate(reports_dict.items()):
                    try:
                        # æ¸…ç†å¹¶å‹ç¼©æ•°æ®
                        compressed_data = clean_and_compress_dataframe(df)
                        
                        # è®¡ç®—å“ˆå¸Œ
                        data_hash = hashlib.md5(compressed_data.encode('utf-8')).hexdigest()[:16]
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†ç‰‡
                        if len(compressed_data) <= MAX_CHUNK_SIZE:
                            all_data.append([
                                store_name, compressed_data, len(df), len(df.columns), 
                                current_time, "1", "1", data_hash
                            ])
                        else:
                            # åˆ†ç‰‡å­˜å‚¨
                            chunks = []
                            for i in range(0, len(compressed_data), MAX_CHUNK_SIZE):
                                chunks.append(compressed_data[i:i + MAX_CHUNK_SIZE])
                            
                            total_chunks = len(chunks)
                            for chunk_idx, chunk in enumerate(chunks):
                                chunk_name = f"{store_name}_åˆ†ç‰‡{chunk_idx+1}"
                                all_data.append([
                                    chunk_name, chunk, len(df), len(df.columns),
                                    current_time, str(chunk_idx+1), str(total_chunks), data_hash
                                ])
                        
                        # æ›´æ–°è¿›åº¦
                        progress_bar.progress((idx + 1) / total_stores)
                        logger.info(f"å¤„ç†å®Œæˆ: {store_name}")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç† {store_name} æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                progress_bar.empty()
            
            # åˆ†æ‰¹ä¿å­˜æ•°æ®
            batch_size = 15
            if len(all_data) > 1:
                for i in range(1, len(all_data), batch_size):
                    batch_data = all_data[i:i+batch_size]
                    
                    if i == 1:
                        worksheet.update('A1', [all_data[0]] + batch_data)
                    else:
                        row_num = i + 1
                        worksheet.update(f'A{row_num}', batch_data)
                    
                    time.sleep(0.8)
            
            logger.info(f"æŠ¥è¡¨æ•°æ®ä¿å­˜å®Œæˆ: {len(all_data) - 1} æ¡è®°å½•")
            
            # æ¸…é™¤ç¼“å­˜
            if 'cache_reports_load' in st.session_state:
                del st.session_state['cache_reports_load']
            
            return True
        
        return retry_operation(_save_operation)

def load_reports_from_sheets(gc) -> Dict[str, pd.DataFrame]:
    """åŠ è½½æŠ¥è¡¨æ•°æ®"""
    # æ£€æŸ¥ç¼“å­˜
    if 'cache_reports_load' in st.session_state:
        cache_data = st.session_state['cache_reports_load']
        if time.time() - cache_data['timestamp'] < CACHE_DURATION:
            logger.info("ä»ç¼“å­˜åŠ è½½æŠ¥è¡¨æ•°æ®")
            return cache_data['data']
    
    with error_handler("åŠ è½½æŠ¥è¡¨æ•°æ®"):
        def _load_operation():
            try:
                spreadsheet = get_or_create_spreadsheet(gc)
                worksheet = spreadsheet.worksheet(REPORTS_SHEET_NAME)
                data = worksheet.get_all_values()
                
                if len(data) <= 1:
                    logger.info("æŠ¥è¡¨æ•°æ®ä¸ºç©º")
                    return {}
                
                # é‡æ„æ•°æ®
                reports_dict = {}
                fragments_dict = {}
                
                for row in data[1:]:
                    if len(row) >= 7:
                        store_name = row[0]
                        compressed_data = row[1]
                        rows_count = row[2]
                        cols_count = row[3]
                        update_time = row[4]
                        chunk_num = row[5]
                        total_chunks = row[6]
                        data_hash = row[7] if len(row) > 7 else ''
                        
                        # å¤„ç†åˆ†ç‰‡æ•°æ®
                        if '_åˆ†ç‰‡' in store_name:
                            base_name = store_name.split('_åˆ†ç‰‡')[0]
                            if base_name not in fragments_dict:
                                fragments_dict[base_name] = []
                            
                            fragments_dict[base_name].append({
                                'data': compressed_data,
                                'chunk_num': chunk_num,
                                'total_chunks': total_chunks,
                                'data_hash': data_hash
                            })
                        else:
                            fragments_dict[store_name] = [{
                                'data': compressed_data,
                                'chunk_num': '1',
                                'total_chunks': '1',
                                'data_hash': data_hash
                            }]
                
                # é‡æ„æ‰€æœ‰æ•°æ®
                for store_name, fragments in fragments_dict.items():
                    try:
                        if len(fragments) == 1:
                            compressed_data = fragments[0]['data']
                        else:
                            # æŒ‰é¡ºåºé‡ç»„åˆ†ç‰‡
                            fragments.sort(key=lambda x: int(x['chunk_num']))
                            compressed_data = ''.join([frag['data'] for frag in fragments])
                        
                        # è§£å‹æ•°æ®
                        json_data = decompress_data(compressed_data)
                        
                        # è§£æä¸ºDataFrame
                        df = pd.read_json(json_data, orient='records')
                        
                        if not df.empty:
                            # æ•°æ®åå¤„ç†
                            if len(df) > 0:
                                first_row = df.iloc[0]
                                non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
                                if non_empty_count <= 2 and len(df) > 1:
                                    df = df.iloc[1:].reset_index(drop=True)
                            
                            # å¤„ç†è¡¨å¤´
                            if len(df) > 1:
                                header_row = df.iloc[0].fillna('').astype(str).tolist()
                                data_rows = df.iloc[1:].copy()
                                
                                cols = []
                                for i, col in enumerate(header_row):
                                    col = str(col).strip()
                                    if col == '' or col == 'nan' or col == '0':
                                        col = f'åˆ—{i+1}' if i > 0 else 'é¡¹ç›®åç§°'
                                    
                                    original_col = col
                                    counter = 1
                                    while col in cols:
                                        col = f"{original_col}_{counter}"
                                        counter += 1
                                    cols.append(col)
                                
                                min_cols = min(len(data_rows.columns), len(cols))
                                cols = cols[:min_cols]
                                data_rows = data_rows.iloc[:, :min_cols]
                                data_rows.columns = cols
                                df = data_rows.reset_index(drop=True).fillna('')
                            
                            reports_dict[store_name] = df
                            logger.info(f"{store_name} æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                    
                    except Exception as e:
                        logger.error(f"è§£æ {store_name} æ•°æ®å¤±è´¥: {str(e)}")
                        continue
                
                logger.info(f"æŠ¥è¡¨æ•°æ®åŠ è½½å®Œæˆ: {len(reports_dict)} ä¸ªé—¨åº—")
                
                # è®¾ç½®ç¼“å­˜
                st.session_state['cache_reports_load'] = {
                    'data': reports_dict,
                    'timestamp': time.time()
                }
                return reports_dict
                
            except gspread.WorksheetNotFound:
                logger.info("æŠ¥è¡¨æ•°æ®è¡¨ä¸å­˜åœ¨")
                return {}
        
        return retry_operation(_load_operation)

def analyze_receivable_data(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†æåº”æ”¶æœªæ”¶é¢æ•°æ®"""
    result = {}
    
    if len(df.columns) == 0 or len(df) == 0:
        return result
    
    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§°
    first_row = df.iloc[0] if len(df) > 0 else None
    if first_row is not None:
        non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
        if non_empty_count <= 2:
            df = df.iloc[1:].reset_index(drop=True)
            result['skipped_store_name_row'] = True
    
    # æŸ¥æ‰¾ç¬¬69è¡Œ
    target_row_index = 68
    
    if len(df) > target_row_index:
        row = df.iloc[target_row_index]
        first_col_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
        
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
        
        for keyword in keywords:
            if keyword in first_col_value:
                for col_idx in range(len(row)-1, 0, -1):
                    val = row.iloc[col_idx]
                    if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                        cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                        
                        if cleaned.startswith('(') and cleaned.endswith(')'):
                            cleaned = '-' + cleaned[1:-1]
                        
                        try:
                            amount = float(cleaned)
                            if amount != 0:
                                result['åº”æ”¶-æœªæ”¶é¢'] = {
                                    'amount': amount,
                                    'column_name': str(df.columns[col_idx]),
                                    'row_name': first_col_value,
                                    'row_index': target_row_index,
                                    'actual_row_number': target_row_index + 1
                                }
                                return result
                        except ValueError:
                            continue
                break
    
    # å¤‡ç”¨æŸ¥æ‰¾
    if 'åº”æ”¶-æœªæ”¶é¢' not in result:
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
        
        for idx, row in df.iterrows():
            try:
                row_name = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
                if not row_name.strip():
                    continue
                
                for keyword in keywords:
                    if keyword in row_name:
                        for col_idx in range(len(row)-1, 0, -1):
                            val = row.iloc[col_idx]
                            if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                                cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                                
                                if cleaned.startswith('(') and cleaned.endswith(')'):
                                    cleaned = '-' + cleaned[1:-1]
                                
                                try:
                                    amount = float(cleaned)
                                    if amount != 0:
                                        result['åº”æ”¶-æœªæ”¶é¢'] = {
                                            'amount': amount,
                                            'column_name': str(df.columns[col_idx]),
                                            'row_name': row_name,
                                            'row_index': idx,
                                            'actual_row_number': idx + 1,
                                            'note': f'åœ¨ç¬¬{idx+1}è¡Œæ‰¾åˆ°ï¼ˆéç¬¬69è¡Œï¼‰'
                                        }
                                        return result
                                except ValueError:
                                    continue
                        break
            except Exception:
                continue
    
    # è°ƒè¯•ä¿¡æ¯
    result['debug_info'] = {
        'total_rows': len(df),
        'checked_row_69': len(df) > target_row_index,
        'row_69_content': str(df.iloc[target_row_index].iloc[0]) if len(df) > target_row_index else 'N/A'
    }
    
    return result

def verify_user_permission(store_name: str, user_id: str, permissions_data: Optional[pd.DataFrame]) -> bool:
    """éªŒè¯ç”¨æˆ·æƒé™"""
    if permissions_data is None or len(permissions_data.columns) < 2:
        return False
    
    store_col = permissions_data.columns[0]
    id_col = permissions_data.columns[1]
    
    for _, row in permissions_data.iterrows():
        stored_store = str(row[store_col]).strip()
        stored_id = str(row[id_col]).strip()
        
        if (store_name in stored_store or stored_store in store_name) and stored_id == str(user_id):
            return True
    
    return False

def find_matching_reports(store_name: str, reports_data: Dict[str, pd.DataFrame]) -> List[str]:
    """æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨"""
    matching = []
    for sheet_name in reports_data.keys():
        if store_name in sheet_name or sheet_name in store_name:
            matching.append(sheet_name)
    return matching

def show_user_info(gc):
    """æ˜¾ç¤ºç”¨æˆ·è´¦å·ä¿¡æ¯"""
    try:
        if 'google_credentials' in st.session_state:
            cred_data = st.session_state['google_credentials']
            
            # å°è¯•è·å–ç”¨æˆ·ä¿¡æ¯
            st.markdown("""
            <div class="status-success">
                âœ… <strong>å·²è¿æ¥åˆ°ä½ çš„ä¸ªäººGoogleè´¦å·</strong><br>
                ğŸ—„ï¸ ä½¿ç”¨ä¸ªäººGoogle Driveå­˜å‚¨ç©ºé—´ (15GB)<br>
                ğŸ”’ æ•°æ®å®‰å…¨å­˜å‚¨åœ¨ä½ çš„ä¸ªäººè´¦å·ä¸­
            </div>
            """, unsafe_allow_html=True)
    except Exception as e:
        logger.warning(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
if 'store_name' not in st.session_state:
    st.session_state.store_name = ""
if 'user_id' not in st.session_state:
    st.session_state.user_id = ""
if 'is_admin' not in st.session_state:
    st.session_state.is_admin = False

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ“Š é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ</h1>', unsafe_allow_html=True)

# Googleè´¦å·è®¤è¯
try:
    gc = get_google_sheets_client()
    if gc:
        show_user_info(gc)
    else:
        if show_oauth_authorization():
            st.rerun()
        else:
            st.stop()
except OAuthError:
    st.error("âŒ è®¤è¯å¤±è´¥ï¼Œè¯·é‡æ–°æˆæƒ")
    if 'google_credentials' in st.session_state:
        del st.session_state['google_credentials']
    st.stop()
except Exception as e:
    st.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    st.stop()

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»ŸåŠŸèƒ½")
    
    # ç³»ç»ŸçŠ¶æ€
    st.subheader("ğŸ“¡ ç³»ç»ŸçŠ¶æ€")
    if gc:
        st.success("ğŸŸ¢ ä¸ªäººè´¦å·å·²è¿æ¥")
        st.info("ğŸ’¾ ä½¿ç”¨ä¸ªäºº15GBå­˜å‚¨")
    else:
        st.error("ğŸ”´ æœªè¿æ¥")
    
    # è´¦å·ç®¡ç†
    st.subheader("ğŸ‘¤ è´¦å·ç®¡ç†")
    if st.button("ğŸ”„ é‡æ–°æˆæƒ"):
        if 'google_credentials' in st.session_state:
            del st.session_state['google_credentials']
        st.rerun()
    
    user_type = st.radio("é€‰æ‹©ç”¨æˆ·ç±»å‹", ["æ™®é€šç”¨æˆ·", "ç®¡ç†å‘˜"])
    
    if user_type == "ç®¡ç†å‘˜":
        st.subheader("ğŸ” ç®¡ç†å‘˜ç™»å½•")
        admin_password = st.text_input("ç®¡ç†å‘˜å¯†ç ", type="password")
        
        if st.button("éªŒè¯ç®¡ç†å‘˜èº«ä»½"):
            if admin_password == ADMIN_PASSWORD:
                st.session_state.is_admin = True
                st.success("âœ… éªŒè¯æˆåŠŸï¼")
                st.rerun()
            else:
                st.error("âŒ å¯†ç é”™è¯¯ï¼")
        
        if st.session_state.is_admin:
            st.subheader("ğŸ“ æ–‡ä»¶ç®¡ç†")
            
            # ç¼“å­˜ç®¡ç†
            if st.button("ğŸ—‘ï¸ æ¸…ç†ç³»ç»Ÿç¼“å­˜"):
                cache_keys = [key for key in st.session_state.keys() if key.startswith('cache_')]
                for key in cache_keys:
                    del st.session_state[key]
                st.success("âœ… ç¼“å­˜å·²æ¸…é™¤")
                st.rerun()
            
            # ä¸Šä¼ æƒé™è¡¨
            permissions_file = st.file_uploader("ä¸Šä¼ é—¨åº—æƒé™è¡¨", type=['xlsx', 'xls'])
            if permissions_file:
                try:
                    with st.spinner("å¤„ç†æƒé™è¡¨æ–‡ä»¶..."):
                        df = pd.read_excel(permissions_file)
                        if len(df.columns) >= 2:
                            with st.spinner("å‹ç¼©å¹¶ä¿å­˜åˆ°ä¸ªäººDrive..."):
                                if save_permissions_to_sheets(df, gc):
                                    st.success(f"âœ… æƒé™è¡¨å·²ä¸Šä¼ ï¼š{len(df)} ä¸ªç”¨æˆ·")
                                    st.balloons()
                                else:
                                    st.error("âŒ ä¿å­˜å¤±è´¥")
                        else:
                            st.error("âŒ æ ¼å¼é”™è¯¯ï¼šéœ€è¦è‡³å°‘ä¸¤åˆ—ï¼ˆé—¨åº—åç§°ã€äººå‘˜ç¼–å·ï¼‰")
                except Exception as e:
                    st.error(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}")
            
            # ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨
            reports_file = st.file_uploader("ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨", type=['xlsx', 'xls'])
            if reports_file:
                try:
                    with st.spinner("å¤„ç†æŠ¥è¡¨æ–‡ä»¶..."):
                        excel_file = pd.ExcelFile(reports_file)
                        reports_dict = {}
                        
                        for sheet in excel_file.sheet_names:
                            try:
                                df = pd.read_excel(reports_file, sheet_name=sheet)
                                if not df.empty:
                                    reports_dict[sheet] = df
                                    logger.info(f"è¯»å–å·¥ä½œè¡¨ '{sheet}': {len(df)} è¡Œ")
                            except Exception as e:
                                logger.warning(f"è·³è¿‡å·¥ä½œè¡¨ '{sheet}': {str(e)}")
                                continue
                        
                        if reports_dict:
                            with st.spinner("å‹ç¼©å¹¶ä¿å­˜åˆ°ä¸ªäººDrive..."):
                                if save_reports_to_sheets(reports_dict, gc):
                                    st.success(f"âœ… æŠ¥è¡¨å·²ä¸Šä¼ ï¼š{len(reports_dict)} ä¸ªé—¨åº—")
                                    st.balloons()
                                else:
                                    st.error("âŒ ä¿å­˜å¤±è´¥")
                        else:
                            st.error("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å·¥ä½œè¡¨")
                            
                except Exception as e:
                    st.error(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}")
    
    else:
        if st.session_state.logged_in:
            st.subheader("ğŸ‘¤ å½“å‰ç™»å½•")
            st.info(f"é—¨åº—ï¼š{st.session_state.store_name}")
            st.info(f"ç¼–å·ï¼š{st.session_state.user_id}")
            
            if st.button("ğŸšª é€€å‡ºç™»å½•"):
                st.session_state.logged_in = False
                st.session_state.store_name = ""
                st.session_state.user_id = ""
                st.rerun()

# ä¸»ç•Œé¢
if user_type == "ç®¡ç†å‘˜" and st.session_state.is_admin:
    st.markdown('<div class="admin-panel"><h3>ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜æ§åˆ¶é¢æ¿</h3><p>ä½¿ç”¨ä¸ªäººGoogleè´¦å·ï¼Œäº«å—15GBå­˜å‚¨ç©ºé—´å’Œé«˜é€Ÿè®¿é—®</p></div>', unsafe_allow_html=True)
    
    try:
        with st.spinner("åŠ è½½æ•°æ®ç»Ÿè®¡..."):
            permissions_data = load_permissions_from_sheets(gc)
            reports_data = load_reports_from_sheets(gc)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            perms_count = len(permissions_data) if permissions_data is not None else 0
            st.metric("æƒé™è¡¨ç”¨æˆ·æ•°", perms_count)
        with col2:
            reports_count = len(reports_data)
            st.metric("æŠ¥è¡¨é—¨åº—æ•°", reports_count)
        with col3:
            cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
            st.metric("ç¼“å­˜é¡¹ç›®æ•°", cache_count)
            
        # æ•°æ®é¢„è§ˆ
        if permissions_data is not None and len(permissions_data) > 0:
            st.subheader("ğŸ‘¥ æƒé™æ•°æ®é¢„è§ˆ")
            st.dataframe(permissions_data.head(10), use_container_width=True)
        
        if reports_data:
            st.subheader("ğŸ“Š æŠ¥è¡¨æ•°æ®é¢„è§ˆ")
            report_names = list(reports_data.keys())[:3]
            for name in report_names:
                with st.expander(f"ğŸ“‹ {name}"):
                    df = reports_data[name]
                    st.write(f"æ•°æ®è§„æ¨¡: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
                    st.dataframe(df.head(3), use_container_width=True)
                    
    except Exception as e:
        st.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}")

elif user_type == "ç®¡ç†å‘˜" and not st.session_state.is_admin:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ç®¡ç†å‘˜å¯†ç ")

else:
    if not st.session_state.logged_in:
        st.subheader("ğŸ” ç”¨æˆ·ç™»å½•")
        
        try:
            with st.spinner("åŠ è½½æƒé™æ•°æ®..."):
                permissions_data = load_permissions_from_sheets(gc)
            
            if permissions_data is None:
                st.warning("âš ï¸ ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
            else:
                stores = sorted(permissions_data[permissions_data.columns[0]].unique().tolist())
                
                with st.form("login_form"):
                    selected_store = st.selectbox("é€‰æ‹©é—¨åº—", stores)
                    user_id = st.text_input("äººå‘˜ç¼–å·")
                    submit = st.form_submit_button("ğŸš€ ç™»å½•")
                    
                    if submit and selected_store and user_id:
                        if verify_user_permission(selected_store, user_id, permissions_data):
                            st.session_state.logged_in = True
                            st.session_state.store_name = selected_store
                            st.session_state.user_id = user_id
                            st.success("âœ… ç™»å½•æˆåŠŸï¼")
                            st.balloons()
                            st.rerun()
                        else:
                            st.error("âŒ é—¨åº—æˆ–ç¼–å·é”™è¯¯ï¼")
                            
        except Exception as e:
            st.error(f"âŒ æƒé™éªŒè¯å¤±è´¥ï¼š{str(e)}")
    
    else:
        # å·²ç™»å½• - æ˜¾ç¤ºæŠ¥è¡¨
        st.markdown(f'<div class="store-info"><h3>ğŸª {st.session_state.store_name}</h3><p>æ“ä½œå‘˜ï¼š{st.session_state.user_id}</p></div>', unsafe_allow_html=True)
        
        try:
            with st.spinner("åŠ è½½æŠ¥è¡¨æ•°æ®..."):
                reports_data = load_reports_from_sheets(gc)
                matching_sheets = find_matching_reports(st.session_state.store_name, reports_data)
            
            if matching_sheets:
                if len(matching_sheets) > 1:
                    selected_sheet = st.selectbox("é€‰æ‹©æŠ¥è¡¨", matching_sheets)
                else:
                    selected_sheet = matching_sheets[0]
                
                df = reports_data[selected_sheet]
                
                # åº”æ”¶-æœªæ”¶é¢çœ‹æ¿
                st.subheader("ğŸ’° åº”æ”¶-æœªæ”¶é¢")
                
                try:
                    analysis_results = analyze_receivable_data(df)
                    
                    if 'åº”æ”¶-æœªæ”¶é¢' in analysis_results:
                        data = analysis_results['åº”æ”¶-æœªæ”¶é¢']
                        amount = data['amount']
                        
                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            if amount > 0:
                                st.markdown(f'''
                                    <div class="receivable-positive">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’³ Â¥{amount:,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">é—¨åº—åº”ä»˜æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            elif amount < 0:
                                st.markdown(f'''
                                    <div class="receivable-negative">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’š Â¥{abs(amount):,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">æ€»éƒ¨åº”é€€æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            else:
                                st.markdown('''
                                    <div style="background: #e8f5e8; color: #2e7d32; padding: 2rem; border-radius: 15px; text-align: center;">
                                        <h1 style="margin: 0; font-size: 3rem;">âš–ï¸ Â¥0.00</h1>
                                        <h3 style="margin: 0.5rem 0;">æ”¶æ”¯å¹³è¡¡</h3>
                                        <p style="margin: 0;">åº”æ”¶æœªæ”¶é¢ä¸ºé›¶ï¼Œè´¦ç›®å¹³è¡¡</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                    
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°åº”æ”¶-æœªæ”¶é¢æ•°æ®")
                        
                        with st.expander("ğŸ” æŸ¥çœ‹è¯¦æƒ…", expanded=False):
                            debug_info = analysis_results.get('debug_info', {})
                            
                            st.markdown("### ğŸ“‹ æ•°æ®æŸ¥æ‰¾è¯´æ˜")
                            st.write(f"- **æŠ¥è¡¨æ€»è¡Œæ•°ï¼š** {debug_info.get('total_rows', 0)} è¡Œ")
                            
                            if debug_info.get('checked_row_69'):
                                st.write(f"- **ç¬¬69è¡Œå†…å®¹ï¼š** {debug_info.get('row_69_content', 'N/A')}")
                            else:
                                st.write("- **ç¬¬69è¡Œï¼š** æŠ¥è¡¨è¡Œæ•°ä¸è¶³69è¡Œ")
                
                except Exception as e:
                    st.error(f"âŒ åˆ†ææ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}")
                
                st.divider()
                
                # å®Œæ•´æŠ¥è¡¨æ•°æ®
                st.subheader("ğŸ“‹ å®Œæ•´æŠ¥è¡¨æ•°æ®")
                
                search_term = st.text_input("ğŸ” æœç´¢æŠ¥è¡¨å†…å®¹")
                
                try:
                    if search_term:
                        search_df = df.copy()
                        for col in search_df.columns:
                            search_df[col] = search_df[col].astype(str).fillna('')
                        
                        mask = search_df.apply(
                            lambda x: x.str.contains(search_term, case=False, na=False, regex=False)
                        ).any(axis=1)
                        filtered_df = df[mask]
                        st.info(f"æ‰¾åˆ° {len(filtered_df)} æ¡åŒ…å« '{search_term}' çš„è®°å½•")
                    else:
                        filtered_df = df
                    
                    st.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼šå…± {len(filtered_df)} æ¡è®°å½•ï¼Œ{len(df.columns)} åˆ—")
                    
                    if len(filtered_df) > 0:
                        display_df = filtered_df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_columns = []
                        for i, col in enumerate(display_df.columns):
                            col_name = str(col)
                            if col_name in unique_columns:
                                col_name = f"{col_name}_{i}"
                            unique_columns.append(col_name)
                        display_df.columns = unique_columns
                        
                        # æ¸…ç†æ•°æ®å†…å®¹
                        for col in display_df.columns:
                            display_df[col] = display_df[col].astype(str).fillna('')
                        
                        st.dataframe(display_df, use_container_width=True, height=400)
                    
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                        
                except Exception as e:
                    st.error(f"âŒ æ•°æ®å¤„ç†æ—¶å‡ºé”™ï¼š{str(e)}")
                
                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ æ•°æ®ä¸‹è½½")
                
                col1, col2 = st.columns(2)
                with col1:
                    try:
                        buffer = io.BytesIO()
                        download_df = df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_cols = []
                        for i, col in enumerate(download_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        download_df.columns = unique_cols
                        
                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            download_df.to_excel(writer, index=False)
                        
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥è¡¨ (Excel)",
                            buffer.getvalue(),
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.xlsx",
                            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    except Exception as e:
                        st.error(f"Excelä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}")
                
                with col2:
                    try:
                        csv_df = df.copy()
                        unique_cols = []
                        for i, col in enumerate(csv_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        csv_df.columns = unique_cols
                        
                        csv = csv_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                            csv,
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.csv",
                            "text/csv"
                        )
                    except Exception as e:
                        st.error(f"CSVä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}")
            
            else:
                st.error(f"âŒ æœªæ‰¾åˆ°é—¨åº— '{st.session_state.store_name}' çš„æŠ¥è¡¨")
                
        except Exception as e:
            st.error(f"âŒ æŠ¥è¡¨åŠ è½½å¤±è´¥ï¼š{str(e)}")

# é¡µé¢åº•éƒ¨çŠ¶æ€ä¿¡æ¯
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"ğŸ•’ å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
with col2:
    cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
    st.caption(f"ğŸ’¾ ç¼“å­˜é¡¹ç›®: {cache_count}")
with col3:
    st.caption("ğŸ”§ ç‰ˆæœ¬: v4.0 (OAuthä¸ªäººè´¦å·ç‰ˆ)")
