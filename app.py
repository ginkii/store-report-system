import streamlit as st
import pandas as pd
import io
import json
from datetime import datetime
import time
import gspread
from google.oauth2.service_account import Credentials
import logging
from typing import Optional, Dict, Any, List
import hashlib
import pickle
import traceback
from contextlib import contextmanager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ç³»ç»Ÿé…ç½®
ADMIN_PASSWORD = "admin123"
PERMISSIONS_SHEET_NAME = "store_permissions"
REPORTS_SHEET_NAME = "store_reports"
SYSTEM_INFO_SHEET_NAME = "system_info" # ç›®å‰ä»£ç ä¸­æœªç”¨åˆ°ï¼Œä½†ä¿ç•™
MAX_RETRIES = 3
RETRY_DELAY = 1
MAX_CHUNK_SIZE = 30000  # å‡å°åˆ†ç‰‡å¤§å°
CACHE_DURATION = 300  # ç¼“å­˜5åˆ†é’Ÿ

# *** æ–°å¢é…ç½®ï¼šä½ ä¸ªäºº Google Drive ä¸­ç›®æ ‡ Google Sheets è¡¨æ ¼çš„å®Œæ•´ URL ***
# è¯·ç¡®ä¿è¿™æ˜¯ä½ å·²ç»åˆ›å»ºå¹¶å…±äº«ç»™æœåŠ¡è´¦æˆ·ï¼ˆç¼–è¾‘è€…æƒé™ï¼‰çš„ Google Sheets è¡¨æ ¼çš„å®Œæ•´ URLã€‚
TARGET_SPREADSHEET_URL = 'https://docs.google.com/spreadsheets/d/1Ly2QCB3zAhQ7o_8h2Aj-lbSLL8YdPI2UZyNSxyWDp_Y/edit?gid=0#gid=0' # <--- ç°åœ¨ç›´æ¥ä½¿ç”¨ä½ æä¾›çš„URLï¼Œæ²¡æœ‰å¤šä½™æ£€æŸ¥äº†ï¼

# Google Drive API æƒé™èŒƒå›´ï¼Œå…è®¸è¯»å†™ Drive æ–‡ä»¶
SCOPES = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]


# CSSæ ·å¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
        margin-bottom: 2rem;
    }
    .store-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .admin-panel {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #fdcb6e;
        margin: 1rem 0;
    }
    .receivable-positive {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        color: #721c24;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #f093fb;
        margin: 1rem 0;
        text-align: center;
    }
    .receivable-negative {
        background: linear-gradient(135deg, #a8edea 0%, #d299c2 100%);
        color: #0c4128;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #48cab2;
        margin: 1rem 0;
        text-align: center;
    }
    .status-success {
        background: #d4edda;
        color: #155724;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
        margin: 0.5rem 0;
    }
    .status-error {
        background: #f8d7da;
        color: #721c24;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
        margin: 0.5rem 0;
    }
    .status-warning {
        background: #fff3cd;
        color: #856404;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #ffeaa7;
        margin: 0.5rem 0;
    }
    </style>
""", unsafe_allow_html=True)

class SheetOperationError(Exception):
    """Google Sheetsæ“ä½œå¼‚å¸¸"""
    pass

class DataProcessingError(Exception):
    """æ•°æ®å¤„ç†å¼‚å¸¸"""
    pass

@contextmanager
def error_handler(operation_name: str):
    """é€šç”¨é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        st.error(f"âŒ {operation_name} å¤±è´¥: {str(e)}")
        raise

def retry_operation(func, *args, max_retries=MAX_RETRIES, delay=RETRY_DELAY, **kwargs):
    """é‡è¯•æ“ä½œè£…é¥°å™¨"""
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"æ“ä½œå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {str(e)}")
                raise
            logger.warning(f"æ“ä½œå¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {str(e)}")
            time.sleep(delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ

def get_cache_key(operation: str, params: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(f"{operation}_{params}".encode()).hexdigest()

def set_cache(key: str, data: Any, duration: int = CACHE_DURATION):
    """è®¾ç½®ç¼“å­˜"""
    try:
        cache_data = {
            'data': data,
            'timestamp': time.time(),
            'duration': duration
        }
        st.session_state[f"cache_{key}"] = cache_data
    except Exception as e:
        logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {str(e)}")

def get_cache(key: str) -> Optional[Any]:
    """è·å–ç¼“å­˜"""
    try:
        cache_key = f"cache_{key}"
        if cache_key in st.session_state:
            cache_data = st.session_state[cache_key]
            if time.time() - cache_data['timestamp'] < cache_data['duration']:
                return cache_data['data']
            else:
                del st.session_state[cache_key]
    except Exception as e:
        logger.warning(f"è·å–ç¼“å­˜å¤±è´¥: {str(e)}")
    return None

@st.cache_resource(show_spinner="è¿æ¥äº‘æ•°æ®åº“...")
def get_google_sheets_client():
    """è·å–Google Sheetså®¢æˆ·ç«¯ - ä½¿ç”¨ç¼“å­˜"""
    try:
        # ç¡®ä¿ `google_sheets` secret å­˜åœ¨ä¸”æœ‰æ•ˆ
        if "google_sheets" not in st.secrets:
            raise ValueError("`st.secrets['google_sheets']` æœªé…ç½®ã€‚è¯·åœ¨ .streamlit/secrets.toml ä¸­æ·»åŠ æœåŠ¡è´¦æˆ·å¯†é’¥ã€‚")

        credentials_info = st.secrets["google_sheets"]
        # SCOPES å®šä¹‰åœ¨æ–‡ä»¶é¡¶éƒ¨
        credentials = Credentials.from_service_account_info(credentials_info, scopes=SCOPES)
        client = gspread.authorize(credentials)
        logger.info("Google Sheetså®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        logger.error(f"Google Sheetså®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        raise SheetOperationError(f"è¿æ¥å¤±è´¥: {str(e)}")

def safe_sheet_operation(operation_func, *args, **kwargs):
    """å®‰å…¨çš„è¡¨æ ¼æ“ä½œ"""
    return retry_operation(operation_func, *args, **kwargs)

# --- ä¿®æ”¹æ­¤å‡½æ•°ä»¥ä½¿ç”¨ TARGET_SPREADSHEET_URL ---
def get_target_spreadsheet(gc):
    """
    é€šè¿‡URLè·å–æŒ‡å®šçš„Google Sheetsè¡¨æ ¼ã€‚
    æ­¤å‡½æ•°æ›¿æ¢äº†åŸå…ˆçš„ get_or_create_spreadsheetï¼Œç¡®ä¿æ“ä½œçš„æ˜¯æŒ‡å®šçš„è¡¨æ ¼ã€‚
    """
    def _operation():
        try:
            # ä½¿ç”¨ open_by_url å°è¯•æ‰“å¼€è¡¨æ ¼
            spreadsheet = gc.open_by_url(TARGET_SPREADSHEET_URL)
            logger.info(f"è¡¨æ ¼ (URL: {TARGET_SPREADSHEET_URL}) å·²æˆåŠŸæ‰“å¼€ã€‚")
            return spreadsheet
        except gspread.exceptions.SpreadsheetNotFound:
            raise SheetOperationError(f"è¡¨æ ¼ (URL: {TARGET_SPREADSHEET_URL}) æœªæ‰¾åˆ°ã€‚è¯·ç¡®è®¤URLæ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠæœåŠ¡è´¦æˆ·æ˜¯å¦æœ‰è®¿é—®æƒé™ã€‚")
        except Exception as e:
            raise SheetOperationError(f"æ‰“å¼€è¡¨æ ¼ (URL: {TARGET_SPREADSHEET_URL}) å¤±è´¥: {str(e)}")

    return safe_sheet_operation(_operation)


def get_or_create_worksheet(spreadsheet, name, rows=1000, cols=20):
    """è·å–æˆ–åˆ›å»ºå·¥ä½œè¡¨ - å¢å¼ºé”™è¯¯å¤„ç†"""
    def _operation():
        try:
            worksheet = spreadsheet.worksheet(name)
            logger.info(f"å·¥ä½œè¡¨ '{name}' å·²å­˜åœ¨")
            return worksheet
        except gspread.WorksheetNotFound:
            logger.info(f"åˆ›å»ºæ–°å·¥ä½œè¡¨ '{name}'")
            worksheet = spreadsheet.add_worksheet(title=name, rows=rows, cols=cols)
            return worksheet

    return safe_sheet_operation(_operation)

def clean_dataframe_for_json(df: pd.DataFrame) -> pd.DataFrame:
    """æ¸…ç†DataFrameä»¥ä¾¿JSONåºåˆ—åŒ–"""
    try:
        df_cleaned = df.copy()

        # å¤„ç†å„ç§æ•°æ®ç±»å‹
        for col in df_cleaned.columns:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†ç‰¹æ®Šå€¼
            df_cleaned[col] = df_cleaned[col].astype(str)
            df_cleaned[col] = df_cleaned[col].replace({
                'nan': '',
                'None': '',
                'NaT': '',
                'null': '',
                '<NA>': ''
            })

            # å¤„ç†è¿‡é•¿çš„å­—ç¬¦ä¸²ï¼ˆé˜²æ­¢å•ä¸ªå•å…ƒæ ¼è¿‡å¤§ï¼‰
            df_cleaned[col] = df_cleaned[col].apply(
                lambda x: x[:1000] + '...' if len(str(x)) > 1000 else x
            )

        logger.info(f"DataFrameæ¸…ç†å®Œæˆ: {len(df_cleaned)} è¡Œ x {len(df_cleaned.columns)} åˆ—")
        return df_cleaned

    except Exception as e:
        logger.error(f"æ¸…ç†DataFrameå¤±è´¥: {str(e)}")
        raise DataProcessingError(f"æ•°æ®æ¸…ç†å¤±è´¥: {str(e)}")

def save_large_data_to_sheets(data_dict: Dict[str, Any], worksheet, batch_size: int = 15) -> bool:
    """åˆ†æ‰¹ä¿å­˜å¤§æ•°æ®åˆ°è¡¨æ ¼"""
    try:
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        all_data = [['é—¨åº—åç§°', 'æŠ¥è¡¨æ•°æ®JSON', 'è¡Œæ•°', 'åˆ—æ•°', 'æ›´æ–°æ—¶é—´', 'åˆ†ç‰‡åºå·', 'æ€»åˆ†ç‰‡æ•°', 'æ•°æ®å“ˆå¸Œ']]

        for store_name, df in data_dict.items():
            try:
                # æ¸…ç†æ•°æ®
                df_cleaned = clean_dataframe_for_json(df)

                # è½¬æ¢ä¸ºJSON
                json_data = df_cleaned.to_json(orient='records', force_ascii=False, ensure_ascii=False)

                # è®¡ç®—æ•°æ®å“ˆå¸Œç”¨äºéªŒè¯
                data_hash = hashlib.md5(json_data.encode('utf-8')).hexdigest()[:16]

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†ç‰‡
                if len(json_data) <= MAX_CHUNK_SIZE:
                    # ä¸éœ€è¦åˆ†ç‰‡
                    all_data.append([
                        store_name,
                        json_data,
                        len(df),
                        len(df.columns),
                        current_time,
                        "1",
                        "1",
                        data_hash
                    ])
                else:
                    # åˆ†ç‰‡å­˜å‚¨
                    chunks = []
                    for i in range(0, len(json_data), MAX_CHUNK_SIZE):
                        chunks.append(json_data[i:i + MAX_CHUNK_SIZE])

                    total_chunks = len(chunks)

                    for idx, chunk in enumerate(chunks):
                        chunk_name = f"{store_name}_åˆ†ç‰‡{idx+1}"
                        all_data.append([
                            chunk_name,
                            chunk,
                            len(df),
                            len(df.columns),
                            current_time,
                            str(idx+1),
                            str(total_chunks),
                            data_hash
                        ])

                logger.info(f"å‡†å¤‡ä¿å­˜ {store_name}: {len(df)} è¡Œæ•°æ®")

            except Exception as e:
                logger.error(f"å¤„ç† {store_name} æ—¶å‡ºé”™: {str(e)}")
                # ä¿å­˜é”™è¯¯ä¿¡æ¯
                error_data = {
                    "error": str(e),
                    "rows": len(df) if 'df' in locals() else 0,
                    "columns": len(df.columns) if 'df' in locals() else 0,
                    "timestamp": current_time
                }
                all_data.append([
                    f"{store_name}_é”™è¯¯",
                    json.dumps(error_data, ensure_ascii=False),
                    0,
                    0,
                    current_time,
                    "1",
                    "1",
                    "ERROR"
                ])
                continue

        # åˆ†æ‰¹ä¸Šä¼ æ•°æ®
        if len(all_data) > 1:
            # First, update the header row and the first batch of data
            # This ensures the header is always correct
            first_batch_data = all_data[1:batch_size+1]
            worksheet.update('A1', [all_data[0]] + first_batch_data)
            time.sleep(0.8)

            # Update remaining batches
            for i in range(batch_size + 1, len(all_data), batch_size):
                batch_data = all_data[i:i+batch_size]
                row_num = i + 1
                worksheet.update(f'A{row_num}', batch_data)

                # APIé™åˆ¶å»¶è¿Ÿ
                time.sleep(0.8)

                # æ˜¾ç¤ºè¿›åº¦
                progress = min(i + batch_size, len(all_data) - 1)
                st.progress(progress / (len(all_data) - 1))

        logger.info(f"æ•°æ®ä¿å­˜å®Œæˆ: {len(all_data) - 1} æ¡è®°å½•")
        return True

    except Exception as e:
        logger.error(f"ä¿å­˜å¤§æ•°æ®å¤±è´¥: {str(e)}")
        raise

def save_reports_to_sheets(reports_dict: Dict[str, pd.DataFrame], gc) -> bool:
    """ä¿å­˜æŠ¥è¡¨æ•°æ® - å¢å¼ºç‰ˆ"""
    with error_handler("ä¿å­˜æŠ¥è¡¨æ•°æ®"):
        def _save_operation():
            # è·å–æŒ‡å®šçš„è¡¨æ ¼
            spreadsheet = get_target_spreadsheet(gc) # <--- ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨æ–°å‡½æ•°
            worksheet = get_or_create_worksheet(spreadsheet, REPORTS_SHEET_NAME, rows=2000, cols=10)

            # æ¸…ç©ºç°æœ‰æ•°æ®
            with st.spinner("æ¸…ç†æ—§æ•°æ®..."):
                worksheet.clear()
                time.sleep(1)

            # ä¿å­˜æ•°æ®
            with st.spinner("ä¿å­˜æ–°æ•°æ®..."):
                success = save_large_data_to_sheets(reports_dict, worksheet)

            if success:
                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                cache_key = get_cache_key("reports", "load")
                if f"cache_{cache_key}" in st.session_state:
                    del st.session_state[f"cache_{cache_key}"]

                logger.info("æŠ¥è¡¨æ•°æ®ä¿å­˜æˆåŠŸ")
                return True
            return False

        return safe_sheet_operation(_save_operation)

def reconstruct_fragmented_data(fragments: List[Dict[str, Any]], store_name: str) -> Optional[pd.DataFrame]:
    """é‡æ„åˆ†ç‰‡æ•°æ®"""
    try:
        if len(fragments) == 1:
            # å•ç‰‡æ•°æ®
            json_data = fragments[0]['json_data']
        else:
            # å¤šç‰‡æ•°æ®éœ€è¦é‡æ„
            fragments.sort(key=lambda x: int(x['chunk_num']))
            json_data = ''.join([frag['json_data'] for frag in fragments])

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        expected_hash = fragments[0].get('data_hash', '')
        if expected_hash and expected_hash != 'ERROR':
            actual_hash = hashlib.md5(json_data.encode('utf-8')).hexdigest()[:16]
            if actual_hash != expected_hash:
                logger.warning(f"{store_name} æ•°æ®å“ˆå¸Œä¸åŒ¹é…ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æŸå")

        # è§£æJSON
        df = pd.read_json(io.StringIO(json_data), orient='records') # ä½¿ç”¨io.StringIOå¤„ç†å­—ç¬¦ä¸²JSON

        # æ•°æ®åå¤„ç†
        if len(df) > 0:
            # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§° (ä¸ºäº†å¤„ç†æŸäº›Excelå¯¼å‡ºå¯èƒ½å¸¦çš„é¢å¤–è¡Œ)
            # è¿™é‡Œçš„é€»è¾‘æ˜¯å‡è®¾excelæ–‡ä»¶çš„ç¬¬ä¸€è¡Œå¯èƒ½æ˜¯é—¨åº—åç§°ï¼Œä¸”åªæœ‰1-2ä¸ªéç©ºå•å…ƒæ ¼
            original_df_rows_count = len(df)
            first_row = df.iloc[0]
            non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')

            if non_empty_count <= 2 and original_df_rows_count > 1: # åªæœ‰å¾ˆå°‘çš„éç©ºå€¼ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜è¡Œ
                df = df.iloc[1:].reset_index(drop=True) # è·³è¿‡ç¬¬ä¸€è¡Œ

        # å¤„ç†è¡¨å¤´
        if len(df) > 0: # ç¡®ä¿DataFrameæœ‰æ•°æ®å†å¤„ç†
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆåˆ—åè¡Œ
            potential_header = df.iloc[0].fillna('').astype(str).tolist()
            # å¦‚æœç¬¬ä¸€è¡Œæœ‰è¶³å¤Ÿçš„éç©ºå€¼ï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯è¡¨å¤´
            if sum(1 for x in potential_header if x.strip() != '') >= 2: # è‡³å°‘æœ‰ä¸¤ä¸ªéç©ºåˆ—å
                header_row = potential_header
                data_rows = df.iloc[1:].copy()
            else: # å¦åˆ™ä½¿ç”¨é»˜è®¤è¡¨å¤´æˆ–è€…è®¤ä¸ºæ•´ä¸ªdfå°±æ˜¯æ•°æ®
                header_row = [f'åˆ—{i+1}' for i in range(len(df.columns))]
                data_rows = df.copy()

            # æ¸…ç†åˆ—åå¹¶å¤„ç†é‡å¤
            cols = []
            for i, col in enumerate(header_row):
                col = str(col).strip()
                if col == '' or col == 'nan' or col == '0':
                    col = f'åˆ—{i+1}' if i < len(data_rows.columns) else f'é¢å¤–åˆ—{i+1}' # é¿å…è¶Šç•Œ

                # å¤„ç†é‡å¤åˆ—å
                original_col = col
                counter = 1
                while col in cols:
                    col = f"{original_col}_{counter}"
                    counter += 1
                cols.append(col)

            # ç¡®ä¿åˆ—æ•°åŒ¹é…ï¼Œé¿å…è®¾ç½®çš„åˆ—åæ•°é‡ä¸å®é™…æ•°æ®åˆ—æ•°ä¸ç¬¦
            if len(cols) != len(data_rows.columns):
                min_cols = min(len(data_rows.columns), len(cols))
                cols = cols[:min_cols]
                data_rows = data_rows.iloc[:, :min_cols] # æˆªæ–­æ•°æ®åˆ—ä»¥åŒ¹é…åˆ—å

            data_rows.columns = cols
            df = data_rows.reset_index(drop=True).fillna('')
        else: # å¦‚æœdfæ˜¯ç©ºçš„
            df = pd.DataFrame() # è¿”å›ç©ºDataFrame

        logger.info(f"{store_name} æ•°æ®é‡æ„æˆåŠŸ: {len(df)} è¡Œ")
        return df

    except Exception as e:
        logger.error(f"é‡æ„ {store_name} æ•°æ®å¤±è´¥: {str(e)}")
        return None

def load_reports_from_sheets(gc) -> Dict[str, pd.DataFrame]:
    """åŠ è½½æŠ¥è¡¨æ•°æ® - ä½¿ç”¨ç¼“å­˜å’Œåˆ†ç‰‡é‡æ„"""
    cache_key = get_cache_key("reports", "load")
    cached_data = get_cache(cache_key)
    if cached_data is not None:
        logger.info("ä»ç¼“å­˜åŠ è½½æŠ¥è¡¨æ•°æ®")
        return cached_data

    with error_handler("åŠ è½½æŠ¥è¡¨æ•°æ®"):
        def _load_operation():
            # è·å–æŒ‡å®šçš„è¡¨æ ¼
            spreadsheet = get_target_spreadsheet(gc) # <--- ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨æ–°å‡½æ•°

            try:
                worksheet = spreadsheet.worksheet(REPORTS_SHEET_NAME)
                data = worksheet.get_all_values()

                if len(data) <= 1:
                    logger.info("æŠ¥è¡¨æ•°æ®ä¸ºç©º")
                    return {}

                # è§£ææ•°æ®
                reports_dict = {}
                fragments_dict = {}  # å­˜å‚¨åˆ†ç‰‡æ•°æ®

                # æ£€æŸ¥å¹¶è·³è¿‡ç©ºè¡Œï¼Œé¿å…ç´¢å¼•é”™è¯¯
                for row_idx, row in enumerate(data[1:]):
                    if not any(cell.strip() for cell in row): # å¦‚æœæ•´è¡Œéƒ½æ˜¯ç©ºçš„ï¼Œè·³è¿‡
                        logger.debug(f"è·³è¿‡ç©ºè¡Œ {row_idx + 2}") # +2 æ˜¯å› ä¸ºè·³è¿‡äº†æ ‡é¢˜è¡Œå’Œ0ç´¢å¼•
                        continue

                    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                    if len(row) < 7: # è‡³å°‘éœ€è¦7åˆ—æ¥è§£æåŸºæœ¬çš„åˆ†ç‰‡ä¿¡æ¯
                        logger.warning(f"è·³è¿‡ä¸å®Œæ•´è¡Œ (è¡Œ {row_idx + 2}): {row}")
                        continue

                    store_name = row[0]
                    json_data = row[1]
                    # rows_count = row[2] # æš‚æ—¶ä¸ä½¿ç”¨ï¼Œå› ä¸ºé‡æ„åä¼šé‡æ–°è®¡ç®—
                    # cols_count = row[3] # æš‚æ—¶ä¸ä½¿ç”¨
                    # update_time = row[4] # æš‚æ—¶ä¸ä½¿ç”¨
                    chunk_num = row[5]
                    total_chunks = row[6]
                    data_hash = row[7] if len(row) > 7 else '' # ç¡®ä¿data_hashå­˜åœ¨

                    # è·³è¿‡é”™è¯¯æ•°æ® (é”™è¯¯æ ‡è®°é€šå¸¸åœ¨ store_name åé¢)
                    if store_name.endswith('_é”™è¯¯'):
                        logger.warning(f"è·³è¿‡é”™è¯¯æ•°æ®: {store_name}")
                        continue

                    # å¤„ç†åˆ†ç‰‡æ•°æ®
                    if '_åˆ†ç‰‡' in store_name:
                        base_name = store_name.split('_åˆ†ç‰‡')[0]
                        if base_name not in fragments_dict:
                            fragments_dict[base_name] = []

                        fragments_dict[base_name].append({
                            'json_data': json_data,
                            'chunk_num': chunk_num,
                            'total_chunks': total_chunks,
                            'data_hash': data_hash
                        })
                    else:
                        # å•ç‰‡æ•°æ®
                        fragments_dict[store_name] = [{
                            'json_data': json_data,
                            'chunk_num': '1',
                            'total_chunks': '1',
                            'data_hash': data_hash
                        }]

                # é‡æ„æ‰€æœ‰åˆ†ç‰‡æ•°æ®
                for store_name, fragments in fragments_dict.items():
                    df = reconstruct_fragmented_data(fragments, store_name)
                    if df is not None and not df.empty: # ç¡®ä¿é‡æ„åä¸æ˜¯ç©ºDataFrame
                        reports_dict[store_name] = df
                    elif df is not None and df.empty:
                        logger.info(f"é—¨åº— '{store_name}' é‡æ„åä¸ºç©ºDataFrameï¼Œè·³è¿‡ã€‚")
                    else:
                        logger.error(f"é—¨åº— '{store_name}' é‡æ„å¤±è´¥ï¼Œè·³è¿‡ã€‚")


                logger.info(f"æŠ¥è¡¨æ•°æ®åŠ è½½æˆåŠŸ: {len(reports_dict)} ä¸ªé—¨åº—")

                # è®¾ç½®ç¼“å­˜
                set_cache(cache_key, reports_dict)
                return reports_dict

            except gspread.WorksheetNotFound:
                logger.info("æŠ¥è¡¨æ•°æ®è¡¨ä¸å­˜åœ¨")
                return {}

        return safe_sheet_operation(_load_operation)

def analyze_receivable_data(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†æåº”æ”¶æœªæ”¶é¢æ•°æ® - ä¸“é—¨æŸ¥æ‰¾ç¬¬69è¡Œ"""
    result = {}

    if len(df.columns) == 0 or len(df) == 0:
        return result

    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§°
    # original_df = df.copy() # ä¸å†éœ€è¦ï¼Œç›´æ¥æ“ä½œdf
    first_row_skipped = False
    first_row = df.iloc[0] if len(df) > 0 else None
    if first_row is not None:
        non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
        # å¦‚æœç¬¬ä¸€è¡Œåªæœ‰1-2ä¸ªéç©ºå€¼ï¼Œä¸”è¡Œæ•°å¤§äº1ï¼Œåˆ™è·³è¿‡ç¬¬ä¸€è¡Œ
        if non_empty_count <= 2 and len(df) > 1:
            df = df.iloc[1:].reset_index(drop=True)
            first_row_skipped = True
            result['skipped_store_name_row'] = True


    # æŸ¥æ‰¾ç¬¬69è¡Œ (0-indexed å¯¹åº” 68)
    # å¦‚æœåŸå§‹excelæœ‰æ ‡é¢˜è¡Œä¸”è¢«è·³è¿‡ï¼Œé‚£ä¹ˆåŸæ¥çš„ç¬¬69è¡Œç°åœ¨æ˜¯ç¬¬68è¡Œ
    target_row_index_in_processed_df = 68 - (1 if first_row_skipped else 0)

    if len(df) > target_row_index_in_processed_df and target_row_index_in_processed_df >= 0: # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
        row = df.iloc[target_row_index_in_processed_df]
        first_col_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""

        # æ£€æŸ¥å…³é”®è¯
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']

        for keyword in keywords:
            if keyword in first_col_value:
                # æŸ¥æ‰¾æ•°å€¼ (ä»å³å‘å·¦æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéç©ºæ•°å€¼)
                for col_idx in range(len(row)-1, 0, -1):
                    val = row.iloc[col_idx]
                    if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                        cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()

                        if cleaned.startswith('(') and cleaned.endswith(')'):
                            cleaned = '-' + cleaned[1:-1]

                        try:
                            amount = float(cleaned)
                            if amount != 0: # æ‰¾åˆ°éé›¶å€¼æ‰è¿”å›
                                result['åº”æ”¶-æœªæ”¶é¢'] = {
                                    'amount': amount,
                                    'column_name': str(df.columns[col_idx]),
                                    'row_name': first_col_value,
                                    'row_index': target_row_index_in_processed_df,
                                    'actual_row_number': target_row_index_in_processed_df + 1 + (1 if first_row_skipped else 0), # æŠ¥å‘ŠåŸå§‹excelè¡Œå·
                                }
                                return result
                        except ValueError:
                            continue # ä¸æ˜¯æœ‰æ•ˆæ•°å­—ï¼Œç»§ç»­æ‰¾
                break # æ‰¾åˆ°å…³é”®è¯ä½†æ²¡æ‰¾åˆ°æœ‰æ•ˆæ•°å­—ï¼Œè·³å‡ºå…³é”®è¯å¾ªç¯

    # å¤‡ç”¨æŸ¥æ‰¾ (å¦‚æœåœ¨ç›®æ ‡è¡Œæ²¡æ‰¾åˆ°ï¼Œåˆ™å…¨è¡¨æŸ¥æ‰¾)
    if 'åº”æ”¶-æœªæ”¶é¢' not in result:
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']

        for idx, row in df.iterrows():
            try:
                row_name = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""

                if not row_name.strip(): # è·³è¿‡ç©ºè¡Œåç§°
                    continue

                for keyword in keywords:
                    if keyword in row_name:
                        for col_idx in range(len(row)-1, 0, -1):
                            val = row.iloc[col_idx]
                            if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                                cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').replace('ï¿¥', '').strip()

                                if cleaned.startswith('(') and cleaned.endswith(')'):
                                    cleaned = '-' + cleaned[1:-1]

                                try:
                                    amount = float(cleaned)
                                    if amount != 0: # æ‰¾åˆ°éé›¶å€¼æ‰è¿”å›
                                        result['åº”æ”¶-æœªæ”¶é¢'] = {
                                            'amount': amount,
                                            'column_name': str(df.columns[col_idx]),
                                            'row_name': row_name,
                                            'row_index': idx,
                                            'actual_row_number': idx + 1 + (1 if first_row_skipped else 0), # æŠ¥å‘ŠåŸå§‹excelè¡Œå·
                                            'note': f'åœ¨åŸå§‹Excelç¬¬{idx + 1 + (1 if first_row_skipped else 0)}è¡Œæ‰¾åˆ°ï¼ˆéé¢„è®¾ç¬¬69è¡Œï¼‰'
                                        }
                                        return result
                                except ValueError:
                                    continue
                        break # æ‰¾åˆ°å…³é”®è¯ä½†æ²¡æ‰¾åˆ°æœ‰æ•ˆæ•°å­—ï¼Œè·³å‡ºå…³é”®è¯å¾ªç¯
            except Exception: # é¿å…å•ä¸ªè¡Œå¤„ç†é”™è¯¯å¯¼è‡´æ•´ä¸ªåˆ†æå¤±è´¥
                continue

    # è°ƒè¯•ä¿¡æ¯
    result['debug_info'] = {
        'total_rows': len(df),
        'first_row_skipped': first_row_skipped,
        'checked_row_69_in_processed_df': len(df) > target_row_index_in_processed_df and target_row_index_in_processed_df >= 0,
        'row_69_content_in_processed_df': str(df.iloc[target_row_index_in_processed_df].iloc[0]) if len(df) > target_row_index_in_processed_df and target_row_index_in_processed_df >= 0 else 'N/A'
    }

    return result

def verify_user_permission(store_name: str, user_id: str, permissions_data: Optional[pd.DataFrame]) -> bool:
    """éªŒè¯ç”¨æˆ·æƒé™"""
    if permissions_data is None or len(permissions_data.columns) < 2:
        return False

    store_col = permissions_data.columns[0]
    id_col = permissions_data.columns[1]

    for _, row in permissions_data.iterrows():
        stored_store = str(row[store_col]).strip()
        stored_id = str(row[id_col]).strip()

        # å¢å¼ºåŒ¹é…é€»è¾‘ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…å’Œå¿½ç•¥å¤§å°å†™ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if (store_name.lower() in stored_store.lower() or stored_store.lower() in store_name.lower()) and stored_id == str(user_id).strip():
            return True

    return False

def find_matching_reports(store_name: str, reports_data: Dict[str, pd.DataFrame]) -> List[str]:
    """æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨"""
    matching = []
    # è½¬æ¢ä¸ºå°å†™è¿›è¡Œä¸æ•æ„ŸåŒ¹é…
    store_name_lower = store_name.lower()
    for sheet_name in reports_data.keys():
        sheet_name_lower = sheet_name.lower()
        if store_name_lower in sheet_name_lower or sheet_name_lower in store_name_lower:
            matching.append(sheet_name)
    return matching

def show_status_message(message: str, status_type: str = "info"):
    """æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯"""
    css_class = f"status-{status_type}"
    st.markdown(f'<div class="{css_class}">{message}</div>', unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
if 'store_name' not in st.session_state:
    st.session_state.store_name = ""
if 'user_id' not in st.session_state:
    st.session_state.user_id = ""
if 'is_admin' not in st.session_state:
    st.session_state.is_admin = False
if 'google_sheets_client' not in st.session_state:
    st.session_state.google_sheets_client = None
if 'operation_status' not in st.session_state:
    st.session_state.operation_status = []

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ“Š é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ</h1>', unsafe_allow_html=True)

# åˆå§‹åŒ–Google Sheetså®¢æˆ·ç«¯
if not st.session_state.google_sheets_client:
    try:
        with st.spinner("è¿æ¥äº‘æ•°æ®åº“..."):
            gc = get_google_sheets_client()
            st.session_state.google_sheets_client = gc
            show_status_message("âœ… äº‘æ•°æ®åº“è¿æ¥æˆåŠŸï¼", "success")
    except Exception as e:
        show_status_message(f"âŒ è¿æ¥å¤±è´¥: {str(e)}", "error")
        st.stop()

gc = st.session_state.google_sheets_client

# æ˜¾ç¤ºæ“ä½œçŠ¶æ€
for status in st.session_state.operation_status:
    show_status_message(status['message'], status['type'])
st.session_state.operation_status = [] # æ¸…é™¤å·²æ˜¾ç¤ºçš„çŠ¶æ€æ¶ˆæ¯

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»ŸåŠŸèƒ½")

    # ç³»ç»ŸçŠ¶æ€
    st.subheader("ğŸ“¡ ç³»ç»ŸçŠ¶æ€")
    if gc:
        st.success("ğŸŸ¢ äº‘æ•°æ®åº“å·²è¿æ¥")
    else:
        st.error("ğŸ”´ äº‘æ•°æ®åº“æ–­å¼€")

    user_type = st.radio("é€‰æ‹©ç”¨æˆ·ç±»å‹", ["æ™®é€šç”¨æˆ·", "ç®¡ç†å‘˜"])

    if user_type == "ç®¡ç†å‘˜":
        st.subheader("ğŸ” ç®¡ç†å‘˜ç™»å½•")
        admin_password = st.text_input("ç®¡ç†å‘˜å¯†ç ", type="password")

        if st.button("éªŒè¯ç®¡ç†å‘˜èº«ä»½"):
            if admin_password == ADMIN_PASSWORD:
                st.session_state.is_admin = True
                show_status_message("âœ… ç®¡ç†å‘˜éªŒè¯æˆåŠŸï¼", "success")
                st.rerun()
            else:
                show_status_message("âŒ å¯†ç é”™è¯¯ï¼", "error")

        if st.session_state.is_admin:
            st.subheader("ğŸ“ æ–‡ä»¶ç®¡ç†")

            # ä¸Šä¼ æƒé™è¡¨
            permissions_file = st.file_uploader("ä¸Šä¼ é—¨åº—æƒé™è¡¨", type=['xlsx', 'xls'], key="permissions_uploader")
            if permissions_file:
                try:
                    with st.spinner("å¤„ç†æƒé™è¡¨æ–‡ä»¶..."):
                        df = pd.read_excel(permissions_file)
                        if len(df.columns) >= 2:
                            with st.spinner("ä¿å­˜åˆ°äº‘ç«¯..."):
                                if save_permissions_to_sheets(df, gc):
                                    show_status_message(f"âœ… æƒé™è¡¨å·²ä¸Šä¼ ï¼š{len(df)} ä¸ªç”¨æˆ·", "success")
                                    st.balloons()
                                else:
                                st.session_state.operation_status.append({"message": "âŒ æƒé™è¡¨ä¿å­˜å¤±è´¥", "type": "error"})
                        else:
                            st.session_state.operation_status.append({"message": "âŒ æ ¼å¼é”™è¯¯ï¼šéœ€è¦è‡³å°‘ä¸¤åˆ—ï¼ˆé—¨åº—åç§°ã€äººå‘˜ç¼–å·ï¼‰", "type": "error"})
                except Exception as e:
                    st.session_state.operation_status.append({"message": f"âŒ å¤„ç†æƒé™è¡¨å¤±è´¥ï¼š{str(e)}", "type": "error"})
                st.rerun() # ä¸Šä¼ ååˆ·æ–°é¡µé¢æ˜¾ç¤ºçŠ¶æ€

            # ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨
            reports_file = st.file_uploader("ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨", type=['xlsx', 'xls'], key="reports_uploader")
            if reports_file:
                try:
                    with st.spinner("å¤„ç†æŠ¥è¡¨æ–‡ä»¶..."):
                        excel_file = pd.ExcelFile(reports_file)
                        reports_dict = {}

                        for sheet in excel_file.sheet_names:
                            try:
                                df = pd.read_excel(reports_file, sheet_name=sheet)
                                if not df.empty:
                                    reports_dict[sheet] = df
                                    logger.info(f"è¯»å–å·¥ä½œè¡¨ '{sheet}': {len(df)} è¡Œ")
                            except Exception as e:
                                logger.warning(f"è·³è¿‡å·¥ä½œè¡¨ '{sheet}': {str(e)}")
                                continue

                        if reports_dict:
                            with st.spinner("ä¿å­˜åˆ°äº‘ç«¯..."):
                                if save_reports_to_sheets(reports_dict, gc):
                                    show_status_message(f"âœ… æŠ¥è¡¨å·²ä¸Šä¼ ï¼š{len(reports_dict)} ä¸ªé—¨åº—", "success")
                                    st.balloons()
                                else:
                                    st.session_state.operation_status.append({"message": "âŒ æŠ¥è¡¨ä¿å­˜å¤±è´¥", "type": "error"})
                        else:
                            st.session_state.operation_status.append({"message": "âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å·¥ä½œè¡¨", "type": "error"})

                except Exception as e:
                    st.session_state.operation_status.append({"message": f"âŒ å¤„ç†æŠ¥è¡¨å¤±è´¥ï¼š{str(e)}", "type": "error"})
                st.rerun() # ä¸Šä¼ ååˆ·æ–°é¡µé¢æ˜¾ç¤ºçŠ¶æ€

            # ç¼“å­˜ç®¡ç†
            st.subheader("ğŸ—‚ï¸ ç¼“å­˜ç®¡ç†")
            if st.button("æ¸…é™¤æ‰€æœ‰ç¼“å­˜", key="clear_cache_button"):
                cache_keys = [key for key in st.session_state.keys() if key.startswith('cache_')]
                for key in cache_keys:
                    del st.session_state[key]
                st.session_state.operation_status.append({"message": "âœ… ç¼“å­˜å·²æ¸…é™¤", "type": "success"})
                st.rerun()
    else:
        if st.session_state.logged_in:
            st.subheader("ğŸ‘¤ å½“å‰ç™»å½•")
            st.info(f"é—¨åº—ï¼š{st.session_state.store_name}")
            st.info(f"ç¼–å·ï¼š{st.session_state.user_id}")

            if st.button("ğŸšª é€€å‡ºç™»å½•", key="logout_button"):
                st.session_state.logged_in = False
                st.session_state.store_name = ""
                st.session_state.user_id = ""
                st.session_state.is_admin = False # é€€å‡ºç™»å½•ä¹Ÿé‡ç½®ç®¡ç†å‘˜çŠ¶æ€
                st.session_state.operation_status.append({"message": "ğŸ‘‹ å·²é€€å‡ºç™»å½•", "type": "success"})
                st.rerun()

# ä¸»ç•Œé¢
if user_type == "ç®¡ç†å‘˜" and st.session_state.is_admin:
    st.markdown('<div class="admin-panel"><h3>ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜æ§åˆ¶é¢æ¿</h3><p>æ•°æ®æ°¸ä¹…ä¿å­˜åœ¨äº‘ç«¯ï¼Œæ”¯æŒåˆ†ç‰‡å­˜å‚¨å’Œç¼“å­˜æœºåˆ¶</p></div>', unsafe_allow_html=True)

    try:
        with st.spinner("åŠ è½½æ•°æ®ç»Ÿè®¡..."):
            permissions_data = load_permissions_from_sheets(gc)
            reports_data = load_reports_from_sheets(gc)

        col1, col2, col3 = st.columns(3)
        with col1:
            perms_count = len(permissions_data) if permissions_data is not None else 0
            st.metric("æƒé™è¡¨ç”¨æˆ·æ•°", perms_count)
        with col2:
            reports_count = len(reports_data)
            st.metric("æŠ¥è¡¨é—¨åº—æ•°", reports_count)
        with col3:
            cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
            st.metric("ç¼“å­˜é¡¹ç›®æ•°", cache_count)

        # æ•°æ®é¢„è§ˆ
        if permissions_data is not None and len(permissions_data) > 0:
            st.subheader("ğŸ‘¥ æƒé™æ•°æ®é¢„è§ˆ")
            st.dataframe(permissions_data.head(10), use_container_width=True)

        if reports_data:
            st.subheader("ğŸ“Š æŠ¥è¡¨æ•°æ®é¢„è§ˆ")
            report_names = list(reports_data.keys())[:5]  # æ˜¾ç¤ºå‰5ä¸ª
            for name in report_names:
                with st.expander(f"ğŸ“‹ {name}"):
                    df = reports_data[name]
                    st.write(f"æ•°æ®è§„æ¨¡: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
                    st.dataframe(df.head(3), use_container_width=True)

    except Exception as e:
        show_status_message(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}", "error")

elif user_type == "ç®¡ç†å‘˜" and not st.session_state.is_admin:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ç®¡ç†å‘˜å¯†ç ")

else: # æ™®é€šç”¨æˆ·ç•Œé¢
    if not st.session_state.logged_in:
        st.subheader("ğŸ” ç”¨æˆ·ç™»å½•")

        try:
            with st.spinner("åŠ è½½æƒé™æ•°æ®..."):
                permissions_data = load_permissions_from_sheets(gc)

            if permissions_data is None:
                st.warning("âš ï¸ ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
            else:
                stores = sorted(permissions_data[permissions_data.columns[0]].unique().tolist())

                with st.form("login_form"):
                    selected_store = st.selectbox("é€‰æ‹©é—¨åº—", stores)
                    user_id = st.text_input("äººå‘˜ç¼–å·")
                    submit = st.form_submit_button("ğŸš€ ç™»å½•")

                    if submit and selected_store and user_id:
                        if verify_user_permission(selected_store, user_id, permissions_data):
                            st.session_state.logged_in = True
                            st.session_state.store_name = selected_store
                            st.session_state.user_id = user_id
                            st.session_state.operation_status.append({"message": "âœ… ç™»å½•æˆåŠŸï¼", "type": "success"})
                            st.balloons()
                            st.rerun()
                        else:
                            st.session_state.operation_status.append({"message": "âŒ é—¨åº—æˆ–ç¼–å·é”™è¯¯ï¼", "type": "error"})
                            st.rerun() # ç™»å½•å¤±è´¥ä¹Ÿåˆ·æ–°ï¼Œæ˜¾ç¤ºé”™è¯¯

        except Exception as e:
            show_status_message(f"âŒ æƒé™éªŒè¯å¤±è´¥ï¼š{str(e)}", "error")

    else:
        # å·²ç™»å½• - æ˜¾ç¤ºæŠ¥è¡¨
        st.markdown(f'<div class="store-info"><h3>ğŸª {st.session_state.store_name}</h3><p>æ“ä½œå‘˜ï¼š{st.session_state.user_id}</p></div>', unsafe_allow_html=True)

        try:
            with st.spinner("åŠ è½½æŠ¥è¡¨æ•°æ®..."):
                reports_data = load_reports_from_sheets(gc)
                matching_sheets = find_matching_reports(st.session_state.store_name, reports_data)

            if matching_sheets:
                if len(matching_sheets) > 1:
                    selected_sheet = st.selectbox("é€‰æ‹©æŠ¥è¡¨", matching_sheets)
                else:
                    selected_sheet = matching_sheets[0]

                df = reports_data[selected_sheet]

                # åº”æ”¶-æœªæ”¶é¢çœ‹æ¿
                st.subheader("ğŸ’° åº”æ”¶-æœªæ”¶é¢")

                try:
                    analysis_results = analyze_receivable_data(df)

                    if 'åº”æ”¶-æœªæ”¶é¢' in analysis_results:
                        data = analysis_results['åº”æ”¶-æœªæ”¶é¢']
                        amount = data['amount']

                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            if amount > 0:
                                st.markdown(f'''
                                    <div class="receivable-positive">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’³ Â¥{amount:,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">é—¨åº—åº”ä»˜æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (åŸå§‹Excelç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)

                            elif amount < 0:
                                st.markdown(f'''
                                    <div class="receivable-negative">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’š Â¥{abs(amount):,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">æ€»éƒ¨åº”é€€æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (åŸå§‹Excelç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)

                            else:
                                st.markdown('''
                                    <div style="background: #e8f5e8; color: #2e7d32; padding: 2rem; border-radius: 15px; text-align: center;">
                                        <h1 style="margin: 0; font-size: 3rem;">âš–ï¸ Â¥0.00</h1>
                                        <h3 style="margin: 0.5rem 0;">æ”¶æ”¯å¹³è¡¡</h3>
                                        <p style="margin: 0;">åº”æ”¶æœªæ”¶é¢ä¸ºé›¶ï¼Œè´¦ç›®å¹³è¡¡</p>
                                    </div>
                                ''', unsafe_allow_html=True)

                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°åº”æ”¶-æœªæ”¶é¢æ•°æ®")

                        with st.expander("ğŸ” æŸ¥çœ‹è¯¦æƒ…", expanded=False):
                            debug_info = analysis_results.get('debug_info', {})

                            st.markdown("### ğŸ“‹ æ•°æ®æŸ¥æ‰¾è¯´æ˜")
                            st.write(f"- **æŠ¥è¡¨æ€»è¡Œæ•°ï¼ˆå¤„ç†åï¼‰ï¼š** {debug_info.get('total_rows', 0)} è¡Œ")
                            st.write(f"- **æ˜¯å¦è·³è¿‡ç¬¬ä¸€è¡Œé—¨åº—åï¼š** {debug_info.get('first_row_skipped', False)}")

                            if debug_info.get('checked_row_69_in_processed_df'):
                                st.write(f"- **å¤„ç†åç¬¬69è¡Œï¼ˆåŸå§‹ç¬¬{debug_info.get('actual_row_number_69', 'N/A')}è¡Œï¼‰å†…å®¹ï¼š** '{debug_info.get('row_69_content_in_processed_df', 'N/A')}'")
                            else:
                                st.write("- **é¢„è®¾åº”æ”¶æœªæ”¶é¢è¡Œï¼ˆåŸå§‹ç¬¬69è¡Œï¼‰ï¼š** æŠ¥è¡¨è¡Œæ•°ä¸è¶³æˆ–å¤„ç†åè¡Œå·æ— æ•ˆ")

                            st.markdown("""
                            ### ğŸ’¡ å¯èƒ½çš„åŸå› 
                            1. æŠ¥è¡¨ä¸­é¢„è®¾çš„ç¬¬69è¡Œï¼ˆæˆ–å…¶å¤„ç†åçš„å¯¹åº”è¡Œï¼‰ä¸åŒ…å«"åº”æ”¶-æœªæ”¶é¢"ç›¸å…³å…³é”®è¯ã€‚
                            2. é¢„è®¾è¡Œä¸­çš„æ•°å€¼ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼ˆå¦‚éæ•°å­—å­—ç¬¦ï¼‰ã€‚
                            3. æŠ¥è¡¨æ•´ä½“æ ¼å¼ä¸ç³»ç»Ÿé¢„æœŸä¸ç¬¦ï¼Œå¯¼è‡´å…³é”®è¡Œè¢«è·³è¿‡æˆ–å®šä½é”™è¯¯ã€‚

                            ### ğŸ› ï¸ å»ºè®®
                            - è¯·ç¡®ä¿åŸå§‹ExcelæŠ¥è¡¨ä¸­çš„"åº”æ”¶-æœªæ”¶é¢"æ•°æ®ä½äº**ç¬¬69è¡Œ**ã€‚
                            - ç¡®è®¤è¯¥è¡Œå¯¹åº”çš„é‡‘é¢æ•°æ®æ˜¯æ¸…æ™°çš„æ•°å­—æ ¼å¼ã€‚
                            - å¦‚éœ€è°ƒæ•´æŸ¥æ‰¾ä½ç½®æˆ–æŠ¥è¡¨è§£æé€»è¾‘ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚
                            """)

                except Exception as e:
                    show_status_message(f"âŒ åˆ†ææ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}", "error")

                st.divider()

                # å®Œæ•´æŠ¥è¡¨æ•°æ®
                st.subheader("ğŸ“‹ å®Œæ•´æŠ¥è¡¨æ•°æ®")

                search_term = st.text_input("ğŸ” æœç´¢æŠ¥è¡¨å†…å®¹", key="report_search_input")

                try:
                    if search_term:
                        search_df = df.copy()
                        for col in search_df.columns:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†å¯èƒ½çš„éå­—ç¬¦ä¸²ç±»å‹ï¼Œç„¶åè¿›è¡Œæœç´¢
                            search_df[col] = search_df[col].astype(str).fillna('')

                        mask = search_df.apply(
                            lambda x: x.str.contains(search_term, case=False, na=False, regex=False)
                        ).any(axis=1)
                        filtered_df = df[mask] # ä½¿ç”¨åŸå§‹dfè¿‡æ»¤ï¼Œé¿å…æ˜¾ç¤ºç±»å‹è½¬æ¢åçš„åˆ—
                        st.info(f"æ‰¾åˆ° {len(filtered_df)} æ¡åŒ…å« '{search_term}' çš„è®°å½•")
                    else:
                        filtered_df = df

                    st.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼šå…± {len(filtered_df)} æ¡è®°å½•ï¼Œ{len(df.columns)} åˆ—")

                    if not filtered_df.empty: # ä½¿ç”¨ .empty åˆ¤æ–­æ˜¯å¦ä¸ºç©º
                        display_df = filtered_df.copy()

                        # ç¡®ä¿åˆ—åå”¯ä¸€ï¼ˆè¿™éƒ¨åˆ†é€»è¾‘åº”åœ¨ reconstruct_fragmented_data ä¸­æ›´å®Œæ•´å¤„ç†ï¼‰
                        # ä½†ä¸ºä¿é™©èµ·è§ï¼Œè¿™é‡Œå†åšä¸€æ¬¡æ£€æŸ¥
                        unique_columns = []
                        for i, col in enumerate(display_df.columns):
                            col_name = str(col)
                            if col_name in unique_columns:
                                col_name = f"{col_name}_{i}"
                            unique_columns.append(col_name)
                        display_df.columns = unique_columns

                        # æ¸…ç†æ•°æ®å†…å®¹ï¼Œç¡®ä¿æ˜¾ç¤ºæ—¶éƒ½æ˜¯å­—ç¬¦ä¸²
                        for col in display_df.columns:
                            display_df[col] = display_df[col].astype(str).fillna('')

                        st.dataframe(display_df, use_container_width=True, height=400)

                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")

                except Exception as e:
                    show_status_message(f"âŒ æ•°æ®å¤„ç†æ—¶å‡ºé”™ï¼š{str(e)}", "error")

                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ æ•°æ®ä¸‹è½½")

                col1, col2 = st.columns(2)
                with col1:
                    try:
                        buffer = io.BytesIO()
                        download_df = df.copy()

                        # ç¡®ä¿åˆ—åå”¯ä¸€ï¼Œè¿™é‡Œå†æ¬¡å¤„ç†ä»¥é˜²ä¸‡ä¸€
                        unique_cols = []
                        for i, col in enumerate(download_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        download_df.columns = unique_cols

                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            download_df.to_excel(writer, index=False)

                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥è¡¨ (Excel)",
                            buffer.getvalue(),
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.xlsx",
                            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            key="download_excel_button"
                        )
                    except Exception as e:
                        show_status_message(f"Excelä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")

                with col2:
                    try:
                        csv_df = df.copy()
                        unique_cols = []
                        for i, col in enumerate(csv_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        csv_df.columns = unique_cols

                        # ensure_ascii=False for Chinese characters
                        csv = csv_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                            csv,
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.csv",
                            "text/csv",
                            key="download_csv_button"
                        )
                    except Exception as e:
                        show_status_message(f"CSVä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")

            else:
                st.error(f"âŒ æœªæ‰¾åˆ°é—¨åº— '{st.session_state.store_name}' çš„æŠ¥è¡¨")

        except Exception as e:
            show_status_message(f"âŒ æŠ¥è¡¨åŠ è½½å¤±è´¥ï¼š{str(e)}", "error")

# é¡µé¢åº•éƒ¨çŠ¶æ€ä¿¡æ¯
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"ğŸ•’ å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
with col2:
    cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
    st.caption(f"ğŸ’¾ ç¼“å­˜é¡¹ç›®: {cache_count}")
with col3:
    st.caption("ğŸ”§ ç‰ˆæœ¬: v2.0 (ç¨³å®šç‰ˆ) - å­˜å‚¨åœ¨ä¸ªäººDrive")
