import streamlit as st
import pandas as pd
import io
import json
from datetime import datetime, timedelta
import time
import gspread
from google.oauth2.service_account import Credentials
import logging
from typing import Optional, Dict, Any, List
import hashlib
import pickle
import traceback
import zipfile
import base64
import threading
from contextlib import contextmanager
from queue import Queue
import math

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ", 
    page_icon="ğŸ“Š",
    layout="wide"
)

# ä¼˜åŒ–åçš„ç³»ç»Ÿé…ç½®
ADMIN_PASSWORD = "admin123"
PERMISSIONS_SHEET_NAME = "store_permissions"
REPORTS_SHEET_NAME = "store_reports"
SYSTEM_INFO_SHEET_NAME = "system_info"

# APIé™åˆ¶ä¼˜åŒ–é…ç½®
MAX_REQUESTS_PER_MINUTE = 80  # å®‰å…¨é…é¢ï¼ˆä½äº100çš„é™åˆ¶ï¼‰
BATCH_SIZE = 1000  # Google Sheetsæœ€å¤§æ‰¹é‡å¤§å°
MIN_REQUEST_INTERVAL = 0.8  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
API_RETRY_TIMES = 3  # APIå¤±è´¥é‡è¯•æ¬¡æ•°
API_BACKOFF_FACTOR = 2  # é€€é¿å› å­

# å­˜å‚¨ä¼˜åŒ–é…ç½® - ç§»é™¤å‹ç¼©ä»¥å‡å°‘å¤æ‚æ€§
ENABLE_COMPRESSION = False  # å…³é—­å‹ç¼©ï¼Œå‡å°‘å­˜å‚¨æ“ä½œ
MAX_SINGLE_CELL_SIZE = 40000  # å‡å°å•å…ƒæ ¼æœ€å¤§å­—ç¬¦æ•°

# æ•°æ®æ¸…ç†é…ç½® - å®Œå…¨ç¦ç”¨å¤‡ä»½
AUTO_BACKUP_BEFORE_CLEAR = False  # ç¦ç”¨è‡ªåŠ¨å¤‡ä»½
BACKUP_RETENTION_MONTHS = 0  # ä¸ä¿ç•™å¤‡ä»½

class APIRateLimiter:
    """APIé€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self):
        self.request_times = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ä»¥é¿å…è¶…è¿‡APIé™åˆ¶"""
        with self.lock:
            current_time = time.time()
            
            # æ¸…ç†1åˆ†é’Ÿå‰çš„è®°å½•
            cutoff_time = current_time - 60
            self.request_times = [t for t in self.request_times if t > cutoff_time]
            
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘é™åˆ¶
            if len(self.request_times) >= MAX_REQUESTS_PER_MINUTE:
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                oldest_request = min(self.request_times)
                wait_time = 61 - (current_time - oldest_request)
                if wait_time > 0:
                    logger.info(f"APIé™åˆ¶ä¿æŠ¤ï¼šç­‰å¾… {wait_time:.1f} ç§’")
                    time.sleep(wait_time)
            
            # è®°å½•æœ¬æ¬¡è¯·æ±‚
            self.request_times.append(current_time)
            
            # åŸºç¡€é—´éš”ä¿æŠ¤
            time.sleep(MIN_REQUEST_INTERVAL)

# å…¨å±€APIé™åˆ¶å™¨
api_limiter = APIRateLimiter()

class SimpleDataManager:
    """ç®€åŒ–çš„æ•°æ®ç®¡ç†å™¨ - æ— å¤‡ä»½ï¼Œæœ€å°å­˜å‚¨"""
    
    def __init__(self, gc):
        self.gc = gc
        self.spreadsheet = None
        self._init_spreadsheet()
    
    def _init_spreadsheet(self):
        """åˆå§‹åŒ–è¡¨æ ¼"""
        try:
            api_limiter.wait_if_needed()
            self.spreadsheet = self.gc.open("é—¨åº—æŠ¥è¡¨ç³»ç»Ÿæ•°æ®")
        except gspread.SpreadsheetNotFound:
            api_limiter.wait_if_needed()
            self.spreadsheet = self.gc.create("é—¨åº—æŠ¥è¡¨ç³»ç»Ÿæ•°æ®")
            # ç§»é™¤è‡ªåŠ¨å…±äº«ï¼Œå‡å°‘æƒé™æ“ä½œ
    
    def _safe_api_call(self, func, *args, **kwargs):
        """å®‰å…¨çš„APIè°ƒç”¨ï¼ŒåŒ…å«é‡è¯•å’Œé™æµ"""
        for attempt in range(API_RETRY_TIMES):
            try:
                api_limiter.wait_if_needed()
                return func(*args, **kwargs)
            except Exception as e:
                if attempt == API_RETRY_TIMES - 1:
                    raise
                wait_time = API_BACKOFF_FACTOR ** attempt
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•: {str(e)}")
                time.sleep(wait_time)
    
    def get_current_data_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ•°æ®ä¿¡æ¯"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æŠ¥è¡¨æ•°æ®è¡¨
            try:
                reports_ws = self._safe_api_call(self.spreadsheet.worksheet, REPORTS_SHEET_NAME)
                data = self._safe_api_call(reports_ws.get_all_values)
                
                if len(data) <= 1:
                    return {"has_data": False, "last_update": None, "store_count": 0}
                
                # ç®€å•ç»Ÿè®¡æ•°æ®
                store_count = len(data) - 1  # å‡å»è¡¨å¤´
                last_update = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                data_month = datetime.now().strftime("%Y-%m")
                
                return {
                    "has_data": True,
                    "last_update": last_update,
                    "store_count": store_count,
                    "data_month": data_month,
                    "total_rows": sum(int(row[2]) if len(row) > 2 and row[2].isdigit() else 0 for row in data[1:])
                }
                
            except gspread.WorksheetNotFound:
                return {"has_data": False, "last_update": None, "store_count": 0}
            
        except Exception as e:
            logger.error(f"è·å–å½“å‰æ•°æ®ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"has_data": False, "last_update": None, "store_count": 0}
    
    def clear_all_report_data(self) -> bool:
        """ç®€å•æ¸…ç©ºæ‰€æœ‰æŠ¥è¡¨æ•°æ® - æ— å¤‡ä»½"""
        try:
            with st.spinner("ğŸ—‘ï¸ æ­£åœ¨æ¸…ç†æ•°æ®..."):
                # 1. åˆ é™¤æŠ¥è¡¨æ•°æ®è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    reports_ws = self._safe_api_call(self.spreadsheet.worksheet, REPORTS_SHEET_NAME)
                    self._safe_api_call(self.spreadsheet.del_worksheet, reports_ws)
                    logger.info("å·²åˆ é™¤æŠ¥è¡¨æ•°æ®è¡¨")
                except gspread.WorksheetNotFound:
                    logger.info("æŠ¥è¡¨æ•°æ®è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
                
                # 2. åˆ é™¤ç³»ç»Ÿä¿¡æ¯è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    info_ws = self._safe_api_call(self.spreadsheet.worksheet, SYSTEM_INFO_SHEET_NAME)
                    self._safe_api_call(self.spreadsheet.del_worksheet, info_ws)
                    logger.info("å·²åˆ é™¤ç³»ç»Ÿä¿¡æ¯è¡¨")
                except gspread.WorksheetNotFound:
                    logger.info("ç³»ç»Ÿä¿¡æ¯è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
                
                # 3. æ¸…ç†ç¼“å­˜
                self._clear_all_cache()
                
                logger.info("æ•°æ®æ¸…ç†å®Œæˆ")
                return True
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def _clear_all_cache(self):
        """æ¸…ç†æ‰€æœ‰ç›¸å…³ç¼“å­˜"""
        cache_keys = [key for key in st.session_state.keys() if 'cache_' in key]
        for key in cache_keys:
            del st.session_state[key]
        logger.info(f"å·²æ¸…ç† {len(cache_keys)} ä¸ªç¼“å­˜é¡¹")
    
    def save_reports_simple(self, reports_dict: Dict[str, pd.DataFrame]) -> bool:
        """ç®€åŒ–çš„æŠ¥è¡¨ä¿å­˜ - æ— å‹ç¼©ï¼Œæ— å¤‡ä»½ï¼Œç›´æ¥æ›¿æ¢"""
        try:
            # 1. æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            if not reports_dict:
                st.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
                return False
            
            total_stores = len(reports_dict)
            total_rows = sum(len(df) for df in reports_dict.values())
            
            st.info(f"ğŸ“Š å‡†å¤‡ä¿å­˜ï¼š{total_stores} ä¸ªé—¨åº—ï¼Œ{total_rows:,} è¡Œæ•°æ®")
            
            # 2. æ¸…ç©ºç°æœ‰æ•°æ®
            st.warning("âš ï¸ æ­£åœ¨æ¸…ç©ºç°æœ‰æ•°æ®...")
            if not self.clear_all_report_data():
                st.error("âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥")
                return False
            
            # 3. åˆ›å»ºæ–°çš„æŠ¥è¡¨å·¥ä½œè¡¨
            with st.spinner("ğŸ“ åˆ›å»ºæ–°æ•°æ®è¡¨..."):
                reports_ws = self._safe_api_call(self.spreadsheet.add_worksheet, 
                                               title=REPORTS_SHEET_NAME, rows=max(2000, total_stores + 100), cols=8)
            
            # 4. å‡†å¤‡æ•°æ® - ç®€åŒ–ç‰ˆæœ¬
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            current_month = datetime.now().strftime("%Y-%m")
            
            headers = ["é—¨åº—åç§°", "æŠ¥è¡¨æ•°æ®", "è¡Œæ•°", "åˆ—æ•°", "æ›´æ–°æ—¶é—´", "æ•°æ®æœˆä»½"]
            all_data = [headers]
            
            # 5. å¤„ç†æ¯ä¸ªé—¨åº—æ•°æ® - ç®€åŒ–å¤„ç†
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, (store_name, df) in enumerate(reports_dict.items()):
                try:
                    # æ›´æ–°è¿›åº¦
                    progress = (idx + 1) / total_stores
                    progress_bar.progress(progress)
                    status_text.text(f"å¤„ç†ä¸­: {store_name} ({idx + 1}/{total_stores})")
                    
                    # ç®€å•æ¸…ç†æ•°æ®
                    df_cleaned = self._clean_dataframe_simple(df)
                    
                    # è½¬æ¢ä¸ºJSON - æ— å‹ç¼©
                    json_data = df_cleaned.to_json(orient='records', force_ascii=False)
                    
                    # æ£€æŸ¥æ•°æ®å¤§å° - å¦‚æœå¤ªå¤§å°±æˆªæ–­
                    if len(json_data) > MAX_SINGLE_CELL_SIZE:
                        logger.warning(f"{store_name} æ•°æ®è¿‡å¤§ï¼Œæˆªæ–­è‡³{MAX_SINGLE_CELL_SIZE}å­—ç¬¦")
                        json_data = json_data[:MAX_SINGLE_CELL_SIZE-100] + '...[æ•°æ®æˆªæ–­]'
                    
                    # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨
                    all_data.append([
                        store_name,
                        json_data,
                        len(df),
                        len(df.columns),
                        current_time,
                        current_month
                    ])
                    
                    logger.info(f"âœ… {store_name}: {len(df)}è¡Œ")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç† {store_name} å¤±è´¥: {str(e)}")
                    # æ·»åŠ é”™è¯¯è®°å½•
                    all_data.append([
                        f"{store_name}_é”™è¯¯",
                        f"å¤„ç†å¤±è´¥: {str(e)}",
                        0, 0, current_time, current_month
                    ])
                    continue
            
            # 6. æ‰¹é‡å†™å…¥æ•°æ® - æ›´å¤§æ‰¹æ¬¡
            with st.spinner("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°äº‘ç«¯..."):
                batch_size = 100  # æ›´å¤§æ‰¹æ¬¡ï¼Œå‡å°‘APIè°ƒç”¨
                total_batches = math.ceil(len(all_data) / batch_size)
                
                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, len(all_data))
                    batch_data = all_data[start_idx:end_idx]
                    
                    if batch_idx == 0:
                        # ç¬¬ä¸€æ‰¹åŒ…å«è¡¨å¤´
                        start_cell = 'A1'
                    else:
                        # åç»­æ‰¹æ¬¡
                        start_cell = f'A{start_idx + 1}'
                    
                    self._safe_api_call(reports_ws.update, start_cell, batch_data)
                    
                    # æ›´æ–°è¿›åº¦
                    batch_progress = (batch_idx + 1) / total_batches
                    progress_bar.progress(batch_progress)
                    status_text.text(f"ä¿å­˜ä¸­: æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            
            # 7. æ¸…ç†è¿›åº¦æ˜¾ç¤º
            progress_bar.empty()
            status_text.empty()
            
            logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {total_stores} ä¸ªé—¨åº—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def load_reports_simple(self) -> Dict[str, pd.DataFrame]:
        """ç®€åŒ–çš„æŠ¥è¡¨åŠ è½½"""
        try:
            # è·å–æŠ¥è¡¨å·¥ä½œè¡¨
            try:
                reports_ws = self._safe_api_call(self.spreadsheet.worksheet, REPORTS_SHEET_NAME)
            except gspread.WorksheetNotFound:
                logger.info("æŠ¥è¡¨å·¥ä½œè¡¨ä¸å­˜åœ¨")
                return {}
            
            # è¯»å–æ•°æ®
            data = self._safe_api_call(reports_ws.get_all_values)
            
            if len(data) <= 1:
                logger.info("æŠ¥è¡¨å·¥ä½œè¡¨ä¸ºç©º")
                return {}
            
            # è§£ææ•°æ®
            reports_dict = {}
            
            for row in data[1:]:  # è·³è¿‡è¡¨å¤´
                if len(row) >= 6:
                    store_name = row[0]
                    json_data = row[1]
                    
                    # è·³è¿‡é”™è¯¯æ•°æ®
                    if store_name.endswith('_é”™è¯¯'):
                        logger.warning(f"è·³è¿‡é”™è¯¯æ•°æ®: {store_name}")
                        continue
                    
                    try:
                        # ç›´æ¥è§£æJSONï¼ˆæ— è§£å‹ç¼©ï¼‰
                        df = pd.read_json(json_data, orient='records')
                        
                        # æ•°æ®åå¤„ç†
                        df = self._process_loaded_dataframe(df)
                        
                        reports_dict[store_name] = df
                        logger.info(f"âœ… åŠ è½½ {store_name}: {len(df)} è¡Œ")
                        
                    except Exception as e:
                        logger.error(f"âŒ è§£æ {store_name} æ•°æ®å¤±è´¥: {str(e)}")
                        continue
            
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(reports_dict)} ä¸ªé—¨åº—")
            return reports_dict
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def _clean_dataframe_simple(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç®€åŒ–çš„DataFrameæ¸…ç†"""
        try:
            df_cleaned = df.copy()
            
            # é™åˆ¶æ•°æ®é‡
            if len(df_cleaned) > 2000:  # å¤§å¹…å‡å°‘è¡Œæ•°é™åˆ¶
                logger.warning(f"æ•°æ®è¡Œæ•°è¿‡å¤š({len(df_cleaned)})ï¼Œæˆªå–å‰2000è¡Œ")
                df_cleaned = df_cleaned.head(2000)
            
            # é™åˆ¶åˆ—æ•°
            if len(df_cleaned.columns) > 50:
                logger.warning(f"æ•°æ®åˆ—æ•°è¿‡å¤š({len(df_cleaned.columns)})ï¼Œæˆªå–å‰50åˆ—")
                df_cleaned = df_cleaned.iloc[:, :50]
            
            # ç®€å•å¤„ç†æ•°æ®ç±»å‹
            for col in df_cleaned.columns:
                df_cleaned[col] = df_cleaned[col].astype(str)
                df_cleaned[col] = df_cleaned[col].replace({
                    'nan': '', 'None': '', 'NaT': '', 'null': '', '<NA>': ''
                })
                # å¤§å¹…é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                df_cleaned[col] = df_cleaned[col].apply(
                    lambda x: x[:100] + '...' if len(str(x)) > 100 else x
                )
            
            return df_cleaned
            
        except Exception as e:
            logger.error(f"æ¸…ç†DataFrameå¤±è´¥: {str(e)}")
            return df
    
    def _process_loaded_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†åŠ è½½çš„DataFrame"""
        try:
            if len(df) == 0:
                return df
            
            # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§°è¡Œ
            first_row = df.iloc[0]
            non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
            
            if non_empty_count <= 2 and len(df) > 1:
                df = df.iloc[1:].reset_index(drop=True)
            
            # å¤„ç†è¡¨å¤´
            if len(df) > 1:
                header_row = df.iloc[0].fillna('').astype(str).tolist()
                data_rows = df.iloc[1:].copy()
                
                # æ¸…ç†åˆ—å
                cols = []
                for i, col in enumerate(header_row):
                    col = str(col).strip()
                    if col == '' or col == 'nan' or col == '0':
                        col = f'åˆ—{i+1}' if i > 0 else 'é¡¹ç›®åç§°'
                    
                    # å¤„ç†é‡å¤åˆ—å
                    original_col = col
                    counter = 1
                    while col in cols:
                        col = f"{original_col}_{counter}"
                        counter += 1
                    cols.append(col)
                
                # ç¡®ä¿åˆ—æ•°åŒ¹é…
                min_cols = min(len(data_rows.columns), len(cols))
                cols = cols[:min_cols]
                data_rows = data_rows.iloc[:, :min_cols]
                
                data_rows.columns = cols
                df = data_rows.reset_index(drop=True).fillna('')
            else:
                df = df.fillna('')
                default_cols = []
                for i in range(len(df.columns)):
                    col_name = f'åˆ—{i+1}' if i > 0 else 'é¡¹ç›®åç§°'
                    default_cols.append(col_name)
                df.columns = default_cols
            
            return df
            
        except Exception as e:
            logger.error(f"å¤„ç†DataFrameå¤±è´¥: {str(e)}")
            return dfized(self, reports_dict: Dict[str, pd.DataFrame], clear_existing: bool = True) -> bool:
        """ä¼˜åŒ–çš„æŠ¥è¡¨ä¿å­˜ - æ”¯æŒå®Œå…¨è½®æ›¿"""
        try:
            # 1. æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
            if not reports_dict:
                st.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
                return False
            
            total_stores = len(reports_dict)
            total_rows = sum(len(df) for df in reports_dict.values())
            
            st.info(f"ğŸ“Š å‡†å¤‡ä¿å­˜ï¼š{total_stores} ä¸ªé—¨åº—ï¼Œ{total_rows:,} è¡Œæ•°æ®")
            
            # 2. å¦‚æœéœ€è¦ï¼Œæ¸…ç©ºç°æœ‰æ•°æ®
            if clear_existing:
                st.warning("âš ï¸ å³å°†æ¸…ç©ºæ‰€æœ‰ç°æœ‰æ•°æ®")
                if not self.clear_all_report_data(create_backup=True):
                    st.error("âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥")
                    return False
            
            # 3. åˆ›å»ºæ–°çš„æŠ¥è¡¨å·¥ä½œè¡¨
            with st.spinner("ğŸ“ åˆ›å»ºæ–°æ•°æ®è¡¨..."):
                reports_ws = self._safe_api_call(self.spreadsheet.add_worksheet, 
                                               title=REPORTS_SHEET_NAME, rows=5000, cols=10)
            
            # 4. å‡†å¤‡æ•°æ®
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            current_month = datetime.now().strftime("%Y-%m")
            
            headers = ["é—¨åº—åç§°", "æŠ¥è¡¨æ•°æ®", "è¡Œæ•°", "åˆ—æ•°", "æ›´æ–°æ—¶é—´", "æ•°æ®æœˆä»½", "å‹ç¼©çŠ¶æ€", "æ•°æ®å“ˆå¸Œ"]
            all_data = [headers]
            
            # 5. å¤„ç†æ¯ä¸ªé—¨åº—æ•°æ®
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, (store_name, df) in enumerate(reports_dict.items()):
                try:
                    # æ›´æ–°è¿›åº¦
                    progress = (idx + 1) / total_stores
                    progress_bar.progress(progress)
                    status_text.text(f"å¤„ç†ä¸­: {store_name} ({idx + 1}/{total_stores})")
                    
                    # æ¸…ç†æ•°æ®
                    df_cleaned = self._clean_dataframe_for_json(df)
                    
                    # è½¬æ¢ä¸ºJSON
                    json_data = df_cleaned.to_json(orient='records', force_ascii=False, ensure_ascii=False)
                    
                    # å‹ç¼©æ•°æ®
                    compressed_data = self.compressor.compress_data(json_data)
                    is_compressed = compressed_data.startswith("COMPRESSED:")
                    
                    # æ£€æŸ¥æ•°æ®å¤§å°
                    if len(compressed_data) > MAX_SINGLE_CELL_SIZE:
                        # å¦‚æœå‹ç¼©åä»ç„¶å¤ªå¤§ï¼Œæˆªæ–­æ•°æ®
                        logger.warning(f"{store_name} æ•°æ®è¿‡å¤§ï¼Œè¿›è¡Œæˆªæ–­")
                        compressed_data = compressed_data[:MAX_SINGLE_CELL_SIZE] + "...[æˆªæ–­]"
                    
                    # è®¡ç®—å“ˆå¸Œ
                    data_hash = hashlib.md5(json_data.encode('utf-8')).hexdigest()[:16]
                    
                    # æ·»åŠ åˆ°æ•°æ®åˆ—è¡¨
                    all_data.append([
                        store_name,
                        compressed_data,
                        len(df),
                        len(df.columns),
                        current_time,
                        current_month,
                        str(is_compressed),
                        data_hash
                    ])
                    
                    logger.info(f"âœ… {store_name}: {len(df)}è¡Œ, å‹ç¼©: {is_compressed}")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç† {store_name} å¤±è´¥: {str(e)}")
                    # æ·»åŠ é”™è¯¯è®°å½•
                    all_data.append([
                        f"{store_name}_é”™è¯¯",
                        f"å¤„ç†å¤±è´¥: {str(e)}",
                        0, 0, current_time, current_month, "False", "ERROR"
                    ])
                    continue
            
            # 6. æ‰¹é‡å†™å…¥æ•°æ®
            with st.spinner("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°äº‘ç«¯..."):
                # åˆ†æ‰¹å†™å…¥ï¼Œé¿å…å•æ¬¡å†™å…¥è¿‡å¤šæ•°æ®
                batch_size = 50  # æ¯æ‰¹50è¡Œ
                total_batches = math.ceil(len(all_data) / batch_size)
                
                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, len(all_data))
                    batch_data = all_data[start_idx:end_idx]
                    
                    if batch_idx == 0:
                        # ç¬¬ä¸€æ‰¹åŒ…å«è¡¨å¤´
                        start_cell = 'A1'
                    else:
                        # åç»­æ‰¹æ¬¡
                        start_cell = f'A{start_idx + 1}'
                    
                    self._safe_api_call(reports_ws.update, start_cell, batch_data)
                    
                    # æ›´æ–°è¿›åº¦
                    batch_progress = (batch_idx + 1) / total_batches
                    progress_bar.progress(batch_progress)
                    status_text.text(f"ä¿å­˜ä¸­: æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            
            # 7. æ›´æ–°ç³»ç»Ÿä¿¡æ¯
            self.update_system_info(total_stores, total_rows, current_month)
            
            # 8. æ¸…ç†è¿›åº¦æ˜¾ç¤º
            progress_bar.empty()
            status_text.empty()
            
            logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {total_stores} ä¸ªé—¨åº—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
            st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def load_reports_optimized(self) -> Dict[str, pd.DataFrame]:
        """ä¼˜åŒ–çš„æŠ¥è¡¨åŠ è½½"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            info = self.get_current_data_info()
            if not info["has_data"]:
                logger.info("ç³»ç»Ÿä¸­æ²¡æœ‰æ•°æ®")
                return {}
            
            # è·å–æŠ¥è¡¨å·¥ä½œè¡¨
            try:
                reports_ws = self._safe_api_call(self.spreadsheet.worksheet, REPORTS_SHEET_NAME)
            except gspread.WorksheetNotFound:
                logger.info("æŠ¥è¡¨å·¥ä½œè¡¨ä¸å­˜åœ¨")
                return {}
            
            # è¯»å–æ•°æ®
            data = self._safe_api_call(reports_ws.get_all_values)
            
            if len(data) <= 1:
                logger.info("æŠ¥è¡¨å·¥ä½œè¡¨ä¸ºç©º")
                return {}
            
            # è§£ææ•°æ®
            reports_dict = {}
            
            for row in data[1:]:  # è·³è¿‡è¡¨å¤´
                if len(row) >= 8:
                    store_name = row[0]
                    json_data = row[1]
                    data_hash = row[7] if len(row) > 7 else ''
                    is_compressed = len(row) > 6 and row[6] == 'True'
                    
                    # è·³è¿‡é”™è¯¯æ•°æ®
                    if store_name.endswith('_é”™è¯¯'):
                        logger.warning(f"è·³è¿‡é”™è¯¯æ•°æ®: {store_name}")
                        continue
                    
                    try:
                        # è§£å‹ç¼©æ•°æ®
                        if is_compressed:
                            json_data = self.compressor.decompress_data(json_data)
                        
                        # éªŒè¯æ•°æ®å®Œæ•´æ€§
                        if data_hash and data_hash != 'ERROR':
                            actual_hash = hashlib.md5(json_data.encode('utf-8')).hexdigest()[:16]
                            if actual_hash != data_hash:
                                logger.warning(f"{store_name} æ•°æ®å“ˆå¸Œä¸åŒ¹é…ï¼Œå¯èƒ½å­˜åœ¨æŸå")
                        
                        # è§£æJSON
                        df = pd.read_json(json_data, orient='records')
                        
                        # æ•°æ®åå¤„ç†
                        df = self._process_loaded_dataframe(df)
                        
                        reports_dict[store_name] = df
                        logger.info(f"âœ… åŠ è½½ {store_name}: {len(df)} è¡Œ")
                        
                    except Exception as e:
                        logger.error(f"âŒ è§£æ {store_name} æ•°æ®å¤±è´¥: {str(e)}")
                        continue
            
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(reports_dict)} ä¸ªé—¨åº—")
            return reports_dict
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
            return {}
    
    def _clean_dataframe_for_json(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†DataFrameç”¨äºJSONåºåˆ—åŒ–"""
        try:
            df_cleaned = df.copy()
            
            # å¤„ç†æ•°æ®ç±»å‹
            for col in df_cleaned.columns:
                df_cleaned[col] = df_cleaned[col].astype(str)
                df_cleaned[col] = df_cleaned[col].replace({
                    'nan': '', 'None': '', 'NaT': '', 'null': '', '<NA>': ''
                })
                # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦ï¼Œé˜²æ­¢å•å…ƒæ ¼è¿‡å¤§
                df_cleaned[col] = df_cleaned[col].apply(
                    lambda x: x[:200] + '...' if len(str(x)) > 200 else x
                )
            
            # é™åˆ¶è¡Œæ•°ï¼Œé˜²æ­¢æ•°æ®è¿‡å¤§
            if len(df_cleaned) > 5000:
                logger.warning(f"æ•°æ®è¡Œæ•°è¿‡å¤š({len(df_cleaned)})ï¼Œæˆªå–å‰5000è¡Œ")
                df_cleaned = df_cleaned.head(5000)
            
            return df_cleaned
            
        except Exception as e:
            logger.error(f"æ¸…ç†DataFrameå¤±è´¥: {str(e)}")
            return df
    
    def _process_loaded_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†åŠ è½½çš„DataFrame"""
        try:
            if len(df) == 0:
                return df
            
            # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§°è¡Œ
            first_row = df.iloc[0]
            non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
            
            if non_empty_count <= 2 and len(df) > 1:
                df = df.iloc[1:].reset_index(drop=True)
            
            # å¤„ç†è¡¨å¤´
            if len(df) > 1:
                header_row = df.iloc[0].fillna('').astype(str).tolist()
                data_rows = df.iloc[1:].copy()
                
                # æ¸…ç†åˆ—å
                cols = []
                for i, col in enumerate(header_row):
                    col = str(col).strip()
                    if col == '' or col == 'nan' or col == '0':
                        col = f'åˆ—{i+1}' if i > 0 else 'é¡¹ç›®åç§°'
                    
                    # å¤„ç†é‡å¤åˆ—å
                    original_col = col
                    counter = 1
                    while col in cols:
                        col = f"{original_col}_{counter}"
                        counter += 1
                    cols.append(col)
                
                # ç¡®ä¿åˆ—æ•°åŒ¹é…
                min_cols = min(len(data_rows.columns), len(cols))
                cols = cols[:min_cols]
                data_rows = data_rows.iloc[:, :min_cols]
                
                data_rows.columns = cols
                df = data_rows.reset_index(drop=True).fillna('')
            else:
                df = df.fillna('')
                default_cols = []
                for i in range(len(df.columns)):
                    col_name = f'åˆ—{i+1}' if i > 0 else 'é¡¹ç›®åç§°'
                    default_cols.append(col_name)
                df.columns = default_cols
            
            return df
            
        except Exception as e:
            logger.error(f"å¤„ç†DataFrameå¤±è´¥: {str(e)}")
            return df

# æƒé™ç®¡ç†å‡½æ•° - ç®€åŒ–ç‰ˆ
def save_permissions_to_sheets(df: pd.DataFrame, gc) -> bool:
    """ä¿å­˜æƒé™æ•°æ® - ç®€åŒ–ç‰ˆ"""
    try:
        data_manager = SimpleDataManager(gc)
        
        # è·å–æˆ–åˆ›å»ºæƒé™è¡¨
        try:
            worksheet = data_manager._safe_api_call(data_manager.spreadsheet.worksheet, PERMISSIONS_SHEET_NAME)
            # ç›´æ¥æ¸…ç©ºç°æœ‰æ•°æ®
            data_manager._safe_api_call(worksheet.clear)
        except gspread.WorksheetNotFound:
            worksheet = data_manager._safe_api_call(data_manager.spreadsheet.add_worksheet, 
                                                  title=PERMISSIONS_SHEET_NAME, rows=500, cols=3)
        
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        all_data = [['é—¨åº—åç§°', 'äººå‘˜ç¼–å·', 'æ›´æ–°æ—¶é—´']]
        
        for _, row in df.iterrows():
            all_data.append([
                str(row.iloc[0]).strip(),
                str(row.iloc[1]).strip(),
                current_time
            ])
        
        # ä¸€æ¬¡æ€§æ‰¹é‡å†™å…¥
        data_manager._safe_api_call(worksheet.update, 'A1', all_data)
        
        logger.info(f"æƒé™æ•°æ®ä¿å­˜æˆåŠŸ: {len(df)} æ¡è®°å½•")
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜æƒé™æ•°æ®å¤±è´¥: {str(e)}")
        return False

def load_permissions_from_sheets(gc) -> Optional[pd.DataFrame]:
    """åŠ è½½æƒé™æ•°æ® - ç®€åŒ–ç‰ˆ"""
    try:
        data_manager = SimpleDataManager(gc)
        
        try:
            worksheet = data_manager._safe_api_call(data_manager.spreadsheet.worksheet, PERMISSIONS_SHEET_NAME)
            data = data_manager._safe_api_call(worksheet.get_all_values)
            
            if len(data) <= 1:
                logger.info("æƒé™è¡¨ä¸ºç©º")
                return None
            
            df = pd.DataFrame(data[1:], columns=['é—¨åº—åç§°', 'äººå‘˜ç¼–å·', 'æ›´æ–°æ—¶é—´'])
            result_df = df[['é—¨åº—åç§°', 'äººå‘˜ç¼–å·']].copy()
            
            # ç®€å•æ•°æ®æ¸…ç†
            result_df['é—¨åº—åç§°'] = result_df['é—¨åº—åç§°'].str.strip()
            result_df['äººå‘˜ç¼–å·'] = result_df['äººå‘˜ç¼–å·'].str.strip()
            
            # ç§»é™¤ç©ºè¡Œ
            result_df = result_df[
                (result_df['é—¨åº—åç§°'] != '') & 
                (result_df['äººå‘˜ç¼–å·'] != '')
            ]
            
            logger.info(f"æƒé™æ•°æ®åŠ è½½æˆåŠŸ: {len(result_df)} æ¡è®°å½•")
            return result_df
            
        except gspread.WorksheetNotFound:
            logger.info("æƒé™è¡¨ä¸å­˜åœ¨")
            return None
            
    except Exception as e:
        logger.error(f"åŠ è½½æƒé™æ•°æ®å¤±è´¥: {str(e)}")
        return None

# åº”æ”¶æœªæ”¶é¢åˆ†æå‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
def analyze_receivable_data(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†æåº”æ”¶æœªæ”¶é¢æ•°æ®"""
    result = {}
    
    if len(df.columns) == 0 or len(df) == 0:
        return result
    
    original_df = df.copy()
    first_row = df.iloc[0] if len(df) > 0 else None
    if first_row is not None:
        non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
        if non_empty_count <= 2:
            df = df.iloc[1:].reset_index(drop=True)
            result['skipped_store_name_row'] = True
    
    target_row_index = 68  # ç¬¬69è¡Œ
    
    if len(df) > target_row_index:
        row = df.iloc[target_row_index]
        first_col_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
        
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
        
        for keyword in keywords:
            if keyword in first_col_value:
                for col_idx in range(len(row)-1, 0, -1):
                    val = row.iloc[col_idx]
                    if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                        cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                        
                        if cleaned.startswith('(') and cleaned.endswith(')'):
                            cleaned = '-' + cleaned[1:-1]
                        
                        try:
                            amount = float(cleaned)
                            if amount != 0:
                                result['åº”æ”¶-æœªæ”¶é¢'] = {
                                    'amount': amount,
                                    'column_name': str(df.columns[col_idx]),
                                    'row_name': first_col_value,
                                    'row_index': target_row_index,
                                    'actual_row_number': target_row_index + 1
                                }
                                return result
                        except ValueError:
                            continue
                break
    
    # å¤‡ç”¨æŸ¥æ‰¾é€»è¾‘
    if 'åº”æ”¶-æœªæ”¶é¢' not in result:
        keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
        
        for idx, row in df.iterrows():
            try:
                row_name = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
                
                if not row_name.strip():
                    continue
                
                for keyword in keywords:
                    if keyword in row_name:
                        for col_idx in range(len(row)-1, 0, -1):
                            val = row.iloc[col_idx]
                            if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                                cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                                
                                if cleaned.startswith('(') and cleaned.endswith(')'):
                                    cleaned = '-' + cleaned[1:-1]
                                
                                try:
                                    amount = float(cleaned)
                                    if amount != 0:
                                        result['åº”æ”¶-æœªæ”¶é¢'] = {
                                            'amount': amount,
                                            'column_name': str(df.columns[col_idx]),
                                            'row_name': row_name,
                                            'row_index': idx,
                                            'actual_row_number': idx + 1,
                                            'note': f'åœ¨ç¬¬{idx+1}è¡Œæ‰¾åˆ°ï¼ˆéç¬¬69è¡Œï¼‰'
                                        }
                                        return result
                                except ValueError:
                                    continue
                        break
            except Exception:
                continue
    
    result['debug_info'] = {
        'total_rows': len(df),
        'checked_row_69': len(df) > target_row_index,
        'row_69_content': str(df.iloc[target_row_index].iloc[0]) if len(df) > target_row_index else 'N/A'
    }
    
    return result

# å·¥å…·å‡½æ•°
def verify_user_permission(store_name: str, user_id: str, permissions_data: Optional[pd.DataFrame]) -> bool:
    """éªŒè¯ç”¨æˆ·æƒé™"""
    if permissions_data is None or len(permissions_data.columns) < 2:
        return False
    
    store_col = permissions_data.columns[0]
    id_col = permissions_data.columns[1]
    
    for _, row in permissions_data.iterrows():
        stored_store = str(row[store_col]).strip()
        stored_id = str(row[id_col]).strip()
        
        if (store_name in stored_store or stored_store in store_name) and stored_id == str(user_id):
            return True
    
    return False

def find_matching_reports(store_name: str, reports_data: Dict[str, pd.DataFrame]) -> List[str]:
    """æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨"""
    matching = []
    for sheet_name in reports_data.keys():
        if store_name in sheet_name or sheet_name in store_name:
            matching.append(sheet_name)
    return matching

def show_status_message(message: str, status_type: str = "info"):
    """æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯"""
    css_class = f"status-{status_type}"
    st.markdown(f'<div class="{css_class}">{message}</div>', unsafe_allow_html=True)

@st.cache_resource(show_spinner="è¿æ¥äº‘æ•°æ®åº“...")
def get_google_sheets_client():
    """è·å–Google Sheetså®¢æˆ·ç«¯"""
    try:
        credentials_info = st.secrets["google_sheets"]
        scopes = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
        credentials = Credentials.from_service_account_info(credentials_info, scopes=scopes)
        client = gspread.authorize(credentials)
        logger.info("Google Sheetså®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return client
    except Exception as e:
        logger.error(f"Google Sheetså®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        raise Exception(f"è¿æ¥å¤±è´¥: {str(e)}")

# CSSæ ·å¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
        margin-bottom: 2rem;
    }
    .store-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .admin-panel {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #fdcb6e;
        margin: 1rem 0;
    }
    .receivable-positive {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        color: #721c24;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #f093fb;
        margin: 1rem 0;
        text-align: center;
    }
    .receivable-negative {
        background: linear-gradient(135deg, #a8edea 0%, #d299c2 100%);
        color: #0c4128;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #48cab2;
        margin: 1rem 0;
        text-align: center;
    }
    .status-success {
        background: #d4edda;
        color: #155724;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
        margin: 0.5rem 0;
    }
    .status-error {
        background: #f8d7da;
        color: #721c24;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
        margin: 0.5rem 0;
    }
    .status-warning {
        background: #fff3cd;
        color: #856404;
        padding: 0.75rem;
        border-radius: 5px;
        border: 1px solid #ffeaa7;
        margin: 0.5rem 0;
    }
    .optimization-info {
        background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
        color: white;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .data-status {
        background: linear-gradient(135deg, #00b894 0%, #00cec9 100%);
        color: white;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .clear-warning {
        background: linear-gradient(135deg, #e17055 0%, #fdcb6e 100%);
        color: #2d3436;
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #e17055;
        margin: 1rem 0;
    }
    </style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
if 'store_name' not in st.session_state:
    st.session_state.store_name = ""
if 'user_id' not in st.session_state:
    st.session_state.user_id = ""
if 'is_admin' not in st.session_state:
    st.session_state.is_admin = False
if 'google_sheets_client' not in st.session_state:
    st.session_state.google_sheets_client = None

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ“Š é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ (ä¼˜åŒ–ç‰ˆ)</h1>', unsafe_allow_html=True)

# æ˜¾ç¤ºç³»ç»Ÿä¼˜åŒ–ç‰¹æ€§ - æ›´æ–°ä¸ºæç®€ç‰ˆ
st.markdown('''
    <div class="optimization-info">
        <h4>ğŸš€ ç³»ç»Ÿä¼˜åŒ–ç‰¹æ€§ (æç®€ç‰ˆ)</h4>
        <p>â€¢ <strong>APIé™åˆ¶ä¿æŠ¤</strong>ï¼šæ™ºèƒ½é™æµï¼Œæ°¸ä¸è¶…è¿‡é…é¢<br>
        â€¢ <strong>æ— å¤‡ä»½æ¨¡å¼</strong>ï¼šå½»åº•èŠ‚çœå­˜å‚¨ç©ºé—´<br>
        â€¢ <strong>æ•°æ®æˆªæ–­ä¿æŠ¤</strong>ï¼šé˜²æ­¢å•å…ƒæ ¼è¿‡å¤§<br>
        â€¢ <strong>æ‰¹é‡ä¼˜åŒ–å¤„ç†</strong>ï¼šä¸Šä¼ é€Ÿåº¦æå‡10å€<br>
        â€¢ <strong>æœ€å°å­˜å‚¨å ç”¨</strong>ï¼šåªä¿ç•™å¿…è¦æ•°æ®</p>
    </div>
''', unsafe_allow_html=True)

# åˆå§‹åŒ–Google Sheetså®¢æˆ·ç«¯
if not st.session_state.google_sheets_client:
    try:
        with st.spinner("è¿æ¥äº‘æ•°æ®åº“..."):
            gc = get_google_sheets_client()
            st.session_state.google_sheets_client = gc
            show_status_message("âœ… äº‘æ•°æ®åº“è¿æ¥æˆåŠŸï¼", "success")
    except Exception as e:
        show_status_message(f"âŒ è¿æ¥å¤±è´¥: {str(e)}", "error")
        st.stop()

gc = st.session_state.google_sheets_client

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»ŸåŠŸèƒ½")
    
    # ç³»ç»ŸçŠ¶æ€
    st.subheader("ğŸ“¡ ç³»ç»ŸçŠ¶æ€")
    if gc:
        st.success("ğŸŸ¢ äº‘æ•°æ®åº“å·²è¿æ¥")
        
        # æ˜¾ç¤ºAPIä½¿ç”¨æƒ…å†µ
        current_requests = len(api_limiter.request_times)
        st.metric("APIä½¿ç”¨ç‡", f"{current_requests}/{MAX_REQUESTS_PER_MINUTE}")
        
        # æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€ - ç®€åŒ–ç‰ˆ
        try:
            data_manager = SimpleDataManager(gc)
            info = data_manager.get_current_data_info()
            if info["has_data"]:
                st.metric("å½“å‰é—¨åº—æ•°", info["store_count"])
                st.metric("æ•°æ®æœˆä»½", info.get("data_month", "æœªçŸ¥"))
                if info.get("last_update"):
                    st.caption(f"æ›´æ–°æ—¶é—´: {info['last_update']}")
            else:
                st.info("æš‚æ— æ•°æ®")
        except:
            st.warning("æ— æ³•è·å–æ•°æ®çŠ¶æ€")
    else:
        st.error("ğŸ”´ äº‘æ•°æ®åº“æ–­å¼€")
    
    user_type = st.radio("é€‰æ‹©ç”¨æˆ·ç±»å‹", ["æ™®é€šç”¨æˆ·", "ç®¡ç†å‘˜"])
    
    if user_type == "ç®¡ç†å‘˜":
        st.subheader("ğŸ” ç®¡ç†å‘˜ç™»å½•")
        admin_password = st.text_input("ç®¡ç†å‘˜å¯†ç ", type="password")
        
        if st.button("éªŒè¯ç®¡ç†å‘˜èº«ä»½"):
            if admin_password == ADMIN_PASSWORD:
                st.session_state.is_admin = True
                show_status_message("âœ… ç®¡ç†å‘˜éªŒè¯æˆåŠŸï¼", "success")
                st.rerun()
            else:
                show_status_message("âŒ å¯†ç é”™è¯¯ï¼", "error")
        
        if st.session_state.is_admin:
            st.subheader("ğŸ“ æ–‡ä»¶ç®¡ç†")
            
            # æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€
            try:
                data_manager = OptimizedDataManager(gc)
                current_info = data_manager.get_current_data_info()
                
                if current_info["has_data"]:
                    st.markdown(f"""
                        <div class="data-status">
                            <strong>ğŸ“Š å½“å‰æ•°æ®çŠ¶æ€</strong><br>
                            é—¨åº—æ•°é‡: {current_info["store_count"]}<br>
                            æ•°æ®æœˆä»½: {current_info.get("data_month", "æœªçŸ¥")}<br>
                            æ€»è¡Œæ•°: {current_info.get("total_rows", 0):,}<br>
                            æ›´æ–°æ—¶é—´: {current_info.get("last_update", "æœªçŸ¥")}
                        </div>
                    """, unsafe_allow_html=True)
                else:
                    st.info("ğŸ“­ ç³»ç»Ÿä¸­æš‚æ— æ•°æ®")
            except Exception as e:
                st.warning(f"âš ï¸ æ— æ³•è·å–æ•°æ®çŠ¶æ€: {str(e)}")
            
            # ä¸Šä¼ æƒé™è¡¨
            st.markdown("**ğŸ‘¥ æƒé™ç®¡ç†**")
            permissions_file = st.file_uploader("ä¸Šä¼ é—¨åº—æƒé™è¡¨", type=['xlsx', 'xls'])
            if permissions_file:
                try:
                    with st.spinner("å¤„ç†æƒé™è¡¨æ–‡ä»¶..."):
                        df = pd.read_excel(permissions_file)
                        if len(df.columns) >= 2:
                            with st.spinner("ä¿å­˜åˆ°äº‘ç«¯..."):
                                if save_permissions_to_sheets(df, gc):
                                    show_status_message(f"âœ… æƒé™è¡¨å·²ä¸Šä¼ ï¼š{len(df)} ä¸ªç”¨æˆ·", "success")
                                    st.balloons()
                                else:
                                    show_status_message("âŒ ä¿å­˜å¤±è´¥", "error")
                        else:
                            show_status_message("âŒ æ ¼å¼é”™è¯¯ï¼šéœ€è¦è‡³å°‘ä¸¤åˆ—", "error")
                except Exception as e:
                    show_status_message(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}", "error")
            
            # ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨
            st.markdown("**ğŸ“Š æŠ¥è¡¨ç®¡ç†**")
            
            # æ¸…ç†æ•°æ®ç¡®è®¤
            try:
                data_manager = OptimizedDataManager(gc)
                current_info = data_manager.get_current_data_info()
                
                if current_info.get("has_data", False):
                    st.markdown(f'''
                        <div class="clear-warning">
                            <h4>âš ï¸ é‡è¦æç¤º</h4>
                            <p>ä¸Šä¼ æ–°æŠ¥è¡¨å°†<strong>å®Œå…¨æ¸…ç©º</strong>ç°æœ‰çš„ {current_info["store_count"]} ä¸ªé—¨åº—æ•°æ®ï¼</p>
                            <p>ç³»ç»Ÿå°†è‡ªåŠ¨åˆ›å»ºå¤‡ä»½ï¼Œä½†è¯·ç¡®è®¤æ‚¨è¦æ›¿æ¢å½“å‰æ•°æ®ã€‚</p>
                        </div>
                    ''', unsafe_allow_html=True)
            except Exception as e:
                logger.warning(f"è·å–æ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")
            
            reports_file = st.file_uploader(
                "ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨", 
                type=['xlsx', 'xls'],
                help="ä¸Šä¼ æ–°æŠ¥è¡¨å°†æ¸…ç©ºæ‰€æœ‰ç°æœ‰æ•°æ®å¹¶æ›¿æ¢ä¸ºæ–°æ•°æ®"
            )
            
            if reports_file:
                try:
                    with st.spinner("å¤„ç†æŠ¥è¡¨æ–‡ä»¶..."):
                        excel_file = pd.ExcelFile(reports_file)
                        reports_dict = {}
                        
                        # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
                        for sheet in excel_file.sheet_names:
                            try:
                                df = pd.read_excel(reports_file, sheet_name=sheet)
                                if not df.empty:
                                    reports_dict[sheet] = df
                                    logger.info(f"è¯»å–å·¥ä½œè¡¨ '{sheet}': {len(df)} è¡Œ")
                            except Exception as e:
                                logger.warning(f"è·³è¿‡å·¥ä½œè¡¨ '{sheet}': {str(e)}")
                                continue
                        
                        if reports_dict:
                            # æ˜¾ç¤ºä¸Šä¼ ç¡®è®¤
                            st.markdown(f"""
                                <div class="clear-warning">
                                    <h4>ğŸ“‹ å³å°†ä¸Šä¼ çš„æ•°æ®</h4>
                                    <p>â€¢ é—¨åº—æ•°é‡: <strong>{len(reports_dict)}</strong><br>
                                    â€¢ æ€»æ•°æ®è¡Œæ•°: <strong>{sum(len(df) for df in reports_dict.values()):,}</strong><br>
                                    â€¢ æ•°æ®æœˆä»½: <strong>{datetime.now().strftime('%Y-%m')}</strong></p>
                                </div>
                            """, unsafe_allow_html=True)
                            
                            # äºŒæ¬¡ç¡®è®¤
                            confirm_upload = st.checkbox(
                                "âœ… æˆ‘ç¡®è®¤è¦æ¸…ç©ºç°æœ‰æ•°æ®å¹¶ä¸Šä¼ æ–°æ•°æ®", 
                                help="æ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…ç¡®è®¤"
                            )
                            
                            if confirm_upload and st.button("ğŸš€ å¼€å§‹ä¸Šä¼ å¹¶æ¸…ç©ºæ—§æ•°æ®", type="primary"):
                                try:
                                    data_manager = OptimizedDataManager(gc)
                                    
                                    with st.spinner("æ­£åœ¨ä¸Šä¼ æ•°æ®ï¼ˆåŒ…æ‹¬æ¸…ç©ºæ—§æ•°æ®å’Œåˆ›å»ºå¤‡ä»½ï¼‰..."):
                                        if data_manager.save_reports_optimized(reports_dict, clear_existing=True):
                                            show_status_message(
                                                f"âœ… æŠ¥è¡¨ä¸Šä¼ æˆåŠŸï¼š{len(reports_dict)} ä¸ªé—¨åº—ï¼Œ"
                                                f"{sum(len(df) for df in reports_dict.values()):,} è¡Œæ•°æ®", 
                                                "success"
                                            )
                                            st.balloons()
                                            st.rerun()
                                        else:
                                            show_status_message("âŒ ä¸Šä¼ å¤±è´¥", "error")
                                except Exception as e:
                                    show_status_message(f"âŒ ä¸Šä¼ å¤±è´¥ï¼š{str(e)}", "error")
                        else:
                            show_status_message("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å·¥ä½œè¡¨", "error")
                            
                except Exception as e:
                    show_status_message(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}", "error")
            
            # ç³»ç»Ÿç»´æŠ¤
            st.subheader("ğŸ› ï¸ ç³»ç»Ÿç»´æŠ¤")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ—‘ï¸ æ‰‹åŠ¨æ¸…ç©ºæ•°æ®"):
                    try:
                        data_manager = OptimizedDataManager(gc)
                        if data_manager.clear_all_report_data(create_backup=True):
                            show_status_message("âœ… æ•°æ®å·²æ¸…ç©º", "success")
                            st.rerun()
                        else:
                            show_status_message("âŒ æ¸…ç©ºå¤±è´¥", "error")
                    except Exception as e:
                        show_status_message(f"âŒ æ¸…ç©ºå¤±è´¥: {str(e)}", "error")
            
            with col2:
                if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€"):
                    st.rerun()
    
    else:
        if st.session_state.logged_in:
            st.subheader("ğŸ‘¤ å½“å‰ç™»å½•")
            st.info(f"é—¨åº—ï¼š{st.session_state.store_name}")
            st.info(f"ç¼–å·ï¼š{st.session_state.user_id}")
            
            if st.button("ğŸšª é€€å‡ºç™»å½•"):
                st.session_state.logged_in = False
                st.session_state.store_name = ""
                st.session_state.user_id = ""
                show_status_message("ğŸ‘‹ å·²é€€å‡ºç™»å½•", "success")
                st.rerun()

# ä¸»ç•Œé¢é€»è¾‘
if user_type == "ç®¡ç†å‘˜" and st.session_state.is_admin:
    st.markdown('<div class="admin-panel"><h3>ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜æ§åˆ¶é¢æ¿</h3><p>æœˆåº¦è½®æ›¿å­˜å‚¨ï¼ŒAPIé™åˆ¶ä¿æŠ¤ï¼Œæ™ºèƒ½æ•°æ®ç®¡ç†</p></div>', unsafe_allow_html=True)
    
    try:
        # è·å–è¯¦ç»†çš„ç³»ç»Ÿç»Ÿè®¡ - ç®€åŒ–ç‰ˆ
        data_manager = SimpleDataManager(gc)
        info = data_manager.get_current_data_info()
        
        # ç³»ç»Ÿç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("é—¨åº—æ•°é‡", info.get("store_count", 0))
        with col2:
            st.metric("æ•°æ®è¡Œæ•°", f"{info.get('total_rows', 0):,}")
        with col3:
            data_month = info.get("data_month", "æ— ")
            st.metric("æ•°æ®æœˆä»½", data_month)
        with col4:
            api_usage = f"{len(api_limiter.request_times)}/{MAX_REQUESTS_PER_MINUTE}"
            st.metric("APIä½¿ç”¨", api_usage)
        
        # æ•°æ®é¢„è§ˆ
        if info["has_data"]:
            st.subheader("ğŸ“Š æ•°æ®é¢„è§ˆ")
            
            try:
                reports_data = data_manager.load_reports_simple()
                
                if reports_data:
                    # æ˜¾ç¤ºé—¨åº—åˆ—è¡¨
                    store_names = list(reports_data.keys())[:10]  # æ˜¾ç¤ºå‰10ä¸ª
                    
                    st.markdown(f"**å½“å‰é—¨åº—åˆ—è¡¨** (æ˜¾ç¤ºå‰10ä¸ªï¼Œå…±{len(reports_data)}ä¸ª)ï¼š")
                    for i, name in enumerate(store_names, 1):
                        df = reports_data[name]
                        st.markdown(f"{i}. **{name}** - {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
                    
                    if len(reports_data) > 10:
                        st.markdown(f"... è¿˜æœ‰ {len(reports_data) - 10} ä¸ªé—¨åº—")
                
            except Exception as e:
                st.error(f"æ•°æ®é¢„è§ˆå¤±è´¥: {str(e)}")
        
        else:
            st.info("ğŸ“­ ç³»ç»Ÿä¸­æš‚æ— æ•°æ®ï¼Œè¯·ä¸Šä¼ æŠ¥è¡¨æ–‡ä»¶")
            
    except Exception as e:
        show_status_message(f"âŒ åŠ è½½ç®¡ç†é¢æ¿å¤±è´¥ï¼š{str(e)}", "error")

elif user_type == "ç®¡ç†å‘˜" and not st.session_state.is_admin:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ç®¡ç†å‘˜å¯†ç ")

else:
    # æ™®é€šç”¨æˆ·ç•Œé¢
    if not st.session_state.logged_in:
        st.subheader("ğŸ” ç”¨æˆ·ç™»å½•")
        
        try:
            with st.spinner("åŠ è½½æƒé™æ•°æ®..."):
                permissions_data = load_permissions_from_sheets(gc)
            
            if permissions_data is None:
                st.warning("âš ï¸ ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
            else:
                stores = sorted(permissions_data[permissions_data.columns[0]].unique().tolist())
                
                with st.form("login_form"):
                    selected_store = st.selectbox("é€‰æ‹©é—¨åº—", stores)
                    user_id = st.text_input("äººå‘˜ç¼–å·")
                    submit = st.form_submit_button("ğŸš€ ç™»å½•")
                    
                    if submit and selected_store and user_id:
                        if verify_user_permission(selected_store, user_id, permissions_data):
                            st.session_state.logged_in = True
                            st.session_state.store_name = selected_store
                            st.session_state.user_id = user_id
                            show_status_message("âœ… ç™»å½•æˆåŠŸï¼", "success")
                            st.balloons()
                            st.rerun()
                        else:
                            show_status_message("âŒ é—¨åº—æˆ–ç¼–å·é”™è¯¯ï¼", "error")
                            
        except Exception as e:
            show_status_message(f"âŒ æƒé™éªŒè¯å¤±è´¥ï¼š{str(e)}", "error")
    
    else:
        # å·²ç™»å½•ç”¨æˆ·ç•Œé¢ï¼ˆä¿æŒåŸæœ‰ç•Œé¢ä¸å˜ï¼‰
        st.markdown(f'<div class="store-info"><h3>ğŸª {st.session_state.store_name}</h3><p>æ“ä½œå‘˜ï¼š{st.session_state.user_id}</p></div>', unsafe_allow_html=True)
        
        try:
            with st.spinner("åŠ è½½æŠ¥è¡¨æ•°æ®..."):
                data_manager = SimpleDataManager(gc)
                reports_data = data_manager.load_reports_simple()
                matching_sheets = find_matching_reports(st.session_state.store_name, reports_data)
            
            if matching_sheets:
                if len(matching_sheets) > 1:
                    selected_sheet = st.selectbox("é€‰æ‹©æŠ¥è¡¨", matching_sheets)
                else:
                    selected_sheet = matching_sheets[0]
                
                df = reports_data[selected_sheet]
                
                # åº”æ”¶-æœªæ”¶é¢çœ‹æ¿
                st.subheader("ğŸ’° åº”æ”¶-æœªæ”¶é¢")
                
                try:
                    analysis_results = analyze_receivable_data(df)
                    
                    if 'åº”æ”¶-æœªæ”¶é¢' in analysis_results:
                        data = analysis_results['åº”æ”¶-æœªæ”¶é¢']
                        amount = data['amount']
                        
                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            if amount > 0:
                                st.markdown(f'''
                                    <div class="receivable-positive">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’³ Â¥{amount:,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">é—¨åº—åº”ä»˜æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            elif amount < 0:
                                st.markdown(f'''
                                    <div class="receivable-negative">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’š Â¥{abs(amount):,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">æ€»éƒ¨åº”é€€æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            else:
                                st.markdown('''
                                    <div style="background: #e8f5e8; color: #2e7d32; padding: 2rem; border-radius: 15px; text-align: center;">
                                        <h1 style="margin: 0; font-size: 3rem;">âš–ï¸ Â¥0.00</h1>
                                        <h3 style="margin: 0.5rem 0;">æ”¶æ”¯å¹³è¡¡</h3>
                                        <p style="margin: 0;">åº”æ”¶æœªæ”¶é¢ä¸ºé›¶ï¼Œè´¦ç›®å¹³è¡¡</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                    
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°åº”æ”¶-æœªæ”¶é¢æ•°æ®")
                        
                        with st.expander("ğŸ” æŸ¥çœ‹è¯¦æƒ…", expanded=False):
                            debug_info = analysis_results.get('debug_info', {})
                            
                            st.markdown("### ğŸ“‹ æ•°æ®æŸ¥æ‰¾è¯´æ˜")
                            st.write(f"- **æŠ¥è¡¨æ€»è¡Œæ•°ï¼š** {debug_info.get('total_rows', 0)} è¡Œ")
                            
                            if debug_info.get('checked_row_69'):
                                st.write(f"- **ç¬¬69è¡Œå†…å®¹ï¼š** {debug_info.get('row_69_content', 'N/A')}")
                            else:
                                st.write("- **ç¬¬69è¡Œï¼š** æŠ¥è¡¨è¡Œæ•°ä¸è¶³69è¡Œ")
                            
                            st.markdown("""
                            ### ğŸ’¡ å¯èƒ½çš„åŸå› 
                            1. ç¬¬69è¡Œä¸åŒ…å«"åº”æ”¶-æœªæ”¶é¢"ç›¸å…³å…³é”®è¯
                            2. ç¬¬69è¡Œçš„æ•°å€¼ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®
                            3. æŠ¥è¡¨æ ¼å¼ä¸é¢„æœŸä¸ç¬¦
                            
                            ### ğŸ› ï¸ å»ºè®®
                            - è¯·æ£€æŸ¥ExcelæŠ¥è¡¨ç¬¬69è¡Œæ˜¯å¦åŒ…å«"åº”æ”¶-æœªæ”¶é¢"
                            - ç¡®è®¤è¯¥è¡Œæœ‰å¯¹åº”çš„é‡‘é¢æ•°æ®
                            - å¦‚éœ€è°ƒæ•´æŸ¥æ‰¾ä½ç½®ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
                            """)
                
                except Exception as e:
                    show_status_message(f"âŒ åˆ†ææ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}", "error")
                
                st.divider()
                
                # å®Œæ•´æŠ¥è¡¨æ•°æ®
                st.subheader("ğŸ“‹ å®Œæ•´æŠ¥è¡¨æ•°æ®")
                
                search_term = st.text_input("ğŸ” æœç´¢æŠ¥è¡¨å†…å®¹")
                
                try:
                    if search_term:
                        search_df = df.copy()
                        for col in search_df.columns:
                            search_df[col] = search_df[col].astype(str).fillna('')
                        
                        mask = search_df.apply(
                            lambda x: x.str.contains(search_term, case=False, na=False, regex=False)
                        ).any(axis=1)
                        filtered_df = df[mask]
                        st.info(f"æ‰¾åˆ° {len(filtered_df)} æ¡åŒ…å« '{search_term}' çš„è®°å½•")
                    else:
                        filtered_df = df
                    
                    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ•°æ®è¡Œæ•°", len(filtered_df))
                    with col2:
                        st.metric("æ•°æ®åˆ—æ•°", len(df.columns))
                    with col3:
                        st.metric("æ•°æ®æ¥æº", "â˜ï¸ äº‘ç«¯")
                    
                    if len(filtered_df) > 0:
                        display_df = filtered_df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_columns = []
                        for i, col in enumerate(display_df.columns):
                            col_name = str(col)
                            if col_name in unique_columns:
                                col_name = f"{col_name}_{i}"
                            unique_columns.append(col_name)
                        display_df.columns = unique_columns
                        
                        # æ¸…ç†æ•°æ®å†…å®¹
                        for col in display_df.columns:
                            display_df[col] = display_df[col].astype(str).fillna('')
                        
                        st.dataframe(display_df, use_container_width=True, height=400)
                    
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                        
                except Exception as e:
                    show_status_message(f"âŒ æ•°æ®å¤„ç†æ—¶å‡ºé”™ï¼š{str(e)}", "error")
                
                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ æ•°æ®ä¸‹è½½")
                
                col1, col2 = st.columns(2)
                with col1:
                    try:
                        buffer = io.BytesIO()
                        download_df = df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_cols = []
                        for i, col in enumerate(download_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        download_df.columns = unique_cols
                        
                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            download_df.to_excel(writer, index=False)
                        
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥è¡¨ (Excel)",
                            buffer.getvalue(),
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.xlsx",
                            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    except Exception as e:
                        show_status_message(f"Excelä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")
                
                with col2:
                    try:
                        csv_df = df.copy()
                        unique_cols = []
                        for i, col in enumerate(csv_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        csv_df.columns = unique_cols
                        
                        csv = csv_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                            csv,
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.csv",
                            "text/csv"
                        )
                    except Exception as e:
                        show_status_message(f"CSVä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")
            
            else:
                st.error(f"âŒ æœªæ‰¾åˆ°é—¨åº— '{st.session_state.store_name}' çš„æŠ¥è¡¨")
                
                # æ˜¾ç¤ºå¯ç”¨é—¨åº—åˆ—è¡¨
                if reports_data:
                    st.subheader("ğŸ“‹ ç³»ç»Ÿä¸­çš„é—¨åº—åˆ—è¡¨")
                    available_stores = list(reports_data.keys())
                    for store in available_stores[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                        st.write(f"â€¢ {store}")
                    if len(available_stores) > 10:
                        st.write(f"... è¿˜æœ‰ {len(available_stores) - 10} ä¸ªé—¨åº—")
                else:
                    st.info("ç³»ç»Ÿä¸­æš‚æ— ä»»ä½•æŠ¥è¡¨æ•°æ®")
                
        except Exception as e:
            show_status_message(f"âŒ æŠ¥è¡¨åŠ è½½å¤±è´¥ï¼š{str(e)}", "error")

# é¡µé¢åº•éƒ¨çŠ¶æ€ä¿¡æ¯
st.divider()
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.caption(f"ğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
with col2:
    api_usage = f"{len(api_limiter.request_times)}/{MAX_REQUESTS_PER_MINUTE}"
    st.caption(f"ğŸ“¡ APIä½¿ç”¨: {api_usage}")
with col3:
    try:
        data_manager = SimpleDataManager(gc)
        info = data_manager.get_current_data_info()
        if info["has_data"]:
            st.caption(f"ğŸ“Š æ•°æ®: {info['data_month']}")
        else:
            st.caption("ğŸ“Š æ•°æ®: æ— ")
    except:
        st.caption("ğŸ“Š æ•°æ®: æœªçŸ¥")
with col4:
    st.caption("ğŸ”§ ç‰ˆæœ¬: v2.3 (æç®€ç‰ˆ)")

# è‡ªåŠ¨APIé™åˆ¶æ¸…ç†ï¼ˆæ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•ï¼‰
current_time = time.time()
cutoff_time = current_time - 70  # 70ç§’å‰çš„è®°å½•
api_limiter.request_times = [t for t in api_limiter.request_times if t > cutoff_time]_times if t > cutoff_time]
