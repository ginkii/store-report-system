import streamlit as st
import pandas as pd
import io
import json
import gzip
import hashlib
from datetime import datetime, timedelta
import time
from qcloud_cos import CosConfig, CosS3Client
from qcloud_cos.cos_exception import CosServiceError, CosClientError
import logging
from typing import Optional, Dict, Any, List
import traceback
from contextlib import contextmanager
import threading
import re

# ===================== é¡µé¢é…ç½® =====================
st.set_page_config(
    page_title="é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ", 
    page_icon="ğŸ“Š",
    layout="wide"
)

# ===================== ç³»ç»Ÿé…ç½® =====================
ADMIN_PASSWORD = st.secrets.get("system", {}).get("admin_password", "admin123")
MAX_STORAGE_GB = 40  # 40GBå­˜å‚¨é™åˆ¶
API_RATE_LIMIT = 500  # æ¯å°æ—¶APIè°ƒç”¨é™åˆ¶
COMPRESSION_LEVEL = 6  # GZIPå‹ç¼©ç­‰çº§
RETRY_ATTEMPTS = 3  # é‡è¯•æ¬¡æ•°
RETRY_DELAY = 1  # é‡è¯•å»¶è¿Ÿ(ç§’)
MAX_RETRIES = 3
CACHE_DURATION = 300  # ç¼“å­˜5åˆ†é’Ÿ

# ===================== CSSæ ·å¼ =====================
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
        margin-bottom: 2rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: 700;
    }
    .store-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 10px 20px rgba(0, 0, 0, 0.15);
        text-align: center;
    }
    .admin-panel {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        border: 2px solid #fdcb6e;
        margin: 1rem 0;
        box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
    }
    .receivable-positive {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        color: #721c24;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #f093fb;
        margin: 1rem 0;
        text-align: center;
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
    }
    .receivable-negative {
        background: linear-gradient(135deg, #a8edea 0%, #d299c2 100%);
        color: #0c4128;
        padding: 2rem;
        border-radius: 15px;
        border: 3px solid #48cab2;
        margin: 1rem 0;
        text-align: center;
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
    }
    .status-success {
        background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
        color: #2d3436;
        padding: 0.75rem;
        border-radius: 8px;
        border-left: 5px solid #00b894;
        margin: 0.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    .status-error {
        background: linear-gradient(135deg, #fd79a8 0%, #e84393 100%);
        color: white;
        padding: 0.75rem;
        border-radius: 8px;
        border-left: 5px solid #d63031;
        margin: 0.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    .status-warning {
        background: linear-gradient(135deg, #fdcb6e 0%, #e17055 100%);
        color: white;
        padding: 0.75rem;
        border-radius: 8px;
        border-left: 5px solid #e84393;
        margin: 0.5rem 0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    .system-stats {
        background: linear-gradient(135deg, #00cec9 0%, #55a3ff 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        text-align: center;
    }
    .debug-info {
        background: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        font-family: monospace;
        font-size: 0.9rem;
        color: #495057;
    }
    </style>
""", unsafe_allow_html=True)

# ===================== æ—¥å¿—é…ç½® =====================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===================== å¼‚å¸¸ç±»å®šä¹‰ =====================
class CosOperationError(Exception):
    """è…¾è®¯äº‘COSæ“ä½œå¼‚å¸¸"""
    pass

class DataProcessingError(Exception):
    """æ•°æ®å¤„ç†å¼‚å¸¸"""
    pass

# ===================== è°ƒè¯•æ—¥å¿—ç®¡ç†å™¨ =====================
class DebugLogger:
    """è°ƒè¯•æ—¥å¿—ç®¡ç†å™¨ï¼Œç”¨äºè¿½è¸ªæ•°æ®æµ"""
    
    def __init__(self):
        self.logs = []
        self.max_logs = 100
    
    def log(self, level: str, message: str, data: Dict = None):
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'message': message,
            'data': data or {}
        }
        self.logs.append(log_entry)
        
        # ä¿æŒæ—¥å¿—æ•°é‡é™åˆ¶
        if len(self.logs) > self.max_logs:
            self.logs = self.logs[-self.max_logs:]
        
        # åŒæ—¶è®°å½•åˆ°æ ‡å‡†æ—¥å¿—
        if level == 'ERROR':
            logger.error(f"[DEBUG] {message}")
        elif level == 'WARNING':
            logger.warning(f"[DEBUG] {message}")
        else:
            logger.info(f"[DEBUG] {message}")
    
    def get_recent_logs(self, count: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„è°ƒè¯•æ—¥å¿—"""
        return self.logs[-count:] if self.logs else []
    
    def clear_logs(self):
        """æ¸…ç©ºæ—¥å¿—"""
        self.logs = []

# å…¨å±€è°ƒè¯•æ—¥å¿—å®ä¾‹
debug_logger = DebugLogger()

# ===================== å·¥å…·å‡½æ•° =====================
@contextmanager
def error_handler(operation_name: str):
    """é€šç”¨é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        debug_logger.log('INFO', f'å¼€å§‹æ‰§è¡Œ: {operation_name}')
        yield
        debug_logger.log('INFO', f'æˆåŠŸå®Œæˆ: {operation_name}')
    except Exception as e:
        error_msg = f"{operation_name} å¤±è´¥: {str(e)}"
        debug_logger.log('ERROR', error_msg)
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        st.error(f"âŒ {error_msg}")
        raise

def retry_operation(func, *args, max_retries=MAX_RETRIES, delay=RETRY_DELAY, **kwargs):
    """é‡è¯•æ“ä½œè£…é¥°å™¨"""
    for attempt in range(max_retries):
        try:
            result = func(*args, **kwargs)
            debug_logger.log('INFO', f'å‡½æ•° {func.__name__} æ‰§è¡ŒæˆåŠŸ', {'attempt': attempt + 1})
            return result
        except Exception as e:
            error_msg = f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}"
            if attempt == max_retries - 1:
                debug_logger.log('ERROR', error_msg, {'final_attempt': True})
                logger.error(error_msg)
                raise
            else:
                debug_logger.log('WARNING', error_msg, {'retry_delay': delay * (attempt + 1)})
                logger.warning(error_msg)
                time.sleep(delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ

def get_cache_key(operation: str, params: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(f"{operation}_{params}".encode()).hexdigest()

def set_cache(key: str, data: Any, duration: int = CACHE_DURATION):
    """è®¾ç½®ç¼“å­˜"""
    try:
        cache_data = {
            'data': data,
            'timestamp': time.time(),
            'duration': duration
        }
        st.session_state[f"cache_{key}"] = cache_data
        debug_logger.log('INFO', f'ç¼“å­˜è®¾ç½®æˆåŠŸ: {key}')
    except Exception as e:
        debug_logger.log('WARNING', f'è®¾ç½®ç¼“å­˜å¤±è´¥: {str(e)}')
        logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {str(e)}")

def get_cache(key: str) -> Optional[Any]:
    """è·å–ç¼“å­˜"""
    try:
        cache_key = f"cache_{key}"
        if cache_key in st.session_state:
            cache_data = st.session_state[cache_key]
            if time.time() - cache_data['timestamp'] < cache_data['duration']:
                debug_logger.log('INFO', f'ç¼“å­˜å‘½ä¸­: {key}')
                return cache_data['data']
            else:
                del st.session_state[cache_key]
                debug_logger.log('INFO', f'ç¼“å­˜è¿‡æœŸåˆ é™¤: {key}')
    except Exception as e:
        debug_logger.log('WARNING', f'è·å–ç¼“å­˜å¤±è´¥: {str(e)}')
        logger.warning(f"è·å–ç¼“å­˜å¤±è´¥: {str(e)}")
    return None

def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    original_filename = filename
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    filename = re.sub(r'\s+', '_', filename.strip())
    filename = filename.strip('.')
    
    debug_logger.log('INFO', 'æ–‡ä»¶åæ¸…ç†', {
        'original': original_filename,
        'sanitized': filename
    })
    
    return filename

def normalize_store_name(store_name: str) -> str:
    """æ ‡å‡†åŒ–é—¨åº—åç§°ï¼Œç”¨äºæ›´å¥½çš„åŒ¹é…"""
    if not store_name:
        return ""
    
    # ç§»é™¤å‰åç©ºæ ¼
    normalized = store_name.strip()
    # ç§»é™¤å¸¸è§çš„é—¨åº—åç¼€
    suffixes = ['åº—', 'åˆ†åº—', 'é—¨åº—', 'è¥ä¸šéƒ¨', 'ä¸“å–åº—']
    for suffix in suffixes:
        if normalized.endswith(suffix):
            normalized = normalized[:-len(suffix)].strip()
            break
    
    debug_logger.log('INFO', 'é—¨åº—åç§°æ ‡å‡†åŒ–', {
        'original': store_name,
        'normalized': normalized
    })
    
    return normalized

# ===================== APIé¢‘ç‡é™åˆ¶å™¨ =====================
class SimpleRateLimiter:
    """ç®€åŒ–çš„APIé¢‘ç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_calls_per_hour: int = API_RATE_LIMIT):
        self.max_calls = max_calls_per_hour
        self.calls = []
        self.lock = threading.Lock()
    
    def can_make_call(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›è¡ŒAPIè°ƒç”¨"""
        with self.lock:
            now = datetime.now()
            # æ¸…ç†ä¸€å°æ—¶å‰çš„è®°å½•
            self.calls = [call_time for call_time in self.calls 
                         if now - call_time < timedelta(hours=1)]
            
            can_call = len(self.calls) < self.max_calls
            debug_logger.log('INFO', f'APIè°ƒç”¨æ£€æŸ¥', {
                'current_calls': len(self.calls),
                'max_calls': self.max_calls,
                'can_call': can_call
            })
            return can_call
    
    def record_call(self):
        """è®°å½•APIè°ƒç”¨"""
        with self.lock:
            self.calls.append(datetime.now())
            debug_logger.log('INFO', f'APIè°ƒç”¨è®°å½•', {
                'total_calls_in_hour': len(self.calls)
            })
    
    def get_remaining_calls(self) -> int:
        """è·å–å‰©ä½™å¯è°ƒç”¨æ¬¡æ•°"""
        with self.lock:
            now = datetime.now()
            self.calls = [call_time for call_time in self.calls 
                         if now - call_time < timedelta(hours=1)]
            return max(0, self.max_calls - len(self.calls))

# ===================== å‹ç¼©ç®¡ç†å™¨ =====================
class CompressionManager:
    """æ•°æ®å‹ç¼©ç®¡ç†å™¨"""
    
    @staticmethod
    def compress_data(data: bytes) -> bytes:
        """å‹ç¼©æ•°æ®"""
        try:
            compressed = gzip.compress(data, compresslevel=COMPRESSION_LEVEL)
            debug_logger.log('INFO', 'æ•°æ®å‹ç¼©æˆåŠŸ', {
                'original_size': len(data),
                'compressed_size': len(compressed),
                'compression_ratio': (1 - len(compressed) / len(data)) * 100
            })
            return compressed
        except Exception as e:
            debug_logger.log('ERROR', f'æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}')
            logger.error(f"æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}")
            return data  # å‹ç¼©å¤±è´¥æ—¶è¿”å›åŸæ•°æ®
    
    @staticmethod
    def decompress_data(data: bytes) -> bytes:
        """è§£å‹æ•°æ®ï¼Œæ”¯æŒå®¹é”™"""
        try:
            decompressed = gzip.decompress(data)
            debug_logger.log('INFO', 'æ•°æ®è§£å‹æˆåŠŸ', {
                'compressed_size': len(data),
                'decompressed_size': len(decompressed)
            })
            return decompressed
        except Exception as e:
            debug_logger.log('WARNING', f'æ•°æ®è§£å‹å¤±è´¥ï¼Œè¿”å›åŸæ•°æ®: {str(e)}')
            logger.warning(f"æ•°æ®è§£å‹å¤±è´¥ï¼Œè¿”å›åŸæ•°æ®: {str(e)}")
            return data  # è§£å‹å¤±è´¥æ—¶è¿”å›åŸæ•°æ®
    
    @staticmethod
    def compress_json(data: dict) -> bytes:
        """å‹ç¼©JSONæ•°æ®"""
        try:
            json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
            json_bytes = json_str.encode('utf-8')
            compressed = gzip.compress(json_bytes, compresslevel=COMPRESSION_LEVEL)
            
            debug_logger.log('INFO', 'JSONå‹ç¼©æˆåŠŸ', {
                'original_size': len(json_bytes),
                'compressed_size': len(compressed),
                'compression_ratio': (1 - len(compressed) / len(json_bytes)) * 100
            })
            return compressed
        except Exception as e:
            debug_logger.log('ERROR', f'JSONå‹ç¼©å¤±è´¥: {str(e)}')
            logger.error(f"JSONå‹ç¼©å¤±è´¥: {str(e)}")
            # è¿”å›æœªå‹ç¼©çš„JSON
            json_str = json.dumps(data, ensure_ascii=False)
            return json_str.encode('utf-8')
    
    @staticmethod
    def decompress_json(data: bytes) -> dict:
        """è§£å‹JSONæ•°æ®ï¼Œæ”¯æŒå®¹é”™"""
        try:
            # å°è¯•è§£å‹
            decompressed = gzip.decompress(data)
            result = json.loads(decompressed.decode('utf-8'))
            debug_logger.log('INFO', 'JSONè§£å‹æˆåŠŸ')
            return result
        except Exception:
            try:
                # å¯èƒ½æ˜¯æœªå‹ç¼©çš„JSON
                result = json.loads(data.decode('utf-8'))
                debug_logger.log('INFO', 'JSONç›´æ¥è§£ææˆåŠŸï¼ˆæœªå‹ç¼©ï¼‰')
                return result
            except Exception as e:
                debug_logger.log('ERROR', f'JSONè§£å‹å¤±è´¥: {str(e)}')
                logger.error(f"JSONè§£å‹å¤±è´¥: {str(e)}")
                return {}

# ===================== è…¾è®¯äº‘COSç®¡ç†å™¨ =====================
class TencentCOSManager:
    """è…¾è®¯äº‘COSç®¡ç†å™¨"""
    
    def __init__(self):
        self.client = None
        self.bucket_name = None
        self.region = None
        self.rate_limiter = SimpleRateLimiter(API_RATE_LIMIT)
        self.compression = CompressionManager()
        self.permissions_file = "system/permissions.json"
        self.metadata_file = "system/metadata.json"
        self.initialize_from_secrets()
    
    def initialize_from_secrets(self):
        """åˆå§‹åŒ–COSå®¢æˆ·ç«¯"""
        try:
            if "tencent_cos" not in st.secrets:
                raise Exception("æœªæ‰¾åˆ°è…¾è®¯äº‘COSé…ç½®")
            
            config = st.secrets["tencent_cos"]
            secret_id = config.get("secret_id")
            secret_key = config.get("secret_key")
            self.region = config.get("region", "ap-beijing")
            self.bucket_name = config.get("bucket_name")
            
            if not all([secret_id, secret_key, self.bucket_name]):
                raise Exception("è…¾è®¯äº‘COSé…ç½®ä¸å®Œæ•´")
            
            cos_config = CosConfig(
                Region=self.region,
                SecretId=secret_id,
                SecretKey=secret_key
            )
            
            self.client = CosS3Client(cos_config)
            
            debug_logger.log('INFO', 'è…¾è®¯äº‘COSåˆå§‹åŒ–æˆåŠŸ', {
                'bucket': self.bucket_name,
                'region': self.region
            })
            logger.info(f"è…¾è®¯äº‘COSåˆå§‹åŒ–æˆåŠŸ: {self.bucket_name}")
            
        except Exception as e:
            debug_logger.log('ERROR', f'è…¾è®¯äº‘COSåˆå§‹åŒ–å¤±è´¥: {str(e)}')
            logger.error(f"è…¾è®¯äº‘COSåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise CosOperationError(f"COSåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _execute_with_limit_check(self, operation):
        """æ‰§è¡Œå¸¦é¢‘ç‡é™åˆ¶æ£€æŸ¥çš„æ“ä½œ"""
        if not self.rate_limiter.can_make_call():
            remaining = self.rate_limiter.get_remaining_calls()
            error_msg = f"APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œå‰©ä½™: {remaining}/å°æ—¶"
            debug_logger.log('ERROR', error_msg)
            raise CosOperationError(error_msg)
        
        result = operation()
        self.rate_limiter.record_call()
        return result
    
    def upload_file(self, file_data: bytes, filename: str, compress: bool = True) -> Optional[str]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°COS"""
        try:
            original_filename = filename
            filename = sanitize_filename(filename)
            
            # å‹ç¼©å¤„ç†
            upload_data = file_data
            final_filename = filename
            
            if compress:
                compressed_data = self.compression.compress_data(file_data)
                if len(compressed_data) < len(file_data):
                    upload_data = compressed_data
                    if not filename.endswith('.gz'):
                        final_filename = filename + '.gz'
                    else:
                        final_filename = filename
                    
                    compression_ratio = (1 - len(compressed_data) / len(file_data)) * 100
                    debug_logger.log('INFO', 'æ–‡ä»¶å‹ç¼©å®Œæˆ', {
                        'original_filename': original_filename,
                        'final_filename': final_filename,
                        'original_size': len(file_data),
                        'compressed_size': len(compressed_data),
                        'compression_ratio': compression_ratio
                    })
            
            # ä¸Šä¼ æ“ä½œ
            def upload_operation():
                return self.client.put_object(
                    Bucket=self.bucket_name,
                    Body=upload_data,
                    Key=final_filename,
                    ContentType='application/octet-stream'
                )
            
            retry_operation(lambda: self._execute_with_limit_check(upload_operation))
            
            file_url = f"https://{self.bucket_name}.cos.{self.region}.myqcloud.com/{final_filename}"
            
            debug_logger.log('INFO', 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ', {
                'filename': final_filename,
                'file_url': file_url,
                'file_size': len(upload_data)
            })
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {final_filename}")
            return file_url
            
        except Exception as e:
            debug_logger.log('ERROR', f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}', {
                'filename': filename,
                'file_size': len(file_data)
            })
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
            raise CosOperationError(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
    
    def download_file(self, filename: str, decompress: bool = True) -> Optional[bytes]:
        """ä»COSä¸‹è½½æ–‡ä»¶"""
        try:
            debug_logger.log('INFO', 'å¼€å§‹ä¸‹è½½æ–‡ä»¶', {'filename': filename})
            
            def download_operation():
                response = self.client.get_object(
                    Bucket=self.bucket_name,
                    Key=filename
                )
                return response['Body'].read()
            
            file_data = retry_operation(lambda: self._execute_with_limit_check(download_operation))
            
            debug_logger.log('INFO', 'æ–‡ä»¶ä¸‹è½½æˆåŠŸ', {
                'filename': filename,
                'downloaded_size': len(file_data),
                'is_compressed': filename.endswith('.gz')
            })
            
            # è§£å‹å¤„ç†
            if decompress and filename.endswith('.gz'):
                original_size = len(file_data)
                file_data = self.compression.decompress_data(file_data)
                debug_logger.log('INFO', 'æ–‡ä»¶è§£å‹å®Œæˆ', {
                    'filename': filename,
                    'compressed_size': original_size,
                    'decompressed_size': len(file_data)
                })
            
            logger.info(f"æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {filename}")
            return file_data
            
        except CosServiceError as e:
            if e.get_error_code() == 'NoSuchKey':
                debug_logger.log('WARNING', f'æ–‡ä»¶ä¸å­˜åœ¨: {filename}')
                logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                return None
            
            debug_logger.log('ERROR', f'COSæœåŠ¡é”™è¯¯: {e.get_error_msg()}', {'filename': filename})
            logger.error(f"COSæœåŠ¡é”™è¯¯: {e.get_error_msg()}")
            return None
        except Exception as e:
            debug_logger.log('ERROR', f'æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}', {'filename': filename})
            logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
            return None
    
    def upload_json(self, data: dict, filename: str) -> bool:
        """ä¸Šä¼ JSONæ•°æ®"""
        try:
            json_bytes = self.compression.compress_json(data)
            
            if not filename.endswith('.gz'):
                filename = filename + '.gz'
            
            result = self.upload_file(json_bytes, filename, compress=False)
            success = result is not None
            
            debug_logger.log('INFO' if success else 'ERROR', 
                           f'JSONä¸Šä¼ {"æˆåŠŸ" if success else "å¤±è´¥"}', {
                'filename': filename,
                'data_size': len(json_bytes)
            })
            
            return success
        except Exception as e:
            debug_logger.log('ERROR', f'JSONä¸Šä¼ å¤±è´¥: {str(e)}', {'filename': filename})
            logger.error(f"JSONä¸Šä¼ å¤±è´¥: {str(e)}")
            return False
    
    def download_json(self, filename: str) -> Optional[dict]:
        """ä¸‹è½½JSONæ•°æ®"""
        try:
            possible_filenames = [filename]
            if not filename.endswith('.gz'):
                possible_filenames.append(filename + '.gz')
            if filename.endswith('.gz'):
                possible_filenames.append(filename[:-3])
            
            for fname in possible_filenames:
                debug_logger.log('INFO', f'å°è¯•ä¸‹è½½JSONæ–‡ä»¶: {fname}')
                file_data = self.download_file(fname, decompress=False)
                if file_data:
                    result = self.compression.decompress_json(file_data)
                    debug_logger.log('INFO', f'JSONä¸‹è½½æˆåŠŸ: {fname}', {
                        'data_keys': list(result.keys()) if isinstance(result, dict) else 'not_dict'
                    })
                    return result
            
            debug_logger.log('WARNING', f'JSONæ–‡ä»¶æœªæ‰¾åˆ°', {
                'attempted_filenames': possible_filenames
            })
            return None
        except Exception as e:
            debug_logger.log('ERROR', f'JSONä¸‹è½½å¤±è´¥: {str(e)}', {'filename': filename})
            logger.error(f"JSONä¸‹è½½å¤±è´¥: {str(e)}")
            return None
    
    def list_files(self, prefix: str = "") -> List[Dict]:
        """åˆ—å‡ºæ–‡ä»¶"""
        try:
            def list_operation():
                return self.client.list_objects(
                    Bucket=self.bucket_name,
                    Prefix=prefix,
                    MaxKeys=1000
                )
            
            response = retry_operation(lambda: self._execute_with_limit_check(list_operation))
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    files.append({
                        'filename': obj['Key'],
                        'size': obj['Size'],
                        'last_modified': obj['LastModified']
                    })
            
            debug_logger.log('INFO', f'æ–‡ä»¶åˆ—è¡¨è·å–æˆåŠŸ', {
                'prefix': prefix,
                'file_count': len(files)
            })
            return files
            
        except Exception as e:
            debug_logger.log('ERROR', f'åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}', {'prefix': prefix})
            logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def get_storage_stats(self) -> Dict:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        try:
            files = self.list_files()
            total_size = sum(f['size'] for f in files)
            
            report_files = [f for f in files if f['filename'].startswith('reports/')]
            system_files = [f for f in files if f['filename'].startswith('system/')]
            
            stats = {
                'total_files': len(files),
                'total_size_gb': total_size / (1024**3),
                'report_files': len(report_files),
                'system_files': len(system_files),
                'usage_percent': (total_size / (1024**3)) / MAX_STORAGE_GB * 100,
                'remaining_calls': self.rate_limiter.get_remaining_calls()
            }
            
            debug_logger.log('INFO', 'å­˜å‚¨ç»Ÿè®¡è·å–æˆåŠŸ', stats)
            return stats
        except Exception as e:
            debug_logger.log('ERROR', f'è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {str(e)}')
            logger.error(f"è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                'total_files': 0,
                'total_size_gb': 0,
                'report_files': 0,
                'system_files': 0,
                'usage_percent': 0,
                'remaining_calls': 0
            }

# ===================== ä¸»åº”ç”¨ç±» =====================
@st.cache_resource(show_spinner="è¿æ¥è…¾è®¯äº‘COS...")
def get_tencent_cos_client():
    """è·å–è…¾è®¯äº‘COSå®¢æˆ·ç«¯ - ä½¿ç”¨ç¼“å­˜"""
    try:
        manager = TencentCOSManager()
        debug_logger.log('INFO', 'COSå®¢æˆ·ç«¯ç¼“å­˜åˆ›å»ºæˆåŠŸ')
        logger.info("è…¾è®¯äº‘COSå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        return manager
    except Exception as e:
        debug_logger.log('ERROR', f'COSå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}')
        logger.error(f"è…¾è®¯äº‘COSå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        raise CosOperationError(f"è¿æ¥å¤±è´¥: {str(e)}")

def clean_dataframe_for_storage(df: pd.DataFrame) -> pd.DataFrame:
    """æ¸…ç†DataFrameä»¥ä¾¿å­˜å‚¨"""
    try:
        df_cleaned = df.copy()
        
        # å¤„ç†å„ç§æ•°æ®ç±»å‹
        for col in df_cleaned.columns:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†ç‰¹æ®Šå€¼
            df_cleaned[col] = df_cleaned[col].astype(str)
            df_cleaned[col] = df_cleaned[col].replace({
                'nan': '',
                'None': '',
                'NaT': '',
                'null': '',
                '<NA>': ''
            })
            
            # å¤„ç†è¿‡é•¿çš„å­—ç¬¦ä¸²
            df_cleaned[col] = df_cleaned[col].apply(
                lambda x: x[:1000] + '...' if len(str(x)) > 1000 else x
            )
        
        debug_logger.log('INFO', f'DataFrameæ¸…ç†å®Œæˆ: {len(df_cleaned)} è¡Œ x {len(df_cleaned.columns)} åˆ—')
        return df_cleaned
        
    except Exception as e:
        debug_logger.log('ERROR', f'æ¸…ç†DataFrameå¤±è´¥: {str(e)}')
        logger.error(f"æ¸…ç†DataFrameå¤±è´¥: {str(e)}")
        raise DataProcessingError(f"æ•°æ®æ¸…ç†å¤±è´¥: {str(e)}")

def save_permissions_to_cos(df: pd.DataFrame, cos_manager: TencentCOSManager) -> bool:
    """ä¿å­˜æƒé™æ•°æ®åˆ°COS"""
    with error_handler("ä¿å­˜æƒé™æ•°æ®"):
        def _save_operation():
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            permissions_data = []
            for _, row in df.iterrows():
                permissions_data.append({
                    "store_name": str(row.iloc[0]).strip(),
                    "user_id": str(row.iloc[1]).strip(),
                    "created_at": current_time
                })
            
            data = {
                'permissions': permissions_data,
                'last_updated': current_time,
                'count': len(permissions_data)
            }
            
            success = cos_manager.upload_json(data, cos_manager.permissions_file)
            
            if success:
                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                cache_key = get_cache_key("permissions", "load")
                if f"cache_{cache_key}" in st.session_state:
                    del st.session_state[f"cache_{cache_key}"]
                
                debug_logger.log('INFO', f'æƒé™æ•°æ®ä¿å­˜æˆåŠŸ: {len(permissions_data)} æ¡è®°å½•')
                logger.info(f"æƒé™æ•°æ®ä¿å­˜æˆåŠŸ: {len(permissions_data)} æ¡è®°å½•")
            
            return success
        
        return retry_operation(_save_operation)

def load_permissions_from_cos(cos_manager: TencentCOSManager) -> Optional[pd.DataFrame]:
    """ä»COSåŠ è½½æƒé™æ•°æ® - ä½¿ç”¨ç¼“å­˜"""
    cache_key = get_cache_key("permissions", "load")
    cached_data = get_cache(cache_key)
    if cached_data is not None:
        debug_logger.log('INFO', "ä»ç¼“å­˜åŠ è½½æƒé™æ•°æ®")
        logger.info("ä»ç¼“å­˜åŠ è½½æƒé™æ•°æ®")
        return cached_data
    
    with error_handler("åŠ è½½æƒé™æ•°æ®"):
        def _load_operation():
            data = cos_manager.download_json(cos_manager.permissions_file)
            
            if not data or 'permissions' not in data:
                debug_logger.log('INFO', "æƒé™æ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                logger.info("æƒé™æ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
                return None
            
            permissions = data['permissions']
            
            # è½¬æ¢ä¸ºDataFrame
            df_data = []
            for perm in permissions:
                df_data.append({
                    'é—¨åº—åç§°': perm.get('store_name', '').strip(),
                    'äººå‘˜ç¼–å·': perm.get('user_id', '').strip()
                })
            
            if not df_data:
                return None
            
            df = pd.DataFrame(df_data)
            
            # ç§»é™¤ç©ºè¡Œ
            df = df[
                (df['é—¨åº—åç§°'] != '') & 
                (df['äººå‘˜ç¼–å·'] != '')
            ]
            
            debug_logger.log('INFO', f'æƒé™æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•')
            logger.info(f"æƒé™æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
            
            # è®¾ç½®ç¼“å­˜
            set_cache(cache_key, df)
            return df
        
        return retry_operation(_load_operation)

def save_reports_to_cos(reports_dict: Dict[str, pd.DataFrame], cos_manager: TencentCOSManager, original_file_data: bytes) -> bool:
    """ä¿å­˜æŠ¥è¡¨æ•°æ®åˆ°COS - æ··åˆå­˜å‚¨ï¼šåŸå§‹æ–‡ä»¶ + é¢„è§£ææ•°æ®"""
    def _save_operation():
        # åŠ è½½ç°æœ‰å…ƒæ•°æ®
        metadata = cos_manager.download_json(cos_manager.metadata_file) or {'reports': []}
        
        current_time = datetime.now().isoformat()
        timestamp = int(time.time())
        uploaded_files = []  # è·Ÿè¸ªå·²ä¸Šä¼ æ–‡ä»¶ï¼Œç”¨äºå›æ»š
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªé—¨åº—ä¿å­˜åŸå§‹æ–‡ä»¶å’Œé¢„è§£ææ•°æ®
            for store_name, df in reports_dict.items():
                sanitized_name = sanitize_filename(store_name)
                file_hash = hashlib.md5(str(df.values.tolist()).encode()).hexdigest()[:8]
                base_name = f"{sanitized_name}_{timestamp}_{file_hash}"
                
                # åŸå§‹æ–‡ä»¶è·¯å¾„
                raw_filename = f"reports/raw/{base_name}.xlsx"
                parsed_filename = f"reports/parsed/{base_name}_data.json"
                
                debug_logger.log('INFO', f'å¼€å§‹å¤„ç†é—¨åº—: {store_name}', {
                    'raw_file': raw_filename,
                    'parsed_file': parsed_filename
                })
                
                # 1. ä¿å­˜åŸå§‹Excelæ–‡ä»¶
                raw_url = cos_manager.upload_file(original_file_data, raw_filename, compress=True)
                if not raw_url:
                    raise Exception(f"åŸå§‹æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {store_name}")
                
                uploaded_files.append(raw_filename + '.gz')
                debug_logger.log('INFO', f'åŸå§‹æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {store_name}')
                
                # 2. ç«‹å³éªŒè¯åŸå§‹æ–‡ä»¶
                verify_data = cos_manager.download_file(raw_filename + '.gz', decompress=True)
                if not verify_data:
                    raise Exception(f"åŸå§‹æ–‡ä»¶éªŒè¯å¤±è´¥: {store_name}")
                
                # 3. ç”Ÿæˆé¢„è§£ææ•°æ®
                try:
                    # åˆ†æåº”æ”¶-æœªæ”¶é¢
                    analysis_result = analyze_receivable_data(df)
                    
                    # æ¸…ç†DataFrameå‡†å¤‡JSONåºåˆ—åŒ–
                    df_cleaned = clean_dataframe_for_storage(df)
                    
                    parsed_data = {
                        'store_name': store_name,
                        'data': df_cleaned.to_dict('records'),
                        'columns': list(df_cleaned.columns),
                        'analysis': analysis_result,
                        'row_count': len(df),
                        'col_count': len(df.columns),
                        'parsed_time': current_time
                    }
                    
                    # 4. ä¿å­˜é¢„è§£ææ•°æ®
                    parsed_success = cos_manager.upload_json(parsed_data, parsed_filename)
                    if parsed_success:
                        uploaded_files.append(parsed_filename + '.gz')
                        debug_logger.log('INFO', f'é¢„è§£ææ•°æ®ä¿å­˜æˆåŠŸ: {store_name}')
                    else:
                        debug_logger.log('WARNING', f'é¢„è§£ææ•°æ®ä¿å­˜å¤±è´¥ï¼Œä½†åŸå§‹æ–‡ä»¶å·²ä¿å­˜: {store_name}')
                
                except Exception as e:
                    debug_logger.log('WARNING', f'é¢„è§£æå¤±è´¥ä½†åŸå§‹æ–‡ä»¶å·²ä¿å­˜: {store_name}, é”™è¯¯: {str(e)}')
                    # é¢„è§£æå¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ï¼Œå› ä¸ºæœ‰åŸå§‹æ–‡ä»¶å…œåº•
                
                # 5. åˆ›å»ºæŠ¥è¡¨å…ƒæ•°æ®
                report_metadata = {
                    "store_name": store_name.strip(),
                    "raw_filename": raw_filename + '.gz',
                    "parsed_filename": parsed_filename + '.gz' if parsed_success else None,
                    "file_url": raw_url,
                    "file_size_mb": len(original_file_data) / 1024 / 1024,
                    "upload_time": current_time,
                    "row_count": len(df),
                    "col_count": len(df.columns),
                    "analysis": analysis_result if 'analysis_result' in locals() else {},
                    "id": f"{store_name}_{timestamp}",
                    "has_parsed_data": parsed_success
                }
                
                # ç§»é™¤åŒé—¨åº—çš„æ—§è®°å½•
                old_reports = [r for r in metadata.get('reports', []) 
                             if normalize_store_name(r.get('store_name', '')) == normalize_store_name(store_name.strip())]
                
                metadata['reports'] = [r for r in metadata.get('reports', []) 
                                     if normalize_store_name(r.get('store_name', '')) != normalize_store_name(store_name.strip())]
                
                # æ¸…ç†æ—§æ–‡ä»¶
                for old_report in old_reports:
                    try:
                        if old_report.get('raw_filename'):
                            cos_manager.delete_file(old_report['raw_filename'])
                        if old_report.get('parsed_filename'):
                            cos_manager.delete_file(old_report['parsed_filename'])
                    except:
                        pass  # å¿½ç•¥æ¸…ç†é”™è¯¯
                
                # æ·»åŠ æ–°è®°å½•
                metadata.setdefault('reports', []).append(report_metadata)
                
                debug_logger.log('INFO', f'é—¨åº— {store_name} å¤„ç†å®Œæˆ', {
                    'raw_file_saved': True,
                    'parsed_file_saved': parsed_success,
                    'metadata_updated': True
                })
            
            # ç¬¬äºŒæ­¥ï¼šä¿å­˜å…ƒæ•°æ®
            metadata['last_updated'] = current_time
            metadata_success = cos_manager.upload_json(metadata, cos_manager.metadata_file)
            
            if not metadata_success:
                raise Exception("å…ƒæ•°æ®ä¿å­˜å¤±è´¥")
            
            # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å…ƒæ•°æ®
            verify_metadata = cos_manager.download_json(cos_manager.metadata_file)
            if not verify_metadata:
                raise Exception("å…ƒæ•°æ®éªŒè¯å¤±è´¥")
            
            # ç¬¬å››æ­¥ï¼šæ¸…é™¤ç¼“å­˜
            cache_keys_to_clear = [
                get_cache_key("reports", "load"),
                get_cache_key("metadata", "load")
            ]
            
            for cache_key in cache_keys_to_clear:
                if f"cache_{cache_key}" in st.session_state:
                    del st.session_state[f"cache_{cache_key}"]
            
            debug_logger.log('INFO', f'æŠ¥è¡¨æ•°æ®ä¿å­˜å®Œæˆ', {
                'stores_processed': len(reports_dict),
                'files_uploaded': len(uploaded_files),
                'metadata_saved': True
            })
            
            return True
            
        except Exception as e:
            debug_logger.log('ERROR', f'ä¿å­˜è¿‡ç¨‹å¤±è´¥ï¼Œå¼€å§‹å›æ»š: {str(e)}')
            
            # å›æ»šï¼šåˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶
            for filename in uploaded_files:
                try:
                    cos_manager.delete_file(filename)
                    debug_logger.log('INFO', f'å›æ»šåˆ é™¤æ–‡ä»¶: {filename}')
                except:
                    debug_logger.log('WARNING', f'å›æ»šåˆ é™¤æ–‡ä»¶å¤±è´¥: {filename}')
            
            raise Exception(f"ä¿å­˜å¤±è´¥å¹¶å·²å›æ»š: {str(e)}")
    
    try:
        return retry_operation(_save_operation)
    except Exception as e:
        logger.error(f"æŠ¥è¡¨ä¿å­˜å¤±è´¥: {str(e)}")
        return False

def load_reports_from_cos(cos_manager: TencentCOSManager) -> Dict[str, pd.DataFrame]:
    """ä»COSåŠ è½½æŠ¥è¡¨æ•°æ® - æ··åˆå­˜å‚¨ï¼šä¼˜å…ˆä½¿ç”¨é¢„è§£ææ•°æ®ï¼Œå¦åˆ™è§£æåŸå§‹æ–‡ä»¶"""
    cache_key = get_cache_key("reports", "load")
    cached_data = get_cache(cache_key)
    if cached_data is not None:
        debug_logger.log('INFO', "ä»ç¼“å­˜åŠ è½½æŠ¥è¡¨æ•°æ®")
        logger.info("ä»ç¼“å­˜åŠ è½½æŠ¥è¡¨æ•°æ®")
        return cached_data
    
    def _load_operation():
        metadata = cos_manager.download_json(cos_manager.metadata_file)
        
        if not metadata or 'reports' not in metadata:
            debug_logger.log('INFO', "æŠ¥è¡¨å…ƒæ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            logger.info("æŠ¥è¡¨å…ƒæ•°æ®ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            return {}
        
        reports_dict = {}
        reports = metadata['reports']
        
        for report in reports:
            store_name = report.get('store_name')
            raw_filename = report.get('raw_filename')
            parsed_filename = report.get('parsed_filename')
            has_parsed_data = report.get('has_parsed_data', False)
            
            if not store_name:
                continue
            
            debug_logger.log('INFO', f'åŠ è½½æŠ¥è¡¨: {store_name}', {
                'has_parsed_data': has_parsed_data,
                'raw_filename': raw_filename,
                'parsed_filename': parsed_filename
            })
            
            df = None
            
            # ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•ä½¿ç”¨é¢„è§£ææ•°æ®
            if has_parsed_data and parsed_filename:
                try:
                    debug_logger.log('INFO', f'å°è¯•åŠ è½½é¢„è§£ææ•°æ®: {store_name}')
                    parsed_data = cos_manager.download_json(parsed_filename.replace('.gz', ''))
                    
                    if parsed_data and 'data' in parsed_data:
                        # ä»é¢„è§£ææ•°æ®é‡å»ºDataFrame
                        df = pd.DataFrame(parsed_data['data'])
                        if 'columns' in parsed_data:
                            df.columns = parsed_data['columns'][:len(df.columns)]
                        
                        debug_logger.log('INFO', f'é¢„è§£ææ•°æ®åŠ è½½æˆåŠŸ: {store_name}', {
                            'rows': len(df),
                            'cols': len(df.columns)
                        })
                
                except Exception as e:
                    debug_logger.log('WARNING', f'é¢„è§£ææ•°æ®åŠ è½½å¤±è´¥: {store_name}, é”™è¯¯: {str(e)}')
                    df = None
            
            # ç­–ç•¥2ï¼šé¢„è§£ææ•°æ®ä¸å¯ç”¨æ—¶ï¼Œè§£æåŸå§‹æ–‡ä»¶
            if df is None and raw_filename:
                try:
                    debug_logger.log('INFO', f'å°è¯•è§£æåŸå§‹æ–‡ä»¶: {store_name}')
                    
                    # ä¸‹è½½åŸå§‹Excelæ–‡ä»¶
                    excel_data = cos_manager.download_file(raw_filename, decompress=True)
                    
                    if excel_data:
                        # è§£æExcelæ–‡ä»¶
                        excel_file = pd.ExcelFile(io.BytesIO(excel_data))
                        
                        # æŸ¥æ‰¾åˆé€‚çš„å·¥ä½œè¡¨
                        sheet_name = None
                        normalized_store = normalize_store_name(store_name)
                        
                        # å¤šå±‚å·¥ä½œè¡¨åŒ¹é…
                        for sheet in excel_file.sheet_names:
                            normalized_sheet = normalize_store_name(sheet)
                            if (sheet == store_name or 
                                normalized_sheet == normalized_store or
                                store_name in sheet or sheet in store_name or
                                normalized_store in normalized_sheet or 
                                normalized_sheet in normalized_store):
                                sheet_name = sheet
                                break
                        
                        # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                        if not sheet_name and excel_file.sheet_names:
                            sheet_name = excel_file.sheet_names[0]
                        
                        if sheet_name:
                            df = pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name)
                            
                            debug_logger.log('INFO', f'åŸå§‹æ–‡ä»¶è§£ææˆåŠŸ: {store_name}', {
                                'sheet_name': sheet_name,
                                'rows': len(df),
                                'cols': len(df.columns)
                            })
                        else:
                            debug_logger.log('ERROR', f'æœªæ‰¾åˆ°åˆé€‚çš„å·¥ä½œè¡¨: {store_name}')
                            continue
                
                except Exception as e:
                    debug_logger.log('ERROR', f'åŸå§‹æ–‡ä»¶è§£æå¤±è´¥: {store_name}, é”™è¯¯: {str(e)}')
                    logger.error(f"åŸå§‹æ–‡ä»¶è§£æå¤±è´¥ {store_name}: {str(e)}")
                    continue
            
            # æˆåŠŸåŠ è½½æ•°æ®
            if df is not None:
                reports_dict[store_name] = df
                debug_logger.log('INFO', f'æŠ¥è¡¨ {store_name} åŠ è½½æˆåŠŸ', {
                    'final_rows': len(df),
                    'final_cols': len(df.columns),
                    'load_method': 'parsed_data' if has_parsed_data and parsed_filename else 'raw_file'
                })
            else:
                debug_logger.log('ERROR', f'æŠ¥è¡¨ {store_name} åŠ è½½å®Œå…¨å¤±è´¥')
        
        debug_logger.log('INFO', f'æŠ¥è¡¨æ•°æ®åŠ è½½å®Œæˆ: {len(reports_dict)} ä¸ªé—¨åº—')
        logger.info(f"æŠ¥è¡¨æ•°æ®åŠ è½½å®Œæˆ: {len(reports_dict)} ä¸ªé—¨åº—")
        
        # è®¾ç½®ç¼“å­˜
        set_cache(cache_key, reports_dict)
        return reports_dict
    
    try:
        return retry_operation(_load_operation)
    except Exception as e:
        debug_logger.log('ERROR', f'åŠ è½½æŠ¥è¡¨æ•°æ®å¤±è´¥: {str(e)}')
        logger.error(f"åŠ è½½æŠ¥è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        return {}

def analyze_receivable_data(df: pd.DataFrame) -> Dict[str, Any]:
    """åˆ†æåº”æ”¶æœªæ”¶é¢æ•°æ® - ä¸“é—¨æŸ¥æ‰¾ç¬¬69è¡Œ"""
    result = {}
    
    try:
        if len(df.columns) == 0 or len(df) == 0:
            return result
        
        # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯é—¨åº—åç§°
        original_df = df.copy()
        first_row = df.iloc[0] if len(df) > 0 else None
        if first_row is not None:
            non_empty_count = sum(1 for val in first_row if pd.notna(val) and str(val).strip() != '')
            if non_empty_count <= 2:
                df = df.iloc[1:].reset_index(drop=True)
                result['skipped_store_name_row'] = True
        
        # æŸ¥æ‰¾ç¬¬69è¡Œ
        target_row_index = 68  # ç¬¬69è¡Œ
        
        if len(df) > target_row_index:
            row = df.iloc[target_row_index]
            first_col_value = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
            
            debug_logger.log('INFO', 'åˆ†æç¬¬69è¡Œåº”æ”¶-æœªæ”¶é¢', {
                'first_col_value': first_col_value,
                'row_data_sample': [str(row.iloc[i]) if pd.notna(row.iloc[i]) else '' for i in range(min(5, len(row)))]
            })
            
            # æ£€æŸ¥å…³é”®è¯
            keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
            
            for keyword in keywords:
                if keyword in first_col_value:
                    debug_logger.log('INFO', f'æ‰¾åˆ°å…³é”®è¯: {keyword}')
                    
                    # æŸ¥æ‰¾æ•°å€¼
                    for col_idx in range(len(row)-1, 0, -1):
                        val = row.iloc[col_idx]
                        if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                            cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                            
                            if cleaned.startswith('(') and cleaned.endswith(')'):
                                cleaned = '-' + cleaned[1:-1]
                            
                            try:
                                amount = float(cleaned)
                                if amount != 0:
                                    result['åº”æ”¶-æœªæ”¶é¢'] = {
                                        'amount': amount,
                                        'column_name': str(df.columns[col_idx]),
                                        'row_name': first_col_value,
                                        'row_index': target_row_index,
                                        'actual_row_number': target_row_index + 1
                                    }
                                    
                                    debug_logger.log('INFO', 'æˆåŠŸåˆ†æåº”æ”¶-æœªæ”¶é¢', {
                                        'amount': amount,
                                        'column_name': str(df.columns[col_idx]),
                                        'column_index': col_idx
                                    })
                                    return result
                            except ValueError:
                                continue
                    break
        
        # å¤‡ç”¨æŸ¥æ‰¾
        if 'åº”æ”¶-æœªæ”¶é¢' not in result:
            keywords = ['åº”æ”¶-æœªæ”¶é¢', 'åº”æ”¶æœªæ”¶é¢', 'åº”æ”¶-æœªæ”¶', 'åº”æ”¶æœªæ”¶']
            
            for idx, row in df.iterrows():
                try:
                    row_name = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
                    
                    if not row_name.strip():
                        continue
                    
                    for keyword in keywords:
                        if keyword in row_name:
                            for col_idx in range(len(row)-1, 0, -1):
                                val = row.iloc[col_idx]
                                if pd.notna(val) and str(val).strip() not in ['', 'None', 'nan']:
                                    cleaned = str(val).replace(',', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                                    
                                    if cleaned.startswith('(') and cleaned.endswith(')'):
                                        cleaned = '-' + cleaned[1:-1]
                                    
                                    try:
                                        amount = float(cleaned)
                                        if amount != 0:
                                            result['åº”æ”¶-æœªæ”¶é¢'] = {
                                                'amount': amount,
                                                'column_name': str(df.columns[col_idx]),
                                                'row_name': row_name,
                                                'row_index': idx,
                                                'actual_row_number': idx + 1,
                                                'note': f'åœ¨ç¬¬{idx+1}è¡Œæ‰¾åˆ°ï¼ˆéç¬¬69è¡Œï¼‰'
                                            }
                                            
                                            debug_logger.log('INFO', f'å¤‡ç”¨æŸ¥æ‰¾æˆåŠŸ: ç¬¬{idx+1}è¡Œ', {
                                                'amount': amount,
                                                'row_name': row_name
                                            })
                                            return result
                                    except ValueError:
                                        continue
                            break
                except Exception:
                    continue
        
        # è°ƒè¯•ä¿¡æ¯
        result['debug_info'] = {
            'total_rows': len(df),
            'checked_row_69': len(df) > target_row_index,
            'row_69_content': str(df.iloc[target_row_index].iloc[0]) if len(df) > target_row_index else 'N/A'
        }
        
        debug_logger.log('WARNING', 'æœªæ‰¾åˆ°åº”æ”¶-æœªæ”¶é¢æ•°æ®', result['debug_info'])
        
    except Exception as e:
        debug_logger.log('ERROR', f'åˆ†æåº”æ”¶-æœªæ”¶é¢æ•°æ®æ—¶å‡ºé”™: {str(e)}')
        logger.warning(f"åˆ†æåº”æ”¶-æœªæ”¶é¢æ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    return result

def verify_user_permission(store_name: str, user_id: str, permissions_data: Optional[pd.DataFrame]) -> bool:
    """éªŒè¯ç”¨æˆ·æƒé™"""
    try:
        if permissions_data is None or len(permissions_data.columns) < 2:
            debug_logger.log('WARNING', 'æƒé™æ•°æ®æ— æ•ˆ')
            return False
        
        store_col = permissions_data.columns[0]
        id_col = permissions_data.columns[1]
        
        normalized_target_store = normalize_store_name(store_name)
        user_id_str = str(user_id).strip()
        
        for _, row in permissions_data.iterrows():
            stored_store = str(row[store_col]).strip()
            stored_id = str(row[id_col]).strip()
            normalized_stored_store = normalize_store_name(stored_store)
            
            # å¤šå±‚åŒ¹é…ç­–ç•¥
            store_match = False
            
            # 1. ç²¾ç¡®åŒ¹é…
            if stored_store == store_name:
                store_match = True
            # 2. æ ‡å‡†åŒ–ååŒ¹é…
            elif normalized_stored_store == normalized_target_store:
                store_match = True
            # 3. åŒ…å«å…³ç³»åŒ¹é…
            elif (store_name in stored_store or stored_store in store_name or
                  normalized_target_store in normalized_stored_store or 
                  normalized_stored_store in normalized_target_store):
                store_match = True
            
            # ç”¨æˆ·IDåŒ¹é…
            if store_match and stored_id == user_id_str:
                debug_logger.log('INFO', 'æƒé™éªŒè¯æˆåŠŸ', {
                    'matched_store': stored_store,
                    'matched_user_id': stored_id,
                    'match_type': 'exact' if stored_store == store_name else 'normalized'
                })
                return True
        
        debug_logger.log('WARNING', 'æƒé™éªŒè¯å¤±è´¥', {
            'store_name': store_name,
            'user_id': user_id,
            'normalized_store': normalized_target_store
        })
        return False
        
    except Exception as e:
        debug_logger.log('ERROR', f'æƒé™éªŒè¯å¼‚å¸¸: {str(e)}')
        logger.error(f"æƒé™éªŒè¯å¤±è´¥: {str(e)}")
        return False

def find_matching_reports(store_name: str, reports_data: Dict[str, pd.DataFrame]) -> List[str]:
    """æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨"""
    try:
        matching = []
        normalized_target = normalize_store_name(store_name)
        
        for sheet_name in reports_data.keys():
            normalized_sheet = normalize_store_name(sheet_name)
            
            if (store_name == sheet_name or
                normalized_target == normalized_sheet or
                store_name in sheet_name or sheet_name in store_name or
                normalized_target in normalized_sheet or normalized_sheet in normalized_target):
                matching.append(sheet_name)
        
        debug_logger.log('INFO', f'æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨: {store_name}', {
            'matching_reports': matching,
            'total_reports': len(reports_data)
        })
        
        return matching
        
    except Exception as e:
        debug_logger.log('ERROR', f'æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨å¤±è´¥: {str(e)}')
        logger.error(f"æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨å¤±è´¥: {str(e)}")
        return []

def get_original_file_for_download(store_name: str, cos_manager: TencentCOSManager) -> Optional[bytes]:
    """è·å–é—¨åº—çš„åŸå§‹Excelæ–‡ä»¶ç”¨äºä¸‹è½½"""
    try:
        # è·å–å…ƒæ•°æ®
        metadata = cos_manager.download_json(cos_manager.metadata_file)
        if not metadata or 'reports' not in metadata:
            return None
        
        # æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨
        normalized_target = normalize_store_name(store_name)
        matching_report = None
        
        for report in metadata['reports']:
            report_store_name = report.get('store_name', '').strip()
            normalized_report = normalize_store_name(report_store_name)
            
            if (report_store_name == store_name or 
                normalized_report == normalized_target or
                store_name in report_store_name or 
                report_store_name in store_name or
                normalized_target in normalized_report or
                normalized_report in normalized_target):
                matching_report = report
                break
        
        if not matching_report:
            debug_logger.log('WARNING', f'æœªæ‰¾åˆ°é—¨åº— {store_name} çš„åŸå§‹æ–‡ä»¶')
            return None
        
        raw_filename = matching_report.get('raw_filename')
        if not raw_filename:
            debug_logger.log('WARNING', f'é—¨åº— {store_name} æ²¡æœ‰åŸå§‹æ–‡ä»¶è®°å½•')
            return None
        
        # ä¸‹è½½åŸå§‹æ–‡ä»¶
        original_data = cos_manager.download_file(raw_filename, decompress=True)
        if original_data:
            debug_logger.log('INFO', f'åŸå§‹æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {store_name}', {
                'filename': raw_filename,
                'size': len(original_data)
            })
            return original_data
        else:
            debug_logger.log('ERROR', f'åŸå§‹æ–‡ä»¶ä¸‹è½½å¤±è´¥: {store_name}')
            return None
            
    except Exception as e:
        debug_logger.log('ERROR', f'è·å–åŸå§‹æ–‡ä»¶å¤±è´¥: {store_name}, é”™è¯¯: {str(e)}')
        logger.error(f"è·å–åŸå§‹æ–‡ä»¶å¤±è´¥ {store_name}: {str(e)}")
        return None
    """æŸ¥æ‰¾åŒ¹é…çš„æŠ¥è¡¨"""
    try:
        matching = []
        normalized_target = normalize_store_name(store_name)
        
        for sheet_name in reports_data.keys():
            normalized_sheet = normalize_store_name(sheet_name)
            
            if (store_name == sheet_name or
                normalized_target == normalized_sheet or
                store_name in sheet_name or sheet_name in store_name or
                normalized_target in normalized_sheet or normalized_sheet in normalized_target):
                matching.append(sheet_name)
        
        debug_logger.log('INFO', f'æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨: {store_name}', {
            'matching_reports': matching,
            'total_reports': len(reports_data)
        })
        
        return matching
        
    except Exception as e:
        debug_logger.log('ERROR', f'æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨å¤±è´¥: {str(e)}')
        logger.error(f"æŸ¥æ‰¾åŒ¹é…æŠ¥è¡¨å¤±è´¥: {str(e)}")
        return []

def show_status_message(message: str, status_type: str = "info"):
    """æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯"""
    css_class = f"status-{status_type}"
    st.markdown(f'<div class="{css_class}">{message}</div>', unsafe_allow_html=True)

def show_debug_info():
    """æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"""
    if st.session_state.get('debug_mode', False):
        with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯", expanded=False):
            recent_logs = debug_logger.get_recent_logs(20)
            
            if recent_logs:
                st.write("**æœ€è¿‘çš„è°ƒè¯•æ—¥å¿—:**")
                for log in reversed(recent_logs):  # æœ€æ–°çš„åœ¨å‰
                    level_color = {
                        'ERROR': '#ff4757',
                        'WARNING': '#ffa502',
                        'INFO': '#3742fa'
                    }.get(log['level'], '#747d8c')
                    
                    st.markdown(f'''
                    <div class="debug-info">
                    <strong style="color: {level_color};">[{log['level']}]</strong> 
                    <small>{log['timestamp'][:19]}</small><br>
                    {log['message']}
                    {f"<br><small>æ•°æ®: {json.dumps(log['data'], ensure_ascii=False, indent=2)}</small>" if log['data'] else ""}
                    </div>
                    ''', unsafe_allow_html=True)
            else:
                st.info("æš‚æ— è°ƒè¯•æ—¥å¿—")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ åˆ·æ–°æ—¥å¿—"):
                    st.rerun()
            with col2:
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—"):
                    debug_logger.clear_logs()
                    st.rerun()

# ===================== ä¼šè¯çŠ¶æ€åˆå§‹åŒ– =====================
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
if 'store_name' not in st.session_state:
    st.session_state.store_name = ""
if 'user_id' not in st.session_state:
    st.session_state.user_id = ""
if 'is_admin' not in st.session_state:
    st.session_state.is_admin = False
if 'cos_manager' not in st.session_state:
    st.session_state.cos_manager = None
if 'operation_status' not in st.session_state:
    st.session_state.operation_status = []
if 'debug_mode' not in st.session_state:
    st.session_state.debug_mode = False

# ===================== ä¸»ç¨‹åº =====================
# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸ“Š é—¨åº—æŠ¥è¡¨æŸ¥è¯¢ç³»ç»Ÿ</h1>', unsafe_allow_html=True)

# åˆå§‹åŒ–è…¾è®¯äº‘COSå®¢æˆ·ç«¯
if not st.session_state.cos_manager:
    try:
        with st.spinner("è¿æ¥è…¾è®¯äº‘COS..."):
            cos_manager = get_tencent_cos_client()
            st.session_state.cos_manager = cos_manager
            show_status_message("âœ… è…¾è®¯äº‘COSè¿æ¥æˆåŠŸï¼", "success")
            debug_logger.log('INFO', 'ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ')
    except Exception as e:
        show_status_message(f"âŒ è¿æ¥å¤±è´¥: {str(e)}", "error")
        debug_logger.log('ERROR', f'ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}')
        st.stop()

cos_manager = st.session_state.cos_manager

# æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€å’Œè°ƒè¯•ä¿¡æ¯
try:
    storage_stats = cos_manager.get_storage_stats()
    
    st.markdown(f'''
    <div class="system-stats">
    <h4>â˜ï¸ è…¾è®¯äº‘COSçŠ¶æ€ç›‘æ§</h4>
    <div style="display: flex; justify-content: space-between; flex-wrap: wrap;">
        <div style="text-align: center; flex: 1; min-width: 120px;">
            <div style="font-size: 1.5rem; font-weight: bold;">
                {storage_stats.get('total_files', 0)}
            </div>
            <div style="font-size: 0.9rem;">æ€»æ–‡ä»¶æ•°</div>
        </div>
        <div style="text-align: center; flex: 1; min-width: 120px;">
            <div style="font-size: 1.5rem; font-weight: bold;">
                {storage_stats.get('total_size_gb', 0):.1f}GB
            </div>
            <div style="font-size: 0.9rem;">å­˜å‚¨ä½¿ç”¨</div>
        </div>
        <div style="text-align: center; flex: 1; min-width: 120px;">
            <div style="font-size: 1.5rem; font-weight: bold;">
                {storage_stats.get('usage_percent', 0):.1f}%
            </div>
            <div style="font-size: 0.9rem;">ä½¿ç”¨ç‡</div>
        </div>
        <div style="text-align: center; flex: 1; min-width: 120px;">
            <div style="font-size: 1.5rem; font-weight: bold;">
                {storage_stats.get('remaining_calls', 0)}
            </div>
            <div style="font-size: 0.9rem;">APIå‰©ä½™</div>
        </div>
    </div>
    </div>
    ''', unsafe_allow_html=True)
    
    # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
    show_debug_info()
    
except Exception as e:
    show_status_message(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {str(e)}", "error")

# æ˜¾ç¤ºæ“ä½œçŠ¶æ€
for status in st.session_state.operation_status:
    show_status_message(status['message'], status['type'])

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»ŸåŠŸèƒ½")
    
    # ç³»ç»ŸçŠ¶æ€
    st.subheader("ğŸ“¡ ç³»ç»ŸçŠ¶æ€")
    if cos_manager:
        st.success("ğŸŸ¢ è…¾è®¯äº‘COSå·²è¿æ¥")
    else:
        st.error("ğŸ”´ è…¾è®¯äº‘COSæ–­å¼€")
    
    # è°ƒè¯•æ¨¡å¼
    st.session_state.debug_mode = st.checkbox("ğŸ” è°ƒè¯•æ¨¡å¼", value=st.session_state.debug_mode)
    
    user_type = st.radio("é€‰æ‹©ç”¨æˆ·ç±»å‹", ["æ™®é€šç”¨æˆ·", "ç®¡ç†å‘˜"])
    
    if user_type == "ç®¡ç†å‘˜":
        st.subheader("ğŸ” ç®¡ç†å‘˜ç™»å½•")
        admin_password = st.text_input("ç®¡ç†å‘˜å¯†ç ", type="password")
        
        if st.button("éªŒè¯ç®¡ç†å‘˜èº«ä»½"):
            if admin_password == ADMIN_PASSWORD:
                st.session_state.is_admin = True
                show_status_message("âœ… ç®¡ç†å‘˜éªŒè¯æˆåŠŸï¼", "success")
                debug_logger.log('INFO', 'ç®¡ç†å‘˜ç™»å½•æˆåŠŸ')
                st.rerun()
            else:
                show_status_message("âŒ å¯†ç é”™è¯¯ï¼", "error")
                debug_logger.log('WARNING', 'ç®¡ç†å‘˜ç™»å½•å¤±è´¥ï¼šå¯†ç é”™è¯¯')
        
        if st.session_state.is_admin:
            st.subheader("ğŸ“ æ–‡ä»¶ç®¡ç†")
            
            # ä¸Šä¼ æƒé™è¡¨
            permissions_file = st.file_uploader("ä¸Šä¼ é—¨åº—æƒé™è¡¨", type=['xlsx', 'xls'])
            if permissions_file:
                try:
                    with st.spinner("å¤„ç†æƒé™è¡¨æ–‡ä»¶..."):
                        df = pd.read_excel(permissions_file)
                        if len(df.columns) >= 2:
                            with st.spinner("ä¿å­˜åˆ°è…¾è®¯äº‘COS..."):
                                if save_permissions_to_cos(df, cos_manager):
                                    show_status_message(f"âœ… æƒé™è¡¨å·²ä¸Šä¼ ï¼š{len(df)} ä¸ªç”¨æˆ·", "success")
                                    st.balloons()
                                else:
                                    show_status_message("âŒ ä¿å­˜å¤±è´¥", "error")
                        else:
                            show_status_message("âŒ æ ¼å¼é”™è¯¯ï¼šéœ€è¦è‡³å°‘ä¸¤åˆ—ï¼ˆé—¨åº—åç§°ã€äººå‘˜ç¼–å·ï¼‰", "error")
                except Exception as e:
                    show_status_message(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}", "error")
            
            # ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨
            reports_file = st.file_uploader("ä¸Šä¼ è´¢åŠ¡æŠ¥è¡¨", type=['xlsx', 'xls'])
            if reports_file:
                try:
                    with st.spinner("å¤„ç†æŠ¥è¡¨æ–‡ä»¶..."):
                        # è·å–åŸå§‹æ–‡ä»¶æ•°æ®
                        original_file_data = reports_file.getvalue()
                        
                        excel_file = pd.ExcelFile(reports_file)
                        reports_dict = {}
                        
                        for sheet in excel_file.sheet_names:
                            try:
                                df = pd.read_excel(reports_file, sheet_name=sheet)
                                if not df.empty:
                                    reports_dict[sheet] = df
                                    debug_logger.log('INFO', f'è¯»å–å·¥ä½œè¡¨ "{sheet}": {len(df)} è¡Œ')
                                    logger.info(f"è¯»å–å·¥ä½œè¡¨ '{sheet}': {len(df)} è¡Œ")
                            except Exception as e:
                                debug_logger.log('WARNING', f'è·³è¿‡å·¥ä½œè¡¨ "{sheet}": {str(e)}')
                                logger.warning(f"è·³è¿‡å·¥ä½œè¡¨ '{sheet}': {str(e)}")
                                continue
                        
                        if reports_dict:
                            with st.spinner("ä¿å­˜åˆ°è…¾è®¯äº‘COS..."):
                                if save_reports_to_cos(reports_dict, cos_manager, original_file_data):
                                    show_status_message(f"âœ… æŠ¥è¡¨å·²ä¸Šä¼ ï¼š{len(reports_dict)} ä¸ªé—¨åº—", "success")
                                    st.balloons()
                                else:
                                    show_status_message("âŒ ä¿å­˜å¤±è´¥", "error")
                        else:
                            show_status_message("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å·¥ä½œè¡¨", "error")
                            
                except Exception as e:
                    show_status_message(f"âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}", "error")
            
            # ç¼“å­˜ç®¡ç†
            st.subheader("ğŸ—‚ï¸ ç¼“å­˜ç®¡ç†")
            cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
            st.info(f"å½“å‰ç¼“å­˜é¡¹ç›®: {cache_count}")
            
            if st.button("æ¸…é™¤æ‰€æœ‰ç¼“å­˜"):
                cache_keys = [key for key in st.session_state.keys() if key.startswith('cache_')]
                for key in cache_keys:
                    del st.session_state[key]
                show_status_message("âœ… ç¼“å­˜å·²æ¸…é™¤", "success")
                debug_logger.log('INFO', 'ç¼“å­˜å·²æ¸…é™¤')
                st.rerun()
    
    else:
        if st.session_state.logged_in:
            st.subheader("ğŸ‘¤ å½“å‰ç™»å½•")
            st.info(f"é—¨åº—ï¼š{st.session_state.store_name}")
            st.info(f"ç¼–å·ï¼š{st.session_state.user_id}")
            
            if st.button("ğŸšª é€€å‡ºç™»å½•"):
                st.session_state.logged_in = False
                st.session_state.store_name = ""
                st.session_state.user_id = ""
                show_status_message("ğŸ‘‹ å·²é€€å‡ºç™»å½•", "success")
                debug_logger.log('INFO', 'ç”¨æˆ·é€€å‡ºç™»å½•')
                st.rerun()

# æ¸…é™¤çŠ¶æ€æ¶ˆæ¯
st.session_state.operation_status = []

# ä¸»ç•Œé¢
if user_type == "ç®¡ç†å‘˜" and st.session_state.is_admin:
    st.markdown('<div class="admin-panel"><h3>ğŸ‘¨â€ğŸ’¼ ç®¡ç†å‘˜æ§åˆ¶é¢æ¿</h3><p>æ•°æ®æ°¸ä¹…ä¿å­˜åœ¨è…¾è®¯äº‘COSï¼Œæ”¯æŒé«˜æ•ˆå‹ç¼©å­˜å‚¨å’Œç¼“å­˜æœºåˆ¶</p></div>', unsafe_allow_html=True)
    
    try:
        with st.spinner("åŠ è½½æ•°æ®ç»Ÿè®¡..."):
            permissions_data = load_permissions_from_cos(cos_manager)
            reports_data = load_reports_from_cos(cos_manager)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            perms_count = len(permissions_data) if permissions_data is not None else 0
            st.metric("æƒé™è¡¨ç”¨æˆ·æ•°", perms_count)
        with col2:
            reports_count = len(reports_data)
            st.metric("æŠ¥è¡¨é—¨åº—æ•°", reports_count)
        with col3:
            cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
            st.metric("ç¼“å­˜é¡¹ç›®æ•°", cache_count)
            
        # æ•°æ®é¢„è§ˆ
        if permissions_data is not None and len(permissions_data) > 0:
            st.subheader("ğŸ‘¥ æƒé™æ•°æ®é¢„è§ˆ")
            st.dataframe(permissions_data.head(10), use_container_width=True)
        
        if reports_data:
            st.subheader("ğŸ“Š æŠ¥è¡¨æ•°æ®é¢„è§ˆ")
            
            # æ˜¾ç¤ºå­˜å‚¨æ–¹å¼ç»Ÿè®¡
            total_reports = len(reports_data)
            parsed_count = 0
            raw_only_count = 0
            
            # è·å–å…ƒæ•°æ®ç»Ÿè®¡
            try:
                metadata = cos_manager.download_json(cos_manager.metadata_file)
                if metadata and 'reports' in metadata:
                    for report in metadata['reports']:
                        if report.get('has_parsed_data', False):
                            parsed_count += 1
                        else:
                            raw_only_count += 1
            except:
                pass
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»æŠ¥è¡¨æ•°", total_reports)
            with col2:
                st.metric("é¢„è§£æå¯ç”¨", parsed_count, f"{parsed_count/total_reports*100:.1f}%" if total_reports > 0 else "0%")
            with col3:
                st.metric("ä»…åŸå§‹æ–‡ä»¶", raw_only_count, f"{raw_only_count/total_reports*100:.1f}%" if total_reports > 0 else "0%")
            
            # æ˜¾ç¤ºæŠ¥è¡¨è¯¦æƒ…
            report_names = list(reports_data.keys())[:5]  # æ˜¾ç¤ºå‰5ä¸ª
            for name in report_names:
                with st.expander(f"ğŸ“‹ {name}"):
                    df = reports_data[name]
                    st.write(f"æ•°æ®è§„æ¨¡: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
                    
                    # æ˜¾ç¤ºå­˜å‚¨çŠ¶æ€
                    try:
                        metadata = cos_manager.download_json(cos_manager.metadata_file)
                        if metadata and 'reports' in metadata:
                            for report in metadata['reports']:
                                if report.get('store_name') == name:
                                    has_parsed = report.get('has_parsed_data', False)
                                    has_raw = bool(report.get('raw_filename'))
                                    
                                    status_info = []
                                    if has_raw:
                                        status_info.append("âœ… åŸå§‹æ–‡ä»¶")
                                    if has_parsed:
                                        status_info.append("âš¡ é¢„è§£ææ•°æ®")
                                    
                                    if status_info:
                                        st.info(f"å­˜å‚¨çŠ¶æ€: {' + '.join(status_info)}")
                                    break
                    except:
                        pass
                    
                    st.dataframe(df.head(3), use_container_width=True)
                    
    except Exception as e:
        show_status_message(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}", "error")

elif user_type == "ç®¡ç†å‘˜" and not st.session_state.is_admin:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥ç®¡ç†å‘˜å¯†ç ")

else:
    if not st.session_state.logged_in:
        st.subheader("ğŸ” ç”¨æˆ·ç™»å½•")
        
        try:
            with st.spinner("åŠ è½½æƒé™æ•°æ®..."):
                permissions_data = load_permissions_from_cos(cos_manager)
            
            if permissions_data is None:
                st.warning("âš ï¸ ç³»ç»Ÿç»´æŠ¤ä¸­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
            else:
                stores = sorted(permissions_data[permissions_data.columns[0]].unique().tolist())
                
                with st.form("login_form"):
                    selected_store = st.selectbox("é€‰æ‹©é—¨åº—", stores)
                    user_id = st.text_input("äººå‘˜ç¼–å·")
                    submit = st.form_submit_button("ğŸš€ ç™»å½•")
                    
                    if submit and selected_store and user_id:
                        debug_logger.log('INFO', 'ç”¨æˆ·å°è¯•ç™»å½•', {
                            'store': selected_store,
                            'user_id': user_id
                        })
                        
                        if verify_user_permission(selected_store, user_id, permissions_data):
                            st.session_state.logged_in = True
                            st.session_state.store_name = selected_store
                            st.session_state.user_id = user_id
                            show_status_message("âœ… ç™»å½•æˆåŠŸï¼", "success")
                            st.balloons()
                            st.rerun()
                        else:
                            show_status_message("âŒ é—¨åº—æˆ–ç¼–å·é”™è¯¯ï¼", "error")
                            
        except Exception as e:
            show_status_message(f"âŒ æƒé™éªŒè¯å¤±è´¥ï¼š{str(e)}", "error")
    
    else:
        # å·²ç™»å½• - æ˜¾ç¤ºæŠ¥è¡¨
        st.markdown(f'<div class="store-info"><h3>ğŸª {st.session_state.store_name}</h3><p>æ“ä½œå‘˜ï¼š{st.session_state.user_id}</p><p>æ•°æ®æ¥æºï¼šè…¾è®¯äº‘COS</p></div>', unsafe_allow_html=True)
        
        # æ•°æ®åˆ·æ–°æŒ‰é’®
        col1, col2 = st.columns([4, 1])
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°æ•°æ®"):
                debug_logger.log('INFO', 'ç”¨æˆ·æ‰‹åŠ¨åˆ·æ–°æ•°æ®')
                st.rerun()
        
        try:
            with st.spinner("åŠ è½½æŠ¥è¡¨æ•°æ®..."):
                reports_data = load_reports_from_cos(cos_manager)
                matching_sheets = find_matching_reports(st.session_state.store_name, reports_data)
            
            if matching_sheets:
                if len(matching_sheets) > 1:
                    selected_sheet = st.selectbox("é€‰æ‹©æŠ¥è¡¨", matching_sheets)
                else:
                    selected_sheet = matching_sheets[0]
                
                df = reports_data[selected_sheet]
                
                # åº”æ”¶-æœªæ”¶é¢çœ‹æ¿
                st.subheader("ğŸ’° åº”æ”¶-æœªæ”¶é¢")
                
                try:
                    analysis_results = analyze_receivable_data(df)
                    
                    if 'åº”æ”¶-æœªæ”¶é¢' in analysis_results:
                        data = analysis_results['åº”æ”¶-æœªæ”¶é¢']
                        amount = data['amount']
                        
                        col1, col2, col3 = st.columns([1, 2, 1])
                        with col2:
                            if amount > 0:
                                st.markdown(f'''
                                    <div class="receivable-positive">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’³ Â¥{amount:,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">é—¨åº—åº”ä»˜æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            elif amount < 0:
                                st.markdown(f'''
                                    <div class="receivable-negative">
                                        <h1 style="margin: 0; font-size: 3rem;">ğŸ’š Â¥{abs(amount):,.2f}</h1>
                                        <h3 style="margin: 0.5rem 0;">æ€»éƒ¨åº”é€€æ¬¾</h3>
                                        <p style="margin: 0; font-size: 0.9rem;">æ•°æ®æ¥æº: {data['row_name']} (ç¬¬{data['actual_row_number']}è¡Œ)</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                            
                            else:
                                st.markdown('''
                                    <div style="background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); color: #2e7d32; padding: 2rem; border-radius: 15px; text-align: center; box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);">
                                        <h1 style="margin: 0; font-size: 3rem;">âš–ï¸ Â¥0.00</h1>
                                        <h3 style="margin: 0.5rem 0;">æ”¶æ”¯å¹³è¡¡</h3>
                                        <p style="margin: 0;">åº”æ”¶æœªæ”¶é¢ä¸ºé›¶ï¼Œè´¦ç›®å¹³è¡¡</p>
                                    </div>
                                ''', unsafe_allow_html=True)
                    
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°åº”æ”¶-æœªæ”¶é¢æ•°æ®")
                        
                        with st.expander("ğŸ” æŸ¥çœ‹è¯¦æƒ…", expanded=False):
                            debug_info = analysis_results.get('debug_info', {})
                            
                            st.markdown("### ğŸ“‹ æ•°æ®æŸ¥æ‰¾è¯´æ˜")
                            st.write(f"- **æŠ¥è¡¨æ€»è¡Œæ•°ï¼š** {debug_info.get('total_rows', 0)} è¡Œ")
                            
                            if debug_info.get('checked_row_69'):
                                st.write(f"- **ç¬¬69è¡Œå†…å®¹ï¼š** {debug_info.get('row_69_content', 'N/A')}")
                            else:
                                st.write("- **ç¬¬69è¡Œï¼š** æŠ¥è¡¨è¡Œæ•°ä¸è¶³69è¡Œ")
                            
                            st.markdown("""
                            ### ğŸ’¡ å¯èƒ½çš„åŸå› 
                            1. ç¬¬69è¡Œä¸åŒ…å«"åº”æ”¶-æœªæ”¶é¢"ç›¸å…³å…³é”®è¯
                            2. ç¬¬69è¡Œçš„æ•°å€¼ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®
                            3. æŠ¥è¡¨æ ¼å¼ä¸é¢„æœŸä¸ç¬¦
                            
                            ### ğŸ› ï¸ å»ºè®®
                            - è¯·æ£€æŸ¥ExcelæŠ¥è¡¨ç¬¬69è¡Œæ˜¯å¦åŒ…å«"åº”æ”¶-æœªæ”¶é¢"
                            - ç¡®è®¤è¯¥è¡Œæœ‰å¯¹åº”çš„é‡‘é¢æ•°æ®
                            - å¦‚éœ€è°ƒæ•´æŸ¥æ‰¾ä½ç½®ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ
                            """)
                
                except Exception as e:
                    show_status_message(f"âŒ åˆ†ææ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}", "error")
                
                st.divider()
                
                # å®Œæ•´æŠ¥è¡¨æ•°æ®
                st.subheader("ğŸ“‹ å®Œæ•´æŠ¥è¡¨æ•°æ®")
                
                search_term = st.text_input("ğŸ” æœç´¢æŠ¥è¡¨å†…å®¹")
                
                try:
                    if search_term:
                        search_df = df.copy()
                        for col in search_df.columns:
                            search_df[col] = search_df[col].astype(str).fillna('')
                        
                        mask = search_df.apply(
                            lambda x: x.str.contains(search_term, case=False, na=False, regex=False)
                        ).any(axis=1)
                        filtered_df = df[mask]
                        st.info(f"æ‰¾åˆ° {len(filtered_df)} æ¡åŒ…å« '{search_term}' çš„è®°å½•")
                    else:
                        filtered_df = df
                    
                    st.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼šå…± {len(filtered_df)} æ¡è®°å½•ï¼Œ{len(df.columns)} åˆ—")
                    
                    if len(filtered_df) > 0:
                        display_df = filtered_df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_columns = []
                        for i, col in enumerate(display_df.columns):
                            col_name = str(col)
                            if col_name in unique_columns:
                                col_name = f"{col_name}_{i}"
                            unique_columns.append(col_name)
                        display_df.columns = unique_columns
                        
                        # æ¸…ç†æ•°æ®å†…å®¹
                        for col in display_df.columns:
                            display_df[col] = display_df[col].astype(str).fillna('')
                        
                        st.dataframe(display_df, use_container_width=True, height=400)
                    
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                        
                except Exception as e:
                    show_status_message(f"âŒ æ•°æ®å¤„ç†æ—¶å‡ºé”™ï¼š{str(e)}", "error")
                
                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ æ•°æ®ä¸‹è½½")
                
                col1, col2, col3 = st.columns(3)
                
                # ä¸‹è½½å¤„ç†åçš„Excel
                with col1:
                    try:
                        buffer = io.BytesIO()
                        download_df = df.copy()
                        
                        # ç¡®ä¿åˆ—åå”¯ä¸€
                        unique_cols = []
                        for i, col in enumerate(download_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        download_df.columns = unique_cols
                        
                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            download_df.to_excel(writer, index=False)
                        
                        st.download_button(
                            "ğŸ“Š ä¸‹è½½å¤„ç†åExcel",
                            buffer.getvalue(),
                            f"{st.session_state.store_name}_å¤„ç†æ•°æ®_{datetime.now().strftime('%Y%m%d')}.xlsx",
                            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    except Exception as e:
                        show_status_message(f"å¤„ç†åExcelä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")
                
                # ä¸‹è½½åŸå§‹Excelæ–‡ä»¶
                with col2:
                    try:
                        original_data = get_original_file_for_download(st.session_state.store_name, cos_manager)
                        if original_data:
                            st.download_button(
                                "ğŸ“„ ä¸‹è½½åŸå§‹Excel",
                                original_data,
                                f"{st.session_state.store_name}_åŸå§‹æ–‡ä»¶_{datetime.now().strftime('%Y%m%d')}.xlsx",
                                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                        else:
                            st.error("åŸå§‹æ–‡ä»¶ä¸å¯ç”¨")
                    except Exception as e:
                        show_status_message(f"åŸå§‹æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼š{str(e)}", "error")
                
                # ä¸‹è½½CSVæ ¼å¼
                with col3:
                    try:
                        csv_df = df.copy()
                        unique_cols = []
                        for i, col in enumerate(csv_df.columns):
                            col_name = str(col)
                            if col_name in unique_cols:
                                col_name = f"{col_name}_{i}"
                            unique_cols.append(col_name)
                        csv_df.columns = unique_cols
                        
                        csv = csv_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            "ğŸ“‹ ä¸‹è½½CSVæ ¼å¼",
                            csv,
                            f"{st.session_state.store_name}_æŠ¥è¡¨_{datetime.now().strftime('%Y%m%d')}.csv",
                            "text/csv"
                        )
                    except Exception as e:
                        show_status_message(f"CSVä¸‹è½½å‡†å¤‡å¤±è´¥ï¼š{str(e)}", "error")
            
            else:
                st.error(f"âŒ æœªæ‰¾åˆ°é—¨åº— '{st.session_state.store_name}' çš„æŠ¥è¡¨")
                
                if st.session_state.debug_mode:
                    with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯"):
                        st.write("**å¯ç”¨æŠ¥è¡¨åˆ—è¡¨:**")
                        available_stores = list(reports_data.keys())
                        st.write(available_stores)
                        st.write(f"**æŸ¥è¯¢é—¨åº—:** '{st.session_state.store_name}'")
                        st.write(f"**æ ‡å‡†åŒ–æŸ¥è¯¢:** '{normalize_store_name(st.session_state.store_name)}'")
                
        except Exception as e:
            show_status_message(f"âŒ æŠ¥è¡¨åŠ è½½å¤±è´¥ï¼š{str(e)}", "error")

# é¡µé¢åº•éƒ¨çŠ¶æ€ä¿¡æ¯
st.divider()
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.caption(f"ğŸ•’ å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
with col2:
    cache_count = len([key for key in st.session_state.keys() if key.startswith('cache_')])
    st.caption(f"ğŸ’¾ ç¼“å­˜é¡¹ç›®: {cache_count}")
with col3:
    st.caption("â˜ï¸ è…¾è®¯äº‘COSå­˜å‚¨")
with col4:
    st.caption(f"ğŸ”§ ç‰ˆæœ¬: v2.1 (COSç‰ˆ) | API: {API_RATE_LIMIT}/h")
